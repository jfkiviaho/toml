{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "beam_training_strats.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPnylpm2IKwAt6OUvFPALEY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jfkiviaho/toml/blob/master/colab_notebooks/beam_training_strats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8onkd02JWwt0"
      },
      "source": [
        "'''\n",
        "The purpose of this script is to investigate different training strategies for\n",
        "physics-informed machine learning and their impact on the solution of the\n",
        "cantilevered beam structural model.\n",
        "\n",
        "The model is defined as\n",
        "\n",
        "             (EIw'')'' = 0 \n",
        "\n",
        "in the domain [0, 1] and\n",
        "\n",
        "    (EIw''(x = L)' + P = 0\n",
        "          EIw''(x = L) = 0\n",
        "             w'(x = 0) = 0\n",
        "              w(x = 0) = 0\n",
        "\n",
        "on the boundary.\n",
        "\n",
        "The training strategies to be investigated are \n",
        "1. gradient descent with boundary conditions applied as exterior penalties;\n",
        "2. Adam with boundary conditions applied as exterior penalties;\n",
        "3. L-BFGS with boundary conditions applied as exterior penalties;\n",
        "4. Augmented Lagrangian (with L-BFGS inner loop) with boundary conditions\n",
        "   applied as constraints; and\n",
        "5. SLSQP with boundary conditions applied as constraints.\n",
        "\n",
        "'''\n",
        "# Load required modules\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "import numpy as np\n",
        "import numpy.linalg as la\n",
        "\n",
        "import time\n",
        "\n",
        "# Set floating point precision in Keras to double\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "# Set global random seed\n",
        "# Note: this is a shortcut for getting reproducible results.\n",
        "tf.random.set_seed(12345)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKonyxOaW2yQ"
      },
      "source": [
        "# Define model\n",
        "class Model(keras.Sequential):\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Initialize model with parameters, collocation points, and network \n",
        "        topology\n",
        "        \n",
        "        '''\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        ########################################################################\n",
        "        # Define the equation and discretization parameters\n",
        "        ########################################################################\n",
        "        self.EI = 1.0  # N*m^2, flexural rigidity\n",
        "        self.P  = 1.0  # N, tip load\n",
        "        self.L = 1.0   # m, length of bar\n",
        "\n",
        "        self.num_pts = 101\n",
        "\n",
        "        ########################################################################\n",
        "        # Define collocation points\n",
        "        ########################################################################\n",
        "        # Define boundary points and domain points\n",
        "        x0 = np.array([0.0])\n",
        "        x1 = np.array([self.L])\n",
        "        self.bnd_pts = np.array([x0, x1])\n",
        "\n",
        "        self.dom_pts = np.linspace(x0, x1, self.num_pts)\n",
        "\n",
        "        # Normalize boundary points and domain points (maps x -> X)\n",
        "        self.mu = self.dom_pts.mean()\n",
        "        self.sigma = self.dom_pts.std()\n",
        "\n",
        "        self.bnd_pts_norm = tf.constant((self.bnd_pts - self.mu)/self.sigma)\n",
        "        self.dom_pts_norm = tf.constant((self.dom_pts - self.mu)/self.sigma)\n",
        "\n",
        "        ########################################################################\n",
        "        # Define the topology of the neural network\n",
        "        ########################################################################\n",
        "        # Add first layer\n",
        "        input_layer = keras.layers.Dense(units=1, input_shape=[1])\n",
        "        self.add(input_layer)\n",
        "\n",
        "        # Add hidden layers\n",
        "        hidden_layer1 = keras.layers.Dense(units=10, activation=tf.nn.tanh)\n",
        "        self.add(hidden_layer1)\n",
        "        \n",
        "        hidden_layer2 = keras.layers.Dense(units=10, activation=tf.nn.tanh)\n",
        "        self.add(hidden_layer2)\n",
        "\n",
        "        # Add last layer\n",
        "        output_layer = keras.layers.Dense(units=1)\n",
        "        self.add(output_layer)\n",
        "\n",
        "        # Addition of layers creates model parameters. Take current list of\n",
        "        # model parameters and create lists of stitch and partition indices to\n",
        "        # be used in converting between TensorFlow's representation of the\n",
        "        # parameters and the 1D numpy array that an external optimizer will use\n",
        "        self.shapes = tf.shape_n(self.trainable_variables)\n",
        "        self.num_tensors = len(self.shapes)\n",
        "\n",
        "        count = 0 \n",
        "        self.stitch_ind = []\n",
        "        self.partit_ind = []\n",
        "\n",
        "        for i, shape in enumerate(self.shapes):\n",
        "            num_elems = np.product(shape)\n",
        "            self.stitch_ind.append(\n",
        "                    tf.reshape(\n",
        "                        tf.range(\n",
        "                            count, \n",
        "                            count + num_elems,\n",
        "                            dtype=tf.int32\n",
        "                        ),\n",
        "                        shape\n",
        "                    )\n",
        "                )\n",
        "            self.partit_ind.extend([i]*num_elems)\n",
        "            count += num_elems\n",
        "\n",
        "        ########################################################################\n",
        "        # Define optimization parameters and data\n",
        "        ########################################################################\n",
        "        self.max_iters = 10**3  # maximum num. of unconstrained opt. iterations\n",
        "        self.tol_loss  = 1e-6   # loss function tolerance\n",
        "\n",
        "        # Weights for exterior penalty formulations of loss function\n",
        "        self.gamma1 = 1.0\n",
        "        self.gamma2 = 1.0\n",
        "        self.gamma3 = 1.0\n",
        "        self.gamma4 = 1.0\n",
        "\n",
        "        # Optimization history arrays\n",
        "        self.iters = [0]\n",
        "        self.res_hist = [self.eval_res().numpy()]\n",
        "        self.bc_hist = [\n",
        "                [self.eval_bnd_cnd1().numpy()],\n",
        "                [self.eval_bnd_cnd2().numpy()],\n",
        "                [self.eval_bnd_cnd3().numpy()],\n",
        "                [self.eval_bnd_cnd4().numpy()]\n",
        "            ]\n",
        "\n",
        "        return\n",
        "\n",
        "    def convert_numpy_array_to_vars(self, params_array):\n",
        "        '''\n",
        "        Convert 1D NumPy array with parameter values to a list of TensorFlow\n",
        "        variables with appropriate shapes\n",
        "        \n",
        "        '''\n",
        "        train_params = tf.dynamic_partition(\n",
        "                params_array,\n",
        "                self.partit_ind,\n",
        "                self.num_tensors\n",
        "            )\n",
        "\n",
        "        return train_params\n",
        "    \n",
        "    def convert_vars_to_numpy_array(self, params_list):\n",
        "        '''\n",
        "        Convert a list of TensorFlow Variable objects (either the model \n",
        "        parameters or the gradient of a function w.r.t the model parameters) to\n",
        "        a 1D NumPy array\n",
        "        \n",
        "        '''\n",
        "        # If an output has no graph dependence on an input, TensorFlow will\n",
        "        # output None for for the derivative of the output w.r.t. that input.\n",
        "        # Replace None with the appropriately sized zero tensor for this case\n",
        "        for i, (shape, param) in enumerate(zip(self.shapes, params_list)):\n",
        "            if param is None:\n",
        "                params_list[i] = tf.zeros(shape, dtype=tf.float64)\n",
        "\n",
        "        params_tensor = tf.dynamic_stitch(\n",
        "                self.stitch_ind,\n",
        "                params_list\n",
        "            )\n",
        "\n",
        "        return params_tensor.numpy()\n",
        "\n",
        "    def eval_res(self):\n",
        "        '''\n",
        "        Evaluate residual of governing equation\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        res: TensorFlow tensor\n",
        "            evaluation of residual\n",
        "        \n",
        "        '''\n",
        "        # Give normalized domain collocation points a more compact name\n",
        "        X = self.dom_pts_norm\n",
        "        \n",
        "        # Evaluate residual of governing equation\n",
        "        with tf.GradientTape() as t1:\n",
        "            t1.watch(X)\n",
        "\n",
        "            with tf.GradientTape() as t2:\n",
        "                t2.watch(X)\n",
        "\n",
        "                with tf.GradientTape() as t3:\n",
        "                    t3.watch(X)\n",
        "\n",
        "                    with tf.GradientTape() as t4:\n",
        "                        t4.watch(X)\n",
        "                        w = self.__call__(X)\n",
        "                \n",
        "                    w_X = t4.gradient(w, X)\n",
        "\n",
        "                w_XX = t3.gradient(w_X, X)\n",
        "\n",
        "            w_XXX = t2.gradient(w_XX, X)\n",
        "\n",
        "        w_XXXX = t1.gradient(w_XXX, X)\n",
        "\n",
        "        # Apply the chain rule\n",
        "        dXdx = 1/self.sigma\n",
        "        w_xxxx = w_XXXX*dXdx**4\n",
        "        \n",
        "        # Compute residual\n",
        "        res = tf.reduce_sum(\n",
        "                tf.square(\n",
        "                    self.EI*w_xxxx\n",
        "                )\n",
        "            )\n",
        "        \n",
        "        return res\n",
        "\n",
        "    def eval_bnd_cnd1(self):\n",
        "        '''\n",
        "        Evaluate zero-displacement boundary condition\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        bnd_cnd: TensorFlow tensor\n",
        "            boundary condition violation\n",
        "\n",
        "        '''\n",
        "        # Extract left boundary point\n",
        "        X0 = tf.slice(self.bnd_pts_norm, [0,0], [1,1])\n",
        "\n",
        "        # Evaluate solution at left boundary point\n",
        "        w_0 = self.__call__(X0)\n",
        "\n",
        "        # Compute boundary condition violation\n",
        "        bnd_cnd = tf.reduce_sum(tf.square(w_0))\n",
        "\n",
        "        return bnd_cnd\n",
        "\n",
        "    def eval_bnd_cnd2(self):\n",
        "        '''\n",
        "        Evaluate the zero slope boundary condition\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        bnd_cnd: TensorFlow tensor\n",
        "            boundary condition violation\n",
        "\n",
        "        '''\n",
        "        # Extract left boundary point\n",
        "        X0 = tf.slice(self.bnd_pts_norm, [0,0], [1,1])\n",
        "\n",
        "        # Evaluate derivative of the solution at right boundary point\n",
        "        with tf.GradientTape() as t:\n",
        "            t.watch(X0)\n",
        "            w_0 = self.__call__(X0)\n",
        "\n",
        "        w_X_0 = t.gradient(w_0, X0)\n",
        "        \n",
        "        # Apply chain rule\n",
        "        dXdx = 1/self.sigma\n",
        "        w_x_0 = w_X_0*dXdx\n",
        "\n",
        "        # Compute boundary condition violation\n",
        "        bnd_cnd = tf.reduce_sum(tf.square(w_x_0))\n",
        "\n",
        "        return bnd_cnd\n",
        "\n",
        "    def eval_bnd_cnd3(self):\n",
        "        '''\n",
        "        Evaluate zero-moment boundary condition\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        bnd_cnd: TensorFlow tensor\n",
        "            boundary condition violation\n",
        "\n",
        "        '''\n",
        "        # Extract right boundary point\n",
        "        XL = tf.slice(self.bnd_pts_norm, [1,0], [1,1])\n",
        "\n",
        "        # Evaluate solution at left boundary point\n",
        "        with tf.GradientTape() as t1:\n",
        "            t1.watch(XL)\n",
        "\n",
        "            with tf.GradientTape() as t2:\n",
        "                t2.watch(XL)\n",
        "                w_L = self.__call__(XL)\n",
        "\n",
        "            w_X_L = t2.gradient(w_L, XL)\n",
        "\n",
        "        w_XX_L = t1.gradient(w_X_L, XL)\n",
        "\n",
        "        # Apply chain rule\n",
        "        dXdx = 1/self.sigma\n",
        "        w_xx_L = w_XX_L*dXdx**2\n",
        "\n",
        "        # Compute boundary condition violation\n",
        "        bnd_cnd = tf.reduce_sum(tf.square(self.EI*w_xx_L))\n",
        "\n",
        "        return bnd_cnd\n",
        "\n",
        "    def eval_bnd_cnd4(self):\n",
        "        '''\n",
        "        Evaluate the tip load boundary condition\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        bnd_cnd: TensorFlow tensor\n",
        "            boundary condition violation\n",
        "\n",
        "        '''\n",
        "        # Extract right boundary point\n",
        "        XL = tf.slice(self.bnd_pts_norm, [1,0], [1,1])\n",
        "\n",
        "        # Evaluate solution at left boundary point\n",
        "        with tf.GradientTape() as t1:\n",
        "            t1.watch(XL)\n",
        "\n",
        "            with tf.GradientTape() as t2:\n",
        "                t2.watch(XL)\n",
        "\n",
        "                with tf.GradientTape() as t3:\n",
        "                    t3.watch(XL)\n",
        "                    w_L = self.__call__(XL)\n",
        "\n",
        "                w_X_L = t3.gradient(w_L, XL)\n",
        "\n",
        "            w_XX_L = t2.gradient(w_X_L, XL)\n",
        "\n",
        "        w_XXX_L = t1.gradient(w_XX_L, XL)\n",
        "\n",
        "        # Apply chain rule\n",
        "        dXdx = 1/self.sigma\n",
        "        w_xxx_L = w_XXX_L*dXdx**3\n",
        "\n",
        "        # Compute boundary condition violation\n",
        "        bnd_cnd = tf.reduce_sum(tf.square(self.EI*w_xxx_L + self.P))\n",
        "\n",
        "        return bnd_cnd\n",
        "\n",
        "    def train_with_gradient_descent(self):\n",
        "        '''\n",
        "        Train the neural network using a loss function that enforces the\n",
        "        boundary conditions using exterior penalties and the gradient descent\n",
        "        optimizer built into TensorFlow\n",
        "\n",
        "        '''\n",
        "        # Define optimizer\n",
        "        init_learn_rate = 0.01\n",
        "        opt = tf.optimizers.SGD(learning_rate=init_learn_rate)\n",
        "\n",
        "        self.count = 0\n",
        "        loss = 1\n",
        "\n",
        "        while self.count < self.max_iters and loss > self.tol_loss:\n",
        "            with tf.GradientTape() as t:\n",
        "                # Compute the loss function\n",
        "                res = self.eval_res()\n",
        "                bc1 = self.eval_bnd_cnd1()\n",
        "                bc2 = self.eval_bnd_cnd2()\n",
        "                bc3 = self.eval_bnd_cnd3()\n",
        "                bc4 = self.eval_bnd_cnd4()\n",
        "\n",
        "                loss = res \\\n",
        "                       + self.gamma1*bc1 \\\n",
        "                       + self.gamma2*bc2 \\\n",
        "                       + self.gamma3*bc3 \\\n",
        "                       + self.gamma4*bc4\n",
        "\n",
        "            # Compute the loss function gradient and update the parameters\n",
        "            grad = t.gradient(loss, self.trainable_variables)\n",
        "            opt.apply_gradients(zip(grad, self.trainable_variables))\n",
        "\n",
        "            self.count += 1\n",
        "            print('iter: {}'.format(self.count))\n",
        "            print('loss: {}'.format(loss.numpy()))\n",
        "            print()\n",
        "\n",
        "            # Record optimization history\n",
        "            self.iters.append(self.count)\n",
        "            self.res_hist.append(res.numpy())\n",
        "            self.bc_hist[0].append(bc1.numpy())\n",
        "            self.bc_hist[1].append(bc2.numpy())\n",
        "            self.bc_hist[2].append(bc3.numpy())\n",
        "            self.bc_hist[3].append(bc4.numpy())\n",
        "\n",
        "        return\n",
        "\n",
        "    def train_with_adam(self):\n",
        "        '''\n",
        "        Train the neural network using a loss function the enforces the\n",
        "        boundary conditions using exterior penalties and the Adam optimizer\n",
        "        built into TensorFlow\n",
        "\n",
        "        '''\n",
        "        # Define optimizer\n",
        "        opt = tf.optimizers.Adam()\n",
        "\n",
        "        self.count = 0\n",
        "        loss = 1\n",
        "\n",
        "        while self.count < self.max_iters and loss > self.tol_loss:\n",
        "            with tf.GradientTape() as t:\n",
        "                # Compute the loss function\n",
        "                res = self.eval_res()\n",
        "                bc1 = self.eval_bnd_cnd1()\n",
        "                bc2 = self.eval_bnd_cnd2()\n",
        "                bc3 = self.eval_bnd_cnd3()\n",
        "                bc4 = self.eval_bnd_cnd4()\n",
        "\n",
        "                loss = res \\\n",
        "                       + self.gamma1*bc1 \\\n",
        "                       + self.gamma2*bc2 \\\n",
        "                       + self.gamma3*bc3 \\\n",
        "                       + self.gamma4*bc4\n",
        "\n",
        "            # Compute the loss function gradient and update the parameters\n",
        "            grad = t.gradient(loss, self.trainable_variables)\n",
        "            opt.apply_gradients(zip(grad, self.trainable_variables))\n",
        "\n",
        "            self.count += 1\n",
        "            print('iter: {}'.format(self.count))\n",
        "            print('loss: {}'.format(loss.numpy()))\n",
        "            print()\n",
        "\n",
        "            # Record optimization history\n",
        "            self.iters.append(self.count)\n",
        "            self.res_hist.append(res.numpy())\n",
        "            self.bc_hist[0].append(bc1.numpy())\n",
        "            self.bc_hist[1].append(bc2.numpy())\n",
        "            self.bc_hist[2].append(bc3.numpy())\n",
        "            self.bc_hist[3].append(bc4.numpy())\n",
        "\n",
        "        return\n",
        "\n",
        "    def train_with_bfgs(self):\n",
        "        '''\n",
        "        Train the neural network using a loss function the enforces the\n",
        "        boundary conditions using exterior penalties and the L-BFGS optimizer\n",
        "        from SciPy\n",
        "        \n",
        "        '''\n",
        "        # Define wrapper function for the residual evaluation, which is now the\n",
        "        # objective function\n",
        "        self.count = 0\n",
        "\n",
        "        def f(p):\n",
        "            # Set model parameters from input NumPy array\n",
        "            train_params = self.convert_numpy_array_to_vars(p)\n",
        "\n",
        "            for i, (shape, param) in enumerate(zip(self.shapes, train_params)):\n",
        "                self.trainable_variables[i].assign(tf.reshape(param, shape))\n",
        "            \n",
        "            # Compute the loss function\n",
        "            res = self.eval_res()\n",
        "            bc1 = self.eval_bnd_cnd1()\n",
        "            bc2 = self.eval_bnd_cnd2()\n",
        "            bc3 = self.eval_bnd_cnd3()\n",
        "            bc4 = self.eval_bnd_cnd4()\n",
        "\n",
        "            loss = res \\\n",
        "                   + self.gamma1*bc1 \\\n",
        "                   + self.gamma2*bc2 \\\n",
        "                   + self.gamma3*bc3 \\\n",
        "                   + self.gamma4*bc4\n",
        "            \n",
        "            return loss.numpy()\n",
        "\n",
        "        # Define wrapper function for gradient of residual w.r.t. parameters\n",
        "        def gradf(p):\n",
        "            # Set model parameters from input NumPy array\n",
        "            train_params = self.convert_numpy_array_to_vars(p)\n",
        "\n",
        "            for i, (shape, param) in enumerate(zip(self.shapes, train_params)):\n",
        "                self.trainable_variables[i].assign(tf.reshape(param, shape))\n",
        "\n",
        "            # Compute gradient and convert to NumPy array\n",
        "            with tf.GradientTape() as t:\n",
        "                # Compute the loss function\n",
        "                res = self.eval_res()\n",
        "                bc1 = self.eval_bnd_cnd1()\n",
        "                bc2 = self.eval_bnd_cnd2()\n",
        "                bc3 = self.eval_bnd_cnd3()\n",
        "                bc4 = self.eval_bnd_cnd4()\n",
        "\n",
        "                loss = res \\\n",
        "                       + self.gamma1*bc1 \\\n",
        "                       + self.gamma2*bc2 \\\n",
        "                       + self.gamma3*bc3 \\\n",
        "                       + self.gamma4*bc4\n",
        "\n",
        "            grad_loss = t.gradient(loss, self.trainable_variables)\n",
        "            grad_loss_array = self.convert_vars_to_numpy_array(grad_loss)\n",
        "\n",
        "            # Print count of times gradient of residual has been evaluated\n",
        "            self.count += 1\n",
        "            print()\n",
        "            print('iter:', self.count)\n",
        "            print('loss:', loss.numpy())\n",
        "\n",
        "            # Record optimization history\n",
        "            self.iters.append(self.count)\n",
        "            self.res_hist.append(res.numpy())\n",
        "            self.bc_hist[0].append(bc1.numpy())\n",
        "            self.bc_hist[1].append(bc2.numpy())\n",
        "            self.bc_hist[2].append(bc3.numpy())\n",
        "            self.bc_hist[3].append(bc4.numpy())\n",
        "\n",
        "            return grad_loss_array\n",
        "\n",
        "        # Extract initial point for optimizer\n",
        "        p0 = self.convert_vars_to_numpy_array(self.trainable_variables)\n",
        "\n",
        "        # Run optimizer\n",
        "        ans = minimize(\n",
        "                f, p0, method='L-BFGS-B', jac=gradf,\n",
        "                options={\n",
        "                    'maxiter': self.max_iters,\n",
        "                    'ftol': self.tol_loss\n",
        "                }\n",
        "            )\n",
        "\n",
        "        # Check if optimizer converged\n",
        "        if not ans.success:\n",
        "            print('Optimizer did not converge.')\n",
        "            print('Terminated with status ', ans.status)\n",
        "            print('Cause of termination:')\n",
        "            print(ans.message)\n",
        "            \n",
        "        # Extract optimized parameters and set model\n",
        "        p = ans.x\n",
        "        opt_params = self.convert_numpy_array_to_vars(p)\n",
        "\n",
        "        for i, (shape, param) in enumerate(zip(self.shapes, opt_params)):\n",
        "            self.trainable_variables[i].assign(tf.reshape(param, shape))\n",
        "        \n",
        "        return\n",
        "\n",
        "    def train_with_aug_lag(self):\n",
        "        '''\n",
        "        Train the neural network using a Augmented Lagrangian approach to the\n",
        "        optimization of the residual and the enforcement of the boundary\n",
        "        conditions. The unconstrained optimization in the inner loop of the\n",
        "        Augment Lagrangian procedure is performed by the L-BFGS optimizer from\n",
        "        SciPy\n",
        "        \n",
        "        '''\n",
        "        # Define wrapper function for the residual evaluation, which is now the\n",
        "        # objective function\n",
        "        self.count = 0\n",
        "\n",
        "        def loss(p, lam, r):\n",
        "            # Set model parameters from input NumPy array\n",
        "            train_params = self.convert_numpy_array_to_vars(p)\n",
        "\n",
        "            for i, (shape, param) in enumerate(zip(self.shapes, train_params)):\n",
        "                self.trainable_variables[i].assign(tf.reshape(param, shape))\n",
        "            \n",
        "            # Compute the loss function\n",
        "            res = self.eval_res()\n",
        "            bc1 = self.eval_bnd_cnd1()\n",
        "            bc2 = self.eval_bnd_cnd2()\n",
        "            bc3 = self.eval_bnd_cnd3()\n",
        "            bc4 = self.eval_bnd_cnd4()\n",
        "\n",
        "            loss = res \\\n",
        "                   + lam[0]*bc1 \\\n",
        "                   + lam[1]*bc2 \\\n",
        "                   + lam[2]*bc3 \\\n",
        "                   + lam[3]*bc4 \\\n",
        "                   + r*(bc1 + bc2 + bc3 + bc4)\n",
        "            \n",
        "            return loss.numpy()\n",
        "\n",
        "        # Define wrapper function for gradient of residual w.r.t. parameters\n",
        "        def loss_grad(p, lam, r):\n",
        "            # Set model parameters from input NumPy array\n",
        "            train_params = self.convert_numpy_array_to_vars(p)\n",
        "\n",
        "            for i, (shape, param) in enumerate(zip(self.shapes, train_params)):\n",
        "                self.trainable_variables[i].assign(tf.reshape(param, shape))\n",
        "\n",
        "            # Compute gradient and convert to NumPy array\n",
        "            with tf.GradientTape() as t:\n",
        "                # Compute the loss function\n",
        "                res = self.eval_res()\n",
        "                bc1 = self.eval_bnd_cnd1()\n",
        "                bc2 = self.eval_bnd_cnd2()\n",
        "                bc3 = self.eval_bnd_cnd3()\n",
        "                bc4 = self.eval_bnd_cnd4()\n",
        "\n",
        "                loss = res \\\n",
        "                       + lam[0]*bc1 \\\n",
        "                       + lam[1]*bc2 \\\n",
        "                       + lam[2]*bc3 \\\n",
        "                       + lam[3]*bc4 \\\n",
        "                       + r*(bc1 + bc2 + bc3 + bc4)\n",
        "\n",
        "            grad_loss = t.gradient(loss, self.trainable_variables)\n",
        "            grad_loss_array = self.convert_vars_to_numpy_array(grad_loss)\n",
        "\n",
        "            # Print count of times gradient of residual has been evaluated\n",
        "            self.count += 1\n",
        "            print()\n",
        "            print('iter:', self.count)\n",
        "            print('loss:', loss.numpy())\n",
        "\n",
        "            # Record optimization history\n",
        "            self.iters.append(self.count)\n",
        "            self.res_hist.append(res.numpy())\n",
        "            self.bc_hist[0].append(bc1.numpy())\n",
        "            self.bc_hist[1].append(bc2.numpy())\n",
        "            self.bc_hist[2].append(bc3.numpy())\n",
        "            self.bc_hist[3].append(bc4.numpy())\n",
        "\n",
        "            return grad_loss_array\n",
        "\n",
        "        # Extract initial point for optimizer\n",
        "        p0 = self.convert_vars_to_numpy_array(self.trainable_variables)\n",
        "\n",
        "        # Initialize pseudo-Lagrange multipliers and penalty parameter\n",
        "        lam = np.zeros(4, dtype=np.float64)\n",
        "        dlam = np.ones(4, dtype=np.float64)\n",
        "\n",
        "        r    = 1.0    # penalty parameter\n",
        "        rmax = 1.0e4  # max. penalty parameter\n",
        "        cr   = 2.0    # penalty parameter growth\n",
        "\n",
        "        # Set constrained and unconstrained optimization parameters\n",
        "        max_iter_con = 10**2           # max. num. of constrained opt. iter.\n",
        "        max_iter_unc = self.max_iters  # max. num. of unconstrained opt. iter.\n",
        "        tol_con      = 1.0e-4          # tolerance on norm of multiplier update\n",
        "\n",
        "        it_con = 0\n",
        "\n",
        "        while la.norm(dlam) > tol_con and it_con < max_iter_con:\n",
        "            # Extract initial point for optimizer\n",
        "            p0 = self.convert_vars_to_numpy_array(self.trainable_variables)\n",
        "\n",
        "            # Run unconstrained optimizer\n",
        "            f = lambda p: loss(p, lam, r)\n",
        "            gradf = lambda p: loss_grad(p, lam, r)\n",
        "            ans = minimize(\n",
        "                    f, p0, method='L-BFGS-B', jac=gradf,\n",
        "                    options={\n",
        "                        'maxiter': self.max_iters,\n",
        "                        'ftol': self.tol_loss\n",
        "                    }\n",
        "                )\n",
        "\n",
        "            # Check if optimizer converged\n",
        "            if not ans.success:\n",
        "                print('Optimizer did not converge.')\n",
        "                print('Terminated with status ', ans.status)\n",
        "                print('Cause of termination:')\n",
        "                print(ans.message)\n",
        "                \n",
        "            # Extract optimized parameters and set model\n",
        "            p = ans.x\n",
        "            opt_params = self.convert_numpy_array_to_vars(p)\n",
        "\n",
        "            for i, (shape, param) in enumerate(zip(self.shapes, opt_params)):\n",
        "                self.trainable_variables[i].assign(tf.reshape(param, shape))\n",
        "\n",
        "            # Extract the constraint evaluations and update the multipliers and\n",
        "            # penalty parameter\n",
        "            cons_eval = np.array([\n",
        "                    self.eval_bnd_cnd1().numpy(),\n",
        "                    self.eval_bnd_cnd2().numpy(),\n",
        "                    self.eval_bnd_cnd3().numpy(),\n",
        "                    self.eval_bnd_cnd4().numpy()\n",
        "                ], dtype=np.float64)\n",
        "            dlam = 2.0*r*cons_eval\n",
        "            lam += dlam\n",
        "            r = min(cr*r, rmax)\n",
        "\n",
        "            it_con += 1\n",
        "            print()\n",
        "            print()\n",
        "            print('########################################')\n",
        "            print('Constrained iteration number {}'.format(it_con))\n",
        "            print('########################################')\n",
        "            print()\n",
        "        \n",
        "        return\n",
        "\n",
        "    def train_with_slsqp(self):\n",
        "        '''\n",
        "        Train the neural network using the SLSQP optimizer from SciPy\n",
        "        \n",
        "        '''\n",
        "        # Define wrapper function for the residual evaluation, which is now the\n",
        "        # objective function\n",
        "        self.count = 0\n",
        "\n",
        "        def f(p):\n",
        "            # Set model parameters from input NumPy array\n",
        "            train_params = self.convert_numpy_array_to_vars(p)\n",
        "\n",
        "            for i, (shape, param) in enumerate(zip(self.shapes, train_params)):\n",
        "                self.trainable_variables[i].assign(tf.reshape(param, shape))\n",
        "            \n",
        "            res = self.eval_res()\n",
        "            \n",
        "            return res.numpy()\n",
        "\n",
        "        # Define wrapper function for gradient of residual w.r.t. parameters\n",
        "        def gradf(p):\n",
        "            # Set model parameters from input NumPy array\n",
        "            train_params = self.convert_numpy_array_to_vars(p)\n",
        "\n",
        "            for i, (shape, param) in enumerate(zip(self.shapes, train_params)):\n",
        "                self.trainable_variables[i].assign(tf.reshape(param, shape))\n",
        "\n",
        "            # Compute gradient and convert to NumPy array\n",
        "            with tf.GradientTape() as t:\n",
        "                res = self.eval_res()\n",
        "\n",
        "            grad_res = t.gradient(res, self.trainable_variables)\n",
        "            grad_res_array = self.convert_vars_to_numpy_array(grad_res)\n",
        "\n",
        "            # Print count of times gradient of residual has been evaluated\n",
        "            self.count += 1\n",
        "            print()\n",
        "            print('iter:', self.count)\n",
        "            print('res: ', res.numpy())\n",
        "\n",
        "            # Record optimization history\n",
        "            self.iters.append(self.count)\n",
        "            self.res_hist.append(res.numpy())\n",
        "\n",
        "            return grad_res_array\n",
        "\n",
        "        # Define wrapper function for boundary condition evaluations\n",
        "        def g_k(k, p):\n",
        "            # Set model parameters from input NumPy array\n",
        "            train_params = self.convert_numpy_array_to_vars(p)\n",
        "\n",
        "            for i, (shape, param) in enumerate(zip(self.shapes, train_params)):\n",
        "                self.trainable_variables[i].assign(tf.reshape(param, shape))\n",
        "\n",
        "            if   k == 1:\n",
        "                bnd_cnd = self.eval_bnd_cnd1()\n",
        "            elif k == 2:\n",
        "                bnd_cnd = self.eval_bnd_cnd2()\n",
        "            elif k == 3:\n",
        "                bnd_cnd = self.eval_bnd_cnd3()\n",
        "            elif k == 4:\n",
        "                bnd_cnd = self.eval_bnd_cnd4()\n",
        "            else:\n",
        "                error_msg = 'No such boundary condition defined'\n",
        "                raise ValueError(error_msg)\n",
        "\n",
        "            return bnd_cnd.numpy()\n",
        "\n",
        "        # Define wrapper function for boundary condition evaluation gradients\n",
        "        def gradg_k(k, p):\n",
        "            # Set model parameters from input NumPy array\n",
        "            train_params = self.convert_numpy_array_to_vars(p)\n",
        "\n",
        "            for i, (shape, param) in enumerate(zip(self.shapes, train_params)):\n",
        "                self.trainable_variables[i].assign(tf.reshape(param, shape))\n",
        "\n",
        "            # Compute gradient and convert to NumPy array\n",
        "            with tf.GradientTape() as t:\n",
        "                if   k == 1:\n",
        "                    bnd_cnd = self.eval_bnd_cnd1()\n",
        "                    self.bc_hist[k-1].append(bnd_cnd.numpy())\n",
        "                elif k == 2:\n",
        "                    bnd_cnd = self.eval_bnd_cnd2()\n",
        "                    self.bc_hist[k-1].append(bnd_cnd.numpy())\n",
        "                elif k == 3:\n",
        "                    bnd_cnd = self.eval_bnd_cnd3()\n",
        "                    self.bc_hist[k-1].append(bnd_cnd.numpy())\n",
        "                elif k == 4:\n",
        "                    bnd_cnd = self.eval_bnd_cnd4()\n",
        "                    self.bc_hist[k-1].append(bnd_cnd.numpy())\n",
        "                else:\n",
        "                    error_msg = 'No such boundary condition defined'\n",
        "                    raise ValueError(error_msg)\n",
        "\n",
        "            grad_bnd_cnd = t.gradient(bnd_cnd, self.trainable_variables)\n",
        "            grad_bnd_cnd_array = self.convert_vars_to_numpy_array(grad_bnd_cnd)\n",
        "            \n",
        "            print('bc{}: '.format(k), bnd_cnd.numpy())\n",
        "\n",
        "            return grad_bnd_cnd_array\n",
        "\n",
        "        # Define constraints for optimizer\n",
        "        cons = [{'type': 'eq', \n",
        "                 'fun': lambda p: g_k(1,p), \n",
        "                 'jac': lambda p: gradg_k(1,p)},\n",
        "                {'type': 'eq', \n",
        "                 'fun': lambda p: g_k(2,p), \n",
        "                 'jac': lambda p: gradg_k(2,p)},\n",
        "                {'type': 'eq', \n",
        "                 'fun': lambda p: g_k(3,p), \n",
        "                 'jac': lambda p: gradg_k(3,p)},\n",
        "                {'type': 'eq', \n",
        "                 'fun': lambda p: g_k(4,p), \n",
        "                 'jac': lambda p: gradg_k(4,p)}]\n",
        "\n",
        "        # Extract initial point for optimizer\n",
        "        p0 = self.convert_vars_to_numpy_array(self.trainable_variables)\n",
        "\n",
        "        # Run optimizer\n",
        "        ans = minimize(\n",
        "                f, p0, method='SLSQP', jac=gradf, constraints=cons,\n",
        "                options={'maxiter':1000}\n",
        "            )\n",
        "\n",
        "        # Check if optimizer converged\n",
        "        if not ans.success:\n",
        "            print('Optimizer did not converge.')\n",
        "            print('Terminated with status ', ans.status)\n",
        "            print('Cause of termination:')\n",
        "            print(ans.message)\n",
        "            \n",
        "        # Extract optimized parameters and set model\n",
        "        p = ans.x\n",
        "        opt_params = self.convert_numpy_array_to_vars(p)\n",
        "\n",
        "        for i, (shape, param) in enumerate(zip(self.shapes, opt_params)):\n",
        "            self.trainable_variables[i].assign(tf.reshape(param, shape))\n",
        "        \n",
        "        return\n",
        "\n",
        "def w(x):\n",
        "    '''\n",
        "    Analytical solution for cantilevered Euler-Bernoulli beam with load at end\n",
        "\n",
        "    '''\n",
        "    # Define dimensional parameters\n",
        "    EI = 1.0  # N*m^2, flexural rigidity\n",
        "    P =  1.0  # N, tip load on beam\n",
        "    L = 1.0\n",
        "\n",
        "    return -P*x**3/(6*EI) + 0.5*P*L*x**2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBbTh25nW-We",
        "outputId": "17a83fe0-69d4-4690-f7aa-086f12a604a0"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    model = Model()\n",
        "    tic = time.time()\n",
        "    #model.train_with_gradient_descent()\n",
        "    model.train_with_adam()\n",
        "    #model.train_with_bfgs()\n",
        "    #model.train_with_aug_lag()\n",
        "    #model.train_with_slsqp()\n",
        "    toc = time.time()\n",
        "    wall_time = toc - tic\n",
        "\n",
        "    # Plot comparison between model solution and analytical solution\n",
        "    n = 1000\n",
        "    lb = np.array([0.0])\n",
        "    ub = np.array([1.0])\n",
        "    x = np.linspace(lb, ub, n)\n",
        "    w_pred = model.predict((x - model.mu)/model.sigma)\n",
        "    w_true = w(x)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(\n",
        "            x, w_pred,\n",
        "            color='cornflowerblue',\n",
        "            linewidth=2,\n",
        "            label='model'\n",
        "        )\n",
        "    ax.plot(\n",
        "            x, w_true,\n",
        "            color='orange',\n",
        "            linewidth=2,\n",
        "            linestyle='--',\n",
        "            label='truth'\n",
        "        )\n",
        "    ax.legend()\n",
        "    ax.set_xlabel(r'$x$')\n",
        "    ax.set_ylabel(r'$w$')\n",
        "\n",
        "    # Plot residual his\n",
        "    fig, ax = plt.subplots(figsize=(8,5))\n",
        "    ax.loglog(\n",
        "            model.iters, model.res_hist,\n",
        "            color='cornflowerblue',\n",
        "            linewidth=2,\n",
        "        )\n",
        "    ax.set_xlabel(r'iteration', fontsize=16)\n",
        "    ax.set_ylabel(r'residual', fontsize=16)\n",
        "\n",
        "    # Plot boundary condition violation histories\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(18,6))\n",
        "    plt.subplots_adjust(\n",
        "            hspace=0.5,  # height of blank space between subplots?\n",
        "            wspace=0.5   # width of blank space between subplots?\n",
        "        )\n",
        "    for k in range(4):\n",
        "        # Do some binary math to organize plots into a 2 x 2 grid\n",
        "        i = (k & 1) >> 0\n",
        "        j = (k & 2) >> 1\n",
        "\n",
        "        ax[i,j].loglog(\n",
        "                model.iters, model.bc_hist[k],\n",
        "                color='cornflowerblue',\n",
        "                linewidth=2,\n",
        "            )\n",
        "        ax[i,j].set_xlabel(r'iteration', fontsize=16)\n",
        "        ax[i,j].set_ylabel(r'boundary condition {}'.format(k+1), fontsize=16)\n",
        "\n",
        "    print()\n",
        "    print('Model results')\n",
        "    print('-------------')\n",
        "    print('res:', model.eval_res().numpy())\n",
        "    print('bc1:', model.eval_bnd_cnd1().numpy())\n",
        "    print('bc2:', model.eval_bnd_cnd2().numpy())\n",
        "    print('bc3:', model.eval_bnd_cnd3().numpy())\n",
        "    print('bc4:', model.eval_bnd_cnd4().numpy())\n",
        "    print()\n",
        "    hrs = int(wall_time//3600)\n",
        "    mns = int((wall_time - hrs*3600)//60)\n",
        "    scs = wall_time - hrs*3600 - mns*60\n",
        "    print(\n",
        "            'Model trained in ' +\n",
        "            '{0:02d}:{1:02d}:{2:6.3f} '.format(hrs, mns, scs) + \n",
        "            '(wall time)'\n",
        "        )\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter: 1\n",
            "loss: 100.63256067204821\n",
            "\n",
            "iter: 2\n",
            "loss: 90.44113577746525\n",
            "\n",
            "iter: 3\n",
            "loss: 81.21007581277077\n",
            "\n",
            "iter: 4\n",
            "loss: 72.87103355733214\n",
            "\n",
            "iter: 5\n",
            "loss: 65.35702169651086\n",
            "\n",
            "iter: 6\n",
            "loss: 58.60266243728375\n",
            "\n",
            "iter: 7\n",
            "loss: 52.54431005400852\n",
            "\n",
            "iter: 8\n",
            "loss: 47.12059135215877\n",
            "\n",
            "iter: 9\n",
            "loss: 42.27287233543734\n",
            "\n",
            "iter: 10\n",
            "loss: 37.94563581820821\n",
            "\n",
            "iter: 11\n",
            "loss: 34.0868558886337\n",
            "\n",
            "iter: 12\n",
            "loss: 30.64831347087092\n",
            "\n",
            "iter: 13\n",
            "loss: 27.585703731455826\n",
            "\n",
            "iter: 14\n",
            "loss: 24.858583784227072\n",
            "\n",
            "iter: 15\n",
            "loss: 22.430247172613853\n",
            "\n",
            "iter: 16\n",
            "loss: 20.267551501141156\n",
            "\n",
            "iter: 17\n",
            "loss: 18.340709893505547\n",
            "\n",
            "iter: 18\n",
            "loss: 16.623057756952434\n",
            "\n",
            "iter: 19\n",
            "loss: 15.090806323263843\n",
            "\n",
            "iter: 20\n",
            "loss: 13.722792901424587\n",
            "\n",
            "iter: 21\n",
            "loss: 12.50023568443108\n",
            "\n",
            "iter: 22\n",
            "loss: 11.406498789942638\n",
            "\n",
            "iter: 23\n",
            "loss: 10.426871205618344\n",
            "\n",
            "iter: 24\n",
            "loss: 9.548361607165356\n",
            "\n",
            "iter: 25\n",
            "loss: 8.759509737588118\n",
            "\n",
            "iter: 26\n",
            "loss: 8.050214220591224\n",
            "\n",
            "iter: 27\n",
            "loss: 7.411576260764925\n",
            "\n",
            "iter: 28\n",
            "loss: 6.835758511347481\n",
            "\n",
            "iter: 29\n",
            "loss: 6.31585831988252\n",
            "\n",
            "iter: 30\n",
            "loss: 5.8457945053644265\n",
            "\n",
            "iter: 31\n",
            "loss: 5.4202067557209865\n",
            "\n",
            "iter: 32\n",
            "loss: 5.034366675406869\n",
            "\n",
            "iter: 33\n",
            "loss: 4.684099476998737\n",
            "\n",
            "iter: 34\n",
            "loss: 4.365715304849579\n",
            "\n",
            "iter: 35\n",
            "loss: 4.07594919950855\n",
            "\n",
            "iter: 36\n",
            "loss: 3.811908750557345\n",
            "\n",
            "iter: 37\n",
            "loss: 3.571028535098646\n",
            "\n",
            "iter: 38\n",
            "loss: 3.35103049482528\n",
            "\n",
            "iter: 39\n",
            "loss: 3.1498894653184646\n",
            "\n",
            "iter: 40\n",
            "loss: 2.9658031381471064\n",
            "\n",
            "iter: 41\n",
            "loss: 2.797165810950305\n",
            "\n",
            "iter: 42\n",
            "loss: 2.6425453629184474\n",
            "\n",
            "iter: 43\n",
            "loss: 2.5006629803022546\n",
            "\n",
            "iter: 44\n",
            "loss: 2.3703752437410714\n",
            "\n",
            "iter: 45\n",
            "loss: 2.2506582700979223\n",
            "\n",
            "iter: 46\n",
            "loss: 2.1405936703512465\n",
            "\n",
            "iter: 47\n",
            "loss: 2.039356137972299\n",
            "\n",
            "iter: 48\n",
            "loss: 1.946202517705938\n",
            "\n",
            "iter: 49\n",
            "loss: 1.8604622239436455\n",
            "\n",
            "iter: 50\n",
            "loss: 1.781528884221634\n",
            "\n",
            "iter: 51\n",
            "loss: 1.7088530814680647\n",
            "\n",
            "iter: 52\n",
            "loss: 1.6419360636622353\n",
            "\n",
            "iter: 53\n",
            "loss: 1.5803242863899896\n",
            "\n",
            "iter: 54\n",
            "loss: 1.523604656028333\n",
            "\n",
            "iter: 55\n",
            "loss: 1.4714003508129254\n",
            "\n",
            "iter: 56\n",
            "loss: 1.4233671136426298\n",
            "\n",
            "iter: 57\n",
            "loss: 1.3791899321566463\n",
            "\n",
            "iter: 58\n",
            "loss: 1.3385800452116625\n",
            "\n",
            "iter: 59\n",
            "loss: 1.3012722369059788\n",
            "\n",
            "iter: 60\n",
            "loss: 1.2670223967787384\n",
            "\n",
            "iter: 61\n",
            "loss: 1.2356053359053574\n",
            "\n",
            "iter: 62\n",
            "loss: 1.2068128528487434\n",
            "\n",
            "iter: 63\n",
            "loss: 1.1804520416699575\n",
            "\n",
            "iter: 64\n",
            "loss: 1.1563438283296907\n",
            "\n",
            "iter: 65\n",
            "loss: 1.1343217142526112\n",
            "\n",
            "iter: 66\n",
            "loss: 1.114230699034885\n",
            "\n",
            "iter: 67\n",
            "loss: 1.0959263502293264\n",
            "\n",
            "iter: 68\n",
            "loss: 1.0792739879659927\n",
            "\n",
            "iter: 69\n",
            "loss: 1.0641479559421128\n",
            "\n",
            "iter: 70\n",
            "loss: 1.050430957145991\n",
            "\n",
            "iter: 71\n",
            "loss: 1.038013440974916\n",
            "\n",
            "iter: 72\n",
            "loss: 1.0267930363281006\n",
            "\n",
            "iter: 73\n",
            "loss: 1.0166740311945461\n",
            "\n",
            "iter: 74\n",
            "loss: 1.007566902222735\n",
            "\n",
            "iter: 75\n",
            "loss: 0.9993878975813602\n",
            "\n",
            "iter: 76\n",
            "loss: 0.9920586737130608\n",
            "\n",
            "iter: 77\n",
            "loss: 0.9855059825202444\n",
            "\n",
            "iter: 78\n",
            "loss: 0.9796614014815924\n",
            "\n",
            "iter: 79\n",
            "loss: 0.9744610963860392\n",
            "\n",
            "iter: 80\n",
            "loss: 0.9698456055260494\n",
            "\n",
            "iter: 81\n",
            "loss: 0.9657596354382603\n",
            "\n",
            "iter: 82\n",
            "loss: 0.9621518611599598\n",
            "\n",
            "iter: 83\n",
            "loss: 0.9589747276308839\n",
            "\n",
            "iter: 84\n",
            "loss: 0.9561842523354636\n",
            "\n",
            "iter: 85\n",
            "loss: 0.9537398317312619\n",
            "\n",
            "iter: 86\n",
            "loss: 0.9516040549890414\n",
            "\n",
            "iter: 87\n",
            "loss: 0.949742528071945\n",
            "\n",
            "iter: 88\n",
            "loss: 0.9481237096009628\n",
            "\n",
            "iter: 89\n",
            "loss: 0.9467187579338585\n",
            "\n",
            "iter: 90\n",
            "loss: 0.945501387107766\n",
            "\n",
            "iter: 91\n",
            "loss: 0.9444477282864523\n",
            "\n",
            "iter: 92\n",
            "loss: 0.9435361933466331\n",
            "\n",
            "iter: 93\n",
            "loss: 0.9427473381441431\n",
            "\n",
            "iter: 94\n",
            "loss: 0.942063724470244\n",
            "\n",
            "iter: 95\n",
            "loss: 0.9414697812642085\n",
            "\n",
            "iter: 96\n",
            "loss: 0.940951666843766\n",
            "\n",
            "iter: 97\n",
            "loss: 0.9404971344625663\n",
            "\n",
            "iter: 98\n",
            "loss: 0.9400954033420953\n",
            "\n",
            "iter: 99\n",
            "loss: 0.939737036609421\n",
            "\n",
            "iter: 100\n",
            "loss: 0.9394138265992013\n",
            "\n",
            "iter: 101\n",
            "loss: 0.9391186870800895\n",
            "\n",
            "iter: 102\n",
            "loss: 0.9388455514025276\n",
            "\n",
            "iter: 103\n",
            "loss: 0.9385892754560112\n",
            "\n",
            "iter: 104\n",
            "loss: 0.9383455446292649\n",
            "\n",
            "iter: 105\n",
            "loss: 0.9381107845212177\n",
            "\n",
            "iter: 106\n",
            "loss: 0.9378820757327826\n",
            "\n",
            "iter: 107\n",
            "loss: 0.9376570734789578\n",
            "\n",
            "iter: 108\n",
            "loss: 0.9374339328795522\n",
            "\n",
            "iter: 109\n",
            "loss: 0.9372112406058147\n",
            "\n",
            "iter: 110\n",
            "loss: 0.9369879531676607\n",
            "\n",
            "iter: 111\n",
            "loss: 0.9367633416667179\n",
            "\n",
            "iter: 112\n",
            "loss: 0.9365369424606805\n",
            "\n",
            "iter: 113\n",
            "loss: 0.9363085129858465\n",
            "\n",
            "iter: 114\n",
            "loss: 0.9360779919965525\n",
            "\n",
            "iter: 115\n",
            "loss: 0.9358454636602502\n",
            "\n",
            "iter: 116\n",
            "loss: 0.9356111252054434\n",
            "\n",
            "iter: 117\n",
            "loss: 0.9353752580553446\n",
            "\n",
            "iter: 118\n",
            "loss: 0.9351382025154268\n",
            "\n",
            "iter: 119\n",
            "loss: 0.934900336086481\n",
            "\n",
            "iter: 120\n",
            "loss: 0.9346620553646491\n",
            "\n",
            "iter: 121\n",
            "loss: 0.934423761320887\n",
            "\n",
            "iter: 122\n",
            "loss: 0.9341858475916281\n",
            "\n",
            "iter: 123\n",
            "loss: 0.9339486913151535\n",
            "\n",
            "iter: 124\n",
            "loss: 0.9337126460400595\n",
            "\n",
            "iter: 125\n",
            "loss: 0.9334780363051333\n",
            "\n",
            "iter: 126\n",
            "loss: 0.933245153609392\n",
            "\n",
            "iter: 127\n",
            "loss: 0.9330142536120876\n",
            "\n",
            "iter: 128\n",
            "loss: 0.9327855544866056\n",
            "\n",
            "iter: 129\n",
            "loss: 0.932559236379978\n",
            "\n",
            "iter: 130\n",
            "loss: 0.9323354419046306\n",
            "\n",
            "iter: 131\n",
            "loss: 0.9321142775329141\n",
            "\n",
            "iter: 132\n",
            "loss: 0.9318958157075733\n",
            "\n",
            "iter: 133\n",
            "loss: 0.93168009744844\n",
            "\n",
            "iter: 134\n",
            "loss: 0.9314671352404821\n",
            "\n",
            "iter: 135\n",
            "loss: 0.9312569160285299\n",
            "\n",
            "iter: 136\n",
            "loss: 0.9310494042047056\n",
            "\n",
            "iter: 137\n",
            "loss: 0.9308445445355394\n",
            "\n",
            "iter: 138\n",
            "loss: 0.9306422650194576\n",
            "\n",
            "iter: 139\n",
            "loss: 0.9304424796828376\n",
            "\n",
            "iter: 140\n",
            "loss: 0.9302450913157867\n",
            "\n",
            "iter: 141\n",
            "loss: 0.9300499941272117\n",
            "\n",
            "iter: 142\n",
            "loss: 0.9298570762760391\n",
            "\n",
            "iter: 143\n",
            "loss: 0.9296662222230818\n",
            "\n",
            "iter: 144\n",
            "loss: 0.9294773148514149\n",
            "\n",
            "iter: 145\n",
            "loss: 0.9292902373207528\n",
            "\n",
            "iter: 146\n",
            "loss: 0.9291048746462223\n",
            "\n",
            "iter: 147\n",
            "loss: 0.9289211150152644\n",
            "\n",
            "iter: 148\n",
            "loss: 0.9287388508711196\n",
            "\n",
            "iter: 149\n",
            "loss: 0.9285579797945364\n",
            "\n",
            "iter: 150\n",
            "loss: 0.9283784052087632\n",
            "\n",
            "iter: 151\n",
            "loss: 0.9282000369216624\n",
            "\n",
            "iter: 152\n",
            "loss: 0.9280227915088264\n",
            "\n",
            "iter: 153\n",
            "loss: 0.9278465925372404\n",
            "\n",
            "iter: 154\n",
            "loss: 0.9276713706316888\n",
            "\n",
            "iter: 155\n",
            "loss: 0.9274970633939711\n",
            "\n",
            "iter: 156\n",
            "loss: 0.9273236151944274\n",
            "\n",
            "iter: 157\n",
            "loss: 0.927150976862386\n",
            "\n",
            "iter: 158\n",
            "loss: 0.9269791053044211\n",
            "\n",
            "iter: 159\n",
            "loss: 0.926807963076433\n",
            "\n",
            "iter: 160\n",
            "loss: 0.9266375179291279\n",
            "\n",
            "iter: 161\n",
            "loss: 0.9264677423392262\n",
            "\n",
            "iter: 162\n",
            "loss: 0.9262986130331184\n",
            "\n",
            "iter: 163\n",
            "loss: 0.9261301105072044\n",
            "\n",
            "iter: 164\n",
            "loss: 0.925962218549631\n",
            "\n",
            "iter: 165\n",
            "loss: 0.9257949237703184\n",
            "\n",
            "iter: 166\n",
            "loss: 0.9256282151482724\n",
            "\n",
            "iter: 167\n",
            "loss: 0.9254620836057611\n",
            "\n",
            "iter: 168\n",
            "loss: 0.9252965216174431\n",
            "\n",
            "iter: 169\n",
            "loss: 0.9251315228594159\n",
            "\n",
            "iter: 170\n",
            "loss: 0.9249670818994573\n",
            "\n",
            "iter: 171\n",
            "loss: 0.9248031939267072\n",
            "\n",
            "iter: 172\n",
            "loss: 0.9246398545174388\n",
            "\n",
            "iter: 173\n",
            "loss: 0.9244770594335315\n",
            "\n",
            "iter: 174\n",
            "loss: 0.9243148044513058\n",
            "\n",
            "iter: 175\n",
            "loss: 0.9241530852196449\n",
            "\n",
            "iter: 176\n",
            "loss: 0.9239918971471012\n",
            "\n",
            "iter: 177\n",
            "loss: 0.9238312353175643\n",
            "\n",
            "iter: 178\n",
            "loss: 0.923671094433142\n",
            "\n",
            "iter: 179\n",
            "loss: 0.9235114687816329\n",
            "\n",
            "iter: 180\n",
            "loss: 0.9233523522248849\n",
            "\n",
            "iter: 181\n",
            "loss: 0.9231937382038551\n",
            "\n",
            "iter: 182\n",
            "loss: 0.923035619756413\n",
            "\n",
            "iter: 183\n",
            "loss: 0.9228779895446512\n",
            "\n",
            "iter: 184\n",
            "loss: 0.9227208398893479\n",
            "\n",
            "iter: 185\n",
            "loss: 0.922564162809897\n",
            "\n",
            "iter: 186\n",
            "loss: 0.922407950068286\n",
            "\n",
            "iter: 187\n",
            "loss: 0.9222521932156416\n",
            "\n",
            "iter: 188\n",
            "loss: 0.9220968836395921\n",
            "\n",
            "iter: 189\n",
            "loss: 0.9219420126105609\n",
            "\n",
            "iter: 190\n",
            "loss: 0.9217875713251847\n",
            "\n",
            "iter: 191\n",
            "loss: 0.9216335509454655\n",
            "\n",
            "iter: 192\n",
            "loss: 0.9214799426328524\n",
            "\n",
            "iter: 193\n",
            "loss: 0.9213267375770547\n",
            "\n",
            "iter: 194\n",
            "loss: 0.9211739270198387\n",
            "\n",
            "iter: 195\n",
            "loss: 0.9210215022742879\n",
            "\n",
            "iter: 196\n",
            "loss: 0.9208694547399962\n",
            "\n",
            "iter: 197\n",
            "loss: 0.9207177759145566\n",
            "\n",
            "iter: 198\n",
            "loss: 0.9205664574015888\n",
            "\n",
            "iter: 199\n",
            "loss: 0.9204154909155284\n",
            "\n",
            "iter: 200\n",
            "loss: 0.920264868283464\n",
            "\n",
            "iter: 201\n",
            "loss: 0.9201145814444376\n",
            "\n",
            "iter: 202\n",
            "loss: 0.9199646224467479\n",
            "\n",
            "iter: 203\n",
            "loss: 0.9198149834438144\n",
            "\n",
            "iter: 204\n",
            "loss: 0.9196656566891169\n",
            "\n",
            "iter: 205\n",
            "loss: 0.9195166345305968\n",
            "\n",
            "iter: 206\n",
            "loss: 0.9193679094047559\n",
            "\n",
            "iter: 207\n",
            "loss: 0.9192194738306091\n",
            "\n",
            "iter: 208\n",
            "loss: 0.9190713204035792\n",
            "\n",
            "iter: 209\n",
            "loss: 0.9189234417894833\n",
            "\n",
            "iter: 210\n",
            "loss: 0.918775830718782\n",
            "\n",
            "iter: 211\n",
            "loss: 0.918628479981309\n",
            "\n",
            "iter: 212\n",
            "loss: 0.9184813824217055\n",
            "\n",
            "iter: 213\n",
            "loss: 0.9183345309357167\n",
            "\n",
            "iter: 214\n",
            "loss: 0.918187918467442\n",
            "\n",
            "iter: 215\n",
            "loss: 0.9180415380075477\n",
            "\n",
            "iter: 216\n",
            "loss: 0.9178953825923778\n",
            "\n",
            "iter: 217\n",
            "loss: 0.9177494453038865\n",
            "\n",
            "iter: 218\n",
            "loss: 0.917603719270286\n",
            "\n",
            "iter: 219\n",
            "loss: 0.9174581976673224\n",
            "\n",
            "iter: 220\n",
            "loss: 0.9173128737200748\n",
            "\n",
            "iter: 221\n",
            "loss: 0.9171677407051753\n",
            "\n",
            "iter: 222\n",
            "loss: 0.9170227919533053\n",
            "\n",
            "iter: 223\n",
            "loss: 0.9168780208518421\n",
            "\n",
            "iter: 224\n",
            "loss: 0.9167334208474893\n",
            "\n",
            "iter: 225\n",
            "loss: 0.9165889854487778\n",
            "\n",
            "iter: 226\n",
            "loss: 0.916444708228326\n",
            "\n",
            "iter: 227\n",
            "loss: 0.9163005828248182\n",
            "\n",
            "iter: 228\n",
            "loss: 0.9161566029446728\n",
            "\n",
            "iter: 229\n",
            "loss: 0.916012762363408\n",
            "\n",
            "iter: 230\n",
            "loss: 0.9158690549267174\n",
            "\n",
            "iter: 231\n",
            "loss: 0.9157254745512851\n",
            "\n",
            "iter: 232\n",
            "loss: 0.9155820152253389\n",
            "\n",
            "iter: 233\n",
            "loss: 0.9154386710089764\n",
            "\n",
            "iter: 234\n",
            "loss: 0.9152954360342621\n",
            "\n",
            "iter: 235\n",
            "loss: 0.9151523045051257\n",
            "\n",
            "iter: 236\n",
            "loss: 0.9150092706970847\n",
            "\n",
            "iter: 237\n",
            "loss: 0.914866328956804\n",
            "\n",
            "iter: 238\n",
            "loss: 0.9147234737015215\n",
            "\n",
            "iter: 239\n",
            "loss: 0.9145806994183444\n",
            "\n",
            "iter: 240\n",
            "loss: 0.9144380006634335\n",
            "\n",
            "iter: 241\n",
            "loss: 0.9142953720610794\n",
            "\n",
            "iter: 242\n",
            "loss: 0.9141528083026895\n",
            "\n",
            "iter: 243\n",
            "loss: 0.9140103041457045\n",
            "\n",
            "iter: 244\n",
            "loss: 0.913867854412468\n",
            "\n",
            "iter: 245\n",
            "loss: 0.9137254539890854\n",
            "\n",
            "iter: 246\n",
            "loss: 0.9135830978242944\n",
            "\n",
            "iter: 247\n",
            "loss: 0.9134407809283746\n",
            "\n",
            "iter: 248\n",
            "loss: 0.9132984983721222\n",
            "\n",
            "iter: 249\n",
            "loss: 0.9131562452858923\n",
            "\n",
            "iter: 250\n",
            "loss: 0.9130140168587226\n",
            "\n",
            "iter: 251\n",
            "loss: 0.912871808337542\n",
            "\n",
            "iter: 252\n",
            "loss: 0.9127296150264548\n",
            "\n",
            "iter: 253\n",
            "loss: 0.9125874322861005\n",
            "\n",
            "iter: 254\n",
            "loss: 0.9124452555330764\n",
            "\n",
            "iter: 255\n",
            "loss: 0.9123030802394096\n",
            "\n",
            "iter: 256\n",
            "loss: 0.9121609019320664\n",
            "\n",
            "iter: 257\n",
            "loss: 0.9120187161924839\n",
            "\n",
            "iter: 258\n",
            "loss: 0.9118765186561142\n",
            "\n",
            "iter: 259\n",
            "loss: 0.9117343050119728\n",
            "\n",
            "iter: 260\n",
            "loss: 0.9115920710021879\n",
            "\n",
            "iter: 261\n",
            "loss: 0.9114498124215488\n",
            "\n",
            "iter: 262\n",
            "loss: 0.9113075251170607\n",
            "\n",
            "iter: 263\n",
            "loss: 0.9111652049874975\n",
            "\n",
            "iter: 264\n",
            "loss: 0.9110228479829727\n",
            "\n",
            "iter: 265\n",
            "loss: 0.9108804501045117\n",
            "\n",
            "iter: 266\n",
            "loss: 0.9107380074036411\n",
            "\n",
            "iter: 267\n",
            "loss: 0.9105955159819858\n",
            "\n",
            "iter: 268\n",
            "loss: 0.9104529719908806\n",
            "\n",
            "iter: 269\n",
            "loss: 0.9103103716309853\n",
            "\n",
            "iter: 270\n",
            "loss: 0.9101677111519074\n",
            "\n",
            "iter: 271\n",
            "loss: 0.9100249868518246\n",
            "\n",
            "iter: 272\n",
            "loss: 0.9098821950771065\n",
            "\n",
            "iter: 273\n",
            "loss: 0.9097393322219289\n",
            "\n",
            "iter: 274\n",
            "loss: 0.9095963947278821\n",
            "\n",
            "iter: 275\n",
            "loss: 0.9094533790835748\n",
            "\n",
            "iter: 276\n",
            "loss: 0.909310281824225\n",
            "\n",
            "iter: 277\n",
            "loss: 0.909167099531257\n",
            "\n",
            "iter: 278\n",
            "loss: 0.9090238288318876\n",
            "\n",
            "iter: 279\n",
            "loss: 0.9088804663987221\n",
            "\n",
            "iter: 280\n",
            "loss: 0.9087370089493492\n",
            "\n",
            "iter: 281\n",
            "loss: 0.9085934532459446\n",
            "\n",
            "iter: 282\n",
            "loss: 0.9084497960948797\n",
            "\n",
            "iter: 283\n",
            "loss: 0.9083060343463385\n",
            "\n",
            "iter: 284\n",
            "loss: 0.9081621648939369\n",
            "\n",
            "iter: 285\n",
            "loss: 0.9080181846743485\n",
            "\n",
            "iter: 286\n",
            "loss: 0.9078740906669281\n",
            "\n",
            "iter: 287\n",
            "loss: 0.9077298798933379\n",
            "\n",
            "iter: 288\n",
            "loss: 0.9075855494171667\n",
            "\n",
            "iter: 289\n",
            "loss: 0.9074410963435471\n",
            "\n",
            "iter: 290\n",
            "loss: 0.9072965178187639\n",
            "\n",
            "iter: 291\n",
            "loss: 0.9071518110298598\n",
            "\n",
            "iter: 292\n",
            "loss: 0.9070069732042332\n",
            "\n",
            "iter: 293\n",
            "loss: 0.9068620016092334\n",
            "\n",
            "iter: 294\n",
            "loss: 0.9067168935517517\n",
            "\n",
            "iter: 295\n",
            "loss: 0.9065716463778136\n",
            "\n",
            "iter: 296\n",
            "loss: 0.9064262574721681\n",
            "\n",
            "iter: 297\n",
            "loss: 0.906280724257878\n",
            "\n",
            "iter: 298\n",
            "loss: 0.9061350441959146\n",
            "\n",
            "iter: 299\n",
            "loss: 0.9059892147847527\n",
            "\n",
            "iter: 300\n",
            "loss: 0.9058432335599623\n",
            "\n",
            "iter: 301\n",
            "loss: 0.9056970980938075\n",
            "\n",
            "iter: 302\n",
            "loss: 0.9055508059948434\n",
            "\n",
            "iter: 303\n",
            "loss: 0.9054043549075099\n",
            "\n",
            "iter: 304\n",
            "loss: 0.905257742511729\n",
            "\n",
            "iter: 305\n",
            "loss: 0.9051109665224969\n",
            "\n",
            "iter: 306\n",
            "loss: 0.904964024689481\n",
            "\n",
            "iter: 307\n",
            "loss: 0.9048169147966112\n",
            "\n",
            "iter: 308\n",
            "loss: 0.904669634661678\n",
            "\n",
            "iter: 309\n",
            "loss: 0.904522182135925\n",
            "\n",
            "iter: 310\n",
            "loss: 0.9043745551036535\n",
            "\n",
            "iter: 311\n",
            "loss: 0.9042267514818201\n",
            "\n",
            "iter: 312\n",
            "loss: 0.9040787692196467\n",
            "\n",
            "iter: 313\n",
            "loss: 0.903930606298228\n",
            "\n",
            "iter: 314\n",
            "loss: 0.9037822607301476\n",
            "\n",
            "iter: 315\n",
            "loss: 0.9036337305590927\n",
            "\n",
            "iter: 316\n",
            "loss: 0.903485013859478\n",
            "\n",
            "iter: 317\n",
            "loss: 0.9033361087360678\n",
            "\n",
            "iter: 318\n",
            "loss: 0.9031870133236007\n",
            "\n",
            "iter: 319\n",
            "loss: 0.9030377257864202\n",
            "\n",
            "iter: 320\n",
            "loss: 0.9028882443181018\n",
            "\n",
            "iter: 321\n",
            "loss: 0.9027385671410846\n",
            "\n",
            "iter: 322\n",
            "loss: 0.9025886925063084\n",
            "\n",
            "iter: 323\n",
            "loss: 0.9024386186928448\n",
            "\n",
            "iter: 324\n",
            "loss: 0.9022883440075393\n",
            "\n",
            "iter: 325\n",
            "loss: 0.9021378667846506\n",
            "\n",
            "iter: 326\n",
            "loss: 0.9019871853854956\n",
            "\n",
            "iter: 327\n",
            "loss: 0.9018362981980984\n",
            "\n",
            "iter: 328\n",
            "loss: 0.9016852036368387\n",
            "\n",
            "iter: 329\n",
            "loss: 0.901533900142109\n",
            "\n",
            "iter: 330\n",
            "loss: 0.9013823861799696\n",
            "\n",
            "iter: 331\n",
            "loss: 0.901230660241809\n",
            "\n",
            "iter: 332\n",
            "loss: 0.9010787208440073\n",
            "\n",
            "iter: 333\n",
            "loss: 0.9009265665275998\n",
            "\n",
            "iter: 334\n",
            "loss: 0.9007741958579439\n",
            "\n",
            "iter: 335\n",
            "loss: 0.9006216074243878\n",
            "\n",
            "iter: 336\n",
            "loss: 0.9004687998399429\n",
            "\n",
            "iter: 337\n",
            "loss: 0.9003157717409541\n",
            "\n",
            "iter: 338\n",
            "loss: 0.9001625217867791\n",
            "\n",
            "iter: 339\n",
            "loss: 0.9000090486594627\n",
            "\n",
            "iter: 340\n",
            "loss: 0.8998553510634197\n",
            "\n",
            "iter: 341\n",
            "loss: 0.8997014277251182\n",
            "\n",
            "iter: 342\n",
            "loss: 0.8995472773927642\n",
            "\n",
            "iter: 343\n",
            "loss: 0.899392898835993\n",
            "\n",
            "iter: 344\n",
            "loss: 0.8992382908455588\n",
            "\n",
            "iter: 345\n",
            "loss: 0.8990834522330302\n",
            "\n",
            "iter: 346\n",
            "loss: 0.8989283818304862\n",
            "\n",
            "iter: 347\n",
            "loss: 0.898773078490216\n",
            "\n",
            "iter: 348\n",
            "loss: 0.8986175410844205\n",
            "\n",
            "iter: 349\n",
            "loss: 0.8984617685049152\n",
            "\n",
            "iter: 350\n",
            "loss: 0.8983057596628379\n",
            "\n",
            "iter: 351\n",
            "loss: 0.8981495134883553\n",
            "\n",
            "iter: 352\n",
            "loss: 0.8979930289303751\n",
            "\n",
            "iter: 353\n",
            "loss: 0.89783630495626\n",
            "\n",
            "iter: 354\n",
            "loss: 0.8976793405515404\n",
            "\n",
            "iter: 355\n",
            "loss: 0.8975221347196378\n",
            "\n",
            "iter: 356\n",
            "loss: 0.8973646864815846\n",
            "\n",
            "iter: 357\n",
            "loss: 0.8972069948757461\n",
            "\n",
            "iter: 358\n",
            "loss: 0.8970490589575505\n",
            "\n",
            "iter: 359\n",
            "loss: 0.8968908777992168\n",
            "\n",
            "iter: 360\n",
            "loss: 0.8967324504894889\n",
            "\n",
            "iter: 361\n",
            "loss: 0.8965737761333682\n",
            "\n",
            "iter: 362\n",
            "loss: 0.8964148538518529\n",
            "\n",
            "iter: 363\n",
            "loss: 0.8962556827816768\n",
            "\n",
            "iter: 364\n",
            "loss: 0.8960962620750517\n",
            "\n",
            "iter: 365\n",
            "loss: 0.8959365908994135\n",
            "\n",
            "iter: 366\n",
            "loss: 0.8957766684371674\n",
            "\n",
            "iter: 367\n",
            "loss: 0.8956164938854398\n",
            "\n",
            "iter: 368\n",
            "loss: 0.8954560664558303\n",
            "\n",
            "iter: 369\n",
            "loss: 0.8952953853741661\n",
            "\n",
            "iter: 370\n",
            "loss: 0.8951344498802607\n",
            "\n",
            "iter: 371\n",
            "loss: 0.8949732592276749\n",
            "\n",
            "iter: 372\n",
            "loss: 0.894811812683477\n",
            "\n",
            "iter: 373\n",
            "loss: 0.8946501095280124\n",
            "\n",
            "iter: 374\n",
            "loss: 0.8944881490546687\n",
            "\n",
            "iter: 375\n",
            "loss: 0.8943259305696459\n",
            "\n",
            "iter: 376\n",
            "loss: 0.8941634533917321\n",
            "\n",
            "iter: 377\n",
            "loss: 0.894000716852076\n",
            "\n",
            "iter: 378\n",
            "loss: 0.8938377202939648\n",
            "\n",
            "iter: 379\n",
            "loss: 0.8936744630726052\n",
            "\n",
            "iter: 380\n",
            "loss: 0.893510944554905\n",
            "\n",
            "iter: 381\n",
            "loss: 0.8933471641192575\n",
            "\n",
            "iter: 382\n",
            "loss: 0.8931831211553285\n",
            "\n",
            "iter: 383\n",
            "loss: 0.8930188150638458\n",
            "\n",
            "iter: 384\n",
            "loss: 0.8928542452563906\n",
            "\n",
            "iter: 385\n",
            "loss: 0.8926894111551913\n",
            "\n",
            "iter: 386\n",
            "loss: 0.8925243121929198\n",
            "\n",
            "iter: 387\n",
            "loss: 0.8923589478124897\n",
            "\n",
            "iter: 388\n",
            "loss: 0.8921933174668574\n",
            "\n",
            "iter: 389\n",
            "loss: 0.8920274206188241\n",
            "\n",
            "iter: 390\n",
            "loss: 0.8918612567408412\n",
            "\n",
            "iter: 391\n",
            "loss: 0.8916948253148174\n",
            "\n",
            "iter: 392\n",
            "loss: 0.8915281258319265\n",
            "\n",
            "iter: 393\n",
            "loss: 0.891361157792421\n",
            "\n",
            "iter: 394\n",
            "loss: 0.8911939207054431\n",
            "\n",
            "iter: 395\n",
            "loss: 0.8910264140888405\n",
            "\n",
            "iter: 396\n",
            "loss: 0.8908586374689843\n",
            "\n",
            "iter: 397\n",
            "loss: 0.8906905903805875\n",
            "\n",
            "iter: 398\n",
            "loss: 0.8905222723665281\n",
            "\n",
            "iter: 399\n",
            "loss: 0.8903536829776691\n",
            "\n",
            "iter: 400\n",
            "loss: 0.8901848217726882\n",
            "\n",
            "iter: 401\n",
            "loss: 0.8900156883179006\n",
            "\n",
            "iter: 402\n",
            "loss: 0.889846282187092\n",
            "\n",
            "iter: 403\n",
            "loss: 0.8896766029613471\n",
            "\n",
            "iter: 404\n",
            "loss: 0.8895066502288824\n",
            "\n",
            "iter: 405\n",
            "loss: 0.8893364235848831\n",
            "\n",
            "iter: 406\n",
            "loss: 0.8891659226313368\n",
            "\n",
            "iter: 407\n",
            "loss: 0.8889951469768743\n",
            "\n",
            "iter: 408\n",
            "loss: 0.8888240962366071\n",
            "\n",
            "iter: 409\n",
            "loss: 0.8886527700319702\n",
            "\n",
            "iter: 410\n",
            "loss: 0.8884811679905674\n",
            "\n",
            "iter: 411\n",
            "loss: 0.8883092897460123\n",
            "\n",
            "iter: 412\n",
            "loss: 0.8881371349377809\n",
            "\n",
            "iter: 413\n",
            "loss: 0.8879647032110538\n",
            "\n",
            "iter: 414\n",
            "loss: 0.8877919942165715\n",
            "\n",
            "iter: 415\n",
            "loss: 0.8876190076104831\n",
            "\n",
            "iter: 416\n",
            "loss: 0.887445743054201\n",
            "\n",
            "iter: 417\n",
            "loss: 0.8872722002142558\n",
            "\n",
            "iter: 418\n",
            "loss: 0.8870983787621526\n",
            "\n",
            "iter: 419\n",
            "loss: 0.8869242783742279\n",
            "\n",
            "iter: 420\n",
            "loss: 0.8867498987315106\n",
            "\n",
            "iter: 421\n",
            "loss: 0.8865752395195814\n",
            "\n",
            "iter: 422\n",
            "loss: 0.886400300428437\n",
            "\n",
            "iter: 423\n",
            "loss: 0.8862250811523511\n",
            "\n",
            "iter: 424\n",
            "loss: 0.8860495813897423\n",
            "\n",
            "iter: 425\n",
            "loss: 0.8858738008430386\n",
            "\n",
            "iter: 426\n",
            "loss: 0.8856977392185466\n",
            "\n",
            "iter: 427\n",
            "loss: 0.8855213962263201\n",
            "\n",
            "iter: 428\n",
            "loss: 0.8853447715800307\n",
            "\n",
            "iter: 429\n",
            "loss: 0.8851678649968394\n",
            "\n",
            "iter: 430\n",
            "loss: 0.8849906761972698\n",
            "\n",
            "iter: 431\n",
            "loss: 0.8848132049050825\n",
            "\n",
            "iter: 432\n",
            "loss: 0.8846354508471505\n",
            "\n",
            "iter: 433\n",
            "loss: 0.8844574137533356\n",
            "\n",
            "iter: 434\n",
            "loss: 0.8842790933563663\n",
            "\n",
            "iter: 435\n",
            "loss: 0.8841004893917187\n",
            "\n",
            "iter: 436\n",
            "loss: 0.8839216015974922\n",
            "\n",
            "iter: 437\n",
            "loss: 0.8837424297142956\n",
            "\n",
            "iter: 438\n",
            "loss: 0.8835629734851271\n",
            "\n",
            "iter: 439\n",
            "loss: 0.8833832326552571\n",
            "\n",
            "iter: 440\n",
            "loss: 0.8832032069721143\n",
            "\n",
            "iter: 441\n",
            "loss: 0.8830228961851707\n",
            "\n",
            "iter: 442\n",
            "loss: 0.8828423000458271\n",
            "\n",
            "iter: 443\n",
            "loss: 0.882661418307302\n",
            "\n",
            "iter: 444\n",
            "loss: 0.8824802507245201\n",
            "\n",
            "iter: 445\n",
            "loss: 0.8822987970539999\n",
            "\n",
            "iter: 446\n",
            "loss: 0.8821170570537462\n",
            "\n",
            "iter: 447\n",
            "loss: 0.8819350304831399\n",
            "\n",
            "iter: 448\n",
            "loss: 0.8817527171028314\n",
            "\n",
            "iter: 449\n",
            "loss: 0.8815701166746323\n",
            "\n",
            "iter: 450\n",
            "loss: 0.8813872289614104\n",
            "\n",
            "iter: 451\n",
            "loss: 0.8812040537269847\n",
            "\n",
            "iter: 452\n",
            "loss: 0.8810205907360191\n",
            "\n",
            "iter: 453\n",
            "loss: 0.8808368397539214\n",
            "\n",
            "iter: 454\n",
            "loss: 0.8806528005467376\n",
            "\n",
            "iter: 455\n",
            "loss: 0.8804684728810528\n",
            "\n",
            "iter: 456\n",
            "loss: 0.8802838565238883\n",
            "\n",
            "iter: 457\n",
            "loss: 0.8800989512426007\n",
            "\n",
            "iter: 458\n",
            "loss: 0.8799137568047833\n",
            "\n",
            "iter: 459\n",
            "loss: 0.8797282729781655\n",
            "\n",
            "iter: 460\n",
            "loss: 0.8795424995305156\n",
            "\n",
            "iter: 461\n",
            "loss: 0.8793564362295424\n",
            "\n",
            "iter: 462\n",
            "loss: 0.8791700828427995\n",
            "\n",
            "iter: 463\n",
            "loss: 0.878983439137585\n",
            "\n",
            "iter: 464\n",
            "loss: 0.8787965048808517\n",
            "\n",
            "iter: 465\n",
            "loss: 0.8786092798391061\n",
            "\n",
            "iter: 466\n",
            "loss: 0.8784217637783169\n",
            "\n",
            "iter: 467\n",
            "loss: 0.8782339564638209\n",
            "\n",
            "iter: 468\n",
            "loss: 0.8780458576602279\n",
            "\n",
            "iter: 469\n",
            "loss: 0.8778574671313304\n",
            "\n",
            "iter: 470\n",
            "loss: 0.877668784640008\n",
            "\n",
            "iter: 471\n",
            "loss: 0.8774798099481386\n",
            "\n",
            "iter: 472\n",
            "loss: 0.8772905428165058\n",
            "\n",
            "iter: 473\n",
            "loss: 0.877100983004708\n",
            "\n",
            "iter: 474\n",
            "loss: 0.8769111302710682\n",
            "\n",
            "iter: 475\n",
            "loss: 0.8767209843725438\n",
            "\n",
            "iter: 476\n",
            "loss: 0.8765305450646398\n",
            "\n",
            "iter: 477\n",
            "loss: 0.8763398121013162\n",
            "\n",
            "iter: 478\n",
            "loss: 0.8761487852349019\n",
            "\n",
            "iter: 479\n",
            "loss: 0.8759574642160067\n",
            "\n",
            "iter: 480\n",
            "loss: 0.8757658487934322\n",
            "\n",
            "iter: 481\n",
            "loss: 0.8755739387140871\n",
            "\n",
            "iter: 482\n",
            "loss: 0.8753817337228983\n",
            "\n",
            "iter: 483\n",
            "loss: 0.8751892335627256\n",
            "\n",
            "iter: 484\n",
            "loss: 0.8749964379742761\n",
            "\n",
            "iter: 485\n",
            "loss: 0.8748033466960172\n",
            "\n",
            "iter: 486\n",
            "loss: 0.8746099594640937\n",
            "\n",
            "iter: 487\n",
            "loss: 0.8744162760122408\n",
            "\n",
            "iter: 488\n",
            "loss: 0.8742222960717005\n",
            "\n",
            "iter: 489\n",
            "loss: 0.874028019371138\n",
            "\n",
            "iter: 490\n",
            "loss: 0.873833445636557\n",
            "\n",
            "iter: 491\n",
            "loss: 0.8736385745912166\n",
            "\n",
            "iter: 492\n",
            "loss: 0.8734434059555483\n",
            "\n",
            "iter: 493\n",
            "loss: 0.8732479394470731\n",
            "\n",
            "iter: 494\n",
            "loss: 0.8730521747803193\n",
            "\n",
            "iter: 495\n",
            "loss: 0.8728561116667387\n",
            "\n",
            "iter: 496\n",
            "loss: 0.8726597498146268\n",
            "\n",
            "iter: 497\n",
            "loss: 0.8724630889290397\n",
            "\n",
            "iter: 498\n",
            "loss: 0.8722661287117124\n",
            "\n",
            "iter: 499\n",
            "loss: 0.8720688688609801\n",
            "\n",
            "iter: 500\n",
            "loss: 0.8718713090716936\n",
            "\n",
            "iter: 501\n",
            "loss: 0.8716734490351425\n",
            "\n",
            "iter: 502\n",
            "loss: 0.8714752884389716\n",
            "\n",
            "iter: 503\n",
            "loss: 0.8712768269671016\n",
            "\n",
            "iter: 504\n",
            "loss: 0.8710780642996522\n",
            "\n",
            "iter: 505\n",
            "loss: 0.8708790001128572\n",
            "\n",
            "iter: 506\n",
            "loss: 0.8706796340789895\n",
            "\n",
            "iter: 507\n",
            "loss: 0.8704799658662793\n",
            "\n",
            "iter: 508\n",
            "loss: 0.8702799951388365\n",
            "\n",
            "iter: 509\n",
            "loss: 0.8700797215565714\n",
            "\n",
            "iter: 510\n",
            "loss: 0.8698791447751155\n",
            "\n",
            "iter: 511\n",
            "loss: 0.8696782644457443\n",
            "\n",
            "iter: 512\n",
            "loss: 0.8694770802152979\n",
            "\n",
            "iter: 513\n",
            "loss: 0.8692755917261046\n",
            "\n",
            "iter: 514\n",
            "loss: 0.869073798615901\n",
            "\n",
            "iter: 515\n",
            "loss: 0.8688717005177568\n",
            "\n",
            "iter: 516\n",
            "loss: 0.8686692970599943\n",
            "\n",
            "iter: 517\n",
            "loss: 0.8684665878661146\n",
            "\n",
            "iter: 518\n",
            "loss: 0.8682635725547185\n",
            "\n",
            "iter: 519\n",
            "loss: 0.868060250739429\n",
            "\n",
            "iter: 520\n",
            "loss: 0.8678566220288167\n",
            "\n",
            "iter: 521\n",
            "loss: 0.8676526860263216\n",
            "\n",
            "iter: 522\n",
            "loss: 0.8674484423301775\n",
            "\n",
            "iter: 523\n",
            "loss: 0.8672438905333344\n",
            "\n",
            "iter: 524\n",
            "loss: 0.867039030223385\n",
            "\n",
            "iter: 525\n",
            "loss: 0.8668338609824866\n",
            "\n",
            "iter: 526\n",
            "loss: 0.866628382387286\n",
            "\n",
            "iter: 527\n",
            "loss: 0.8664225940088442\n",
            "\n",
            "iter: 528\n",
            "loss: 0.8662164954125603\n",
            "\n",
            "iter: 529\n",
            "loss: 0.8660100861580977\n",
            "\n",
            "iter: 530\n",
            "loss: 0.8658033657993065\n",
            "\n",
            "iter: 531\n",
            "loss: 0.8655963338841512\n",
            "\n",
            "iter: 532\n",
            "loss: 0.8653889899546343\n",
            "\n",
            "iter: 533\n",
            "loss: 0.8651813335467208\n",
            "\n",
            "iter: 534\n",
            "loss: 0.8649733641902673\n",
            "\n",
            "iter: 535\n",
            "loss: 0.8647650814089427\n",
            "\n",
            "iter: 536\n",
            "loss: 0.8645564847201569\n",
            "\n",
            "iter: 537\n",
            "loss: 0.8643475736349873\n",
            "\n",
            "iter: 538\n",
            "loss: 0.8641383476581034\n",
            "\n",
            "iter: 539\n",
            "loss: 0.8639288062876922\n",
            "\n",
            "iter: 540\n",
            "loss: 0.8637189490153864\n",
            "\n",
            "iter: 541\n",
            "loss: 0.8635087753261905\n",
            "\n",
            "iter: 542\n",
            "loss: 0.8632982846984067\n",
            "\n",
            "iter: 543\n",
            "loss: 0.8630874766035619\n",
            "\n",
            "iter: 544\n",
            "loss: 0.8628763505063356\n",
            "\n",
            "iter: 545\n",
            "loss: 0.8626649058644841\n",
            "\n",
            "iter: 546\n",
            "loss: 0.8624531421287724\n",
            "\n",
            "iter: 547\n",
            "loss: 0.862241058742897\n",
            "\n",
            "iter: 548\n",
            "loss: 0.8620286551434159\n",
            "\n",
            "iter: 549\n",
            "loss: 0.8618159307596752\n",
            "\n",
            "iter: 550\n",
            "loss: 0.8616028850137387\n",
            "\n",
            "iter: 551\n",
            "loss: 0.8613895173203128\n",
            "\n",
            "iter: 552\n",
            "loss: 0.8611758270866771\n",
            "\n",
            "iter: 553\n",
            "loss: 0.8609618137126117\n",
            "\n",
            "iter: 554\n",
            "loss: 0.860747476590325\n",
            "\n",
            "iter: 555\n",
            "loss: 0.8605328151043854\n",
            "\n",
            "iter: 556\n",
            "loss: 0.8603178286316446\n",
            "\n",
            "iter: 557\n",
            "loss: 0.8601025165411718\n",
            "\n",
            "iter: 558\n",
            "loss: 0.859886878194179\n",
            "\n",
            "iter: 559\n",
            "loss: 0.8596709129439526\n",
            "\n",
            "iter: 560\n",
            "loss: 0.859454620135781\n",
            "\n",
            "iter: 561\n",
            "loss: 0.8592379991068859\n",
            "\n",
            "iter: 562\n",
            "loss: 0.8590210491863506\n",
            "\n",
            "iter: 563\n",
            "loss: 0.85880376969505\n",
            "\n",
            "iter: 564\n",
            "loss: 0.858586159945582\n",
            "\n",
            "iter: 565\n",
            "loss: 0.8583682192421959\n",
            "\n",
            "iter: 566\n",
            "loss: 0.8581499468807232\n",
            "\n",
            "iter: 567\n",
            "loss: 0.8579313421485107\n",
            "\n",
            "iter: 568\n",
            "loss: 0.857712404324347\n",
            "\n",
            "iter: 569\n",
            "loss: 0.8574931326783972\n",
            "\n",
            "iter: 570\n",
            "loss: 0.857273526472132\n",
            "\n",
            "iter: 571\n",
            "loss: 0.857053584958259\n",
            "\n",
            "iter: 572\n",
            "loss: 0.8568333073806568\n",
            "\n",
            "iter: 573\n",
            "loss: 0.8566126929743026\n",
            "\n",
            "iter: 574\n",
            "loss: 0.8563917409652086\n",
            "\n",
            "iter: 575\n",
            "loss: 0.8561704505703503\n",
            "\n",
            "iter: 576\n",
            "loss: 0.855948820997602\n",
            "\n",
            "iter: 577\n",
            "loss: 0.8557268514456672\n",
            "\n",
            "iter: 578\n",
            "loss: 0.8555045411040124\n",
            "\n",
            "iter: 579\n",
            "loss: 0.8552818891528011\n",
            "\n",
            "iter: 580\n",
            "loss: 0.8550588947628244\n",
            "\n",
            "iter: 581\n",
            "loss: 0.8548355570954376\n",
            "\n",
            "iter: 582\n",
            "loss: 0.8546118753024922\n",
            "\n",
            "iter: 583\n",
            "loss: 0.8543878485262705\n",
            "\n",
            "iter: 584\n",
            "loss: 0.85416347589942\n",
            "\n",
            "iter: 585\n",
            "loss: 0.853938756544887\n",
            "\n",
            "iter: 586\n",
            "loss: 0.8537136895758531\n",
            "\n",
            "iter: 587\n",
            "loss: 0.8534882740956684\n",
            "\n",
            "iter: 588\n",
            "loss: 0.8532625091977893\n",
            "\n",
            "iter: 589\n",
            "loss: 0.8530363939657107\n",
            "\n",
            "iter: 590\n",
            "loss: 0.8528099274729055\n",
            "\n",
            "iter: 591\n",
            "loss: 0.8525831087827586\n",
            "\n",
            "iter: 592\n",
            "loss: 0.8523559369485043\n",
            "\n",
            "iter: 593\n",
            "loss: 0.8521284110131632\n",
            "\n",
            "iter: 594\n",
            "loss: 0.8519005300094784\n",
            "\n",
            "iter: 595\n",
            "loss: 0.8516722929598541\n",
            "\n",
            "iter: 596\n",
            "loss: 0.8514436988762939\n",
            "\n",
            "iter: 597\n",
            "loss: 0.8512147467603369\n",
            "\n",
            "iter: 598\n",
            "loss: 0.8509854356029978\n",
            "\n",
            "iter: 599\n",
            "loss: 0.8507557643847058\n",
            "\n",
            "iter: 600\n",
            "loss: 0.8505257320752431\n",
            "\n",
            "iter: 601\n",
            "loss: 0.850295337633685\n",
            "\n",
            "iter: 602\n",
            "loss: 0.8500645800083393\n",
            "\n",
            "iter: 603\n",
            "loss: 0.849833458136688\n",
            "\n",
            "iter: 604\n",
            "loss: 0.8496019709453267\n",
            "\n",
            "iter: 605\n",
            "loss: 0.8493701173499062\n",
            "\n",
            "iter: 606\n",
            "loss: 0.8491378962550745\n",
            "\n",
            "iter: 607\n",
            "loss: 0.8489053065544185\n",
            "\n",
            "iter: 608\n",
            "loss: 0.8486723471304064\n",
            "\n",
            "iter: 609\n",
            "loss: 0.8484390168543313\n",
            "\n",
            "iter: 610\n",
            "loss: 0.8482053145862538\n",
            "\n",
            "iter: 611\n",
            "loss: 0.8479712391749459\n",
            "\n",
            "iter: 612\n",
            "loss: 0.8477367894578363\n",
            "\n",
            "iter: 613\n",
            "loss: 0.8475019642609534\n",
            "\n",
            "iter: 614\n",
            "loss: 0.8472667623988732\n",
            "\n",
            "iter: 615\n",
            "loss: 0.8470311826746636\n",
            "\n",
            "iter: 616\n",
            "loss: 0.8467952238798304\n",
            "\n",
            "iter: 617\n",
            "loss: 0.8465588847942657\n",
            "\n",
            "iter: 618\n",
            "loss: 0.8463221641861939\n",
            "\n",
            "iter: 619\n",
            "loss: 0.8460850608121209\n",
            "\n",
            "iter: 620\n",
            "loss: 0.8458475734167826\n",
            "\n",
            "iter: 621\n",
            "loss: 0.845609700733094\n",
            "\n",
            "iter: 622\n",
            "loss: 0.8453714414820996\n",
            "\n",
            "iter: 623\n",
            "loss: 0.8451327943729228\n",
            "\n",
            "iter: 624\n",
            "loss: 0.844893758102717\n",
            "\n",
            "iter: 625\n",
            "loss: 0.8446543313566206\n",
            "\n",
            "iter: 626\n",
            "loss: 0.8444145128077045\n",
            "\n",
            "iter: 627\n",
            "loss: 0.8441743011169301\n",
            "\n",
            "iter: 628\n",
            "loss: 0.8439336949330999\n",
            "\n",
            "iter: 629\n",
            "loss: 0.843692692892813\n",
            "\n",
            "iter: 630\n",
            "loss: 0.8434512936204218\n",
            "\n",
            "iter: 631\n",
            "loss: 0.8432094957279871\n",
            "\n",
            "iter: 632\n",
            "loss: 0.8429672978152344\n",
            "\n",
            "iter: 633\n",
            "loss: 0.8427246984695128\n",
            "\n",
            "iter: 634\n",
            "loss: 0.842481696265753\n",
            "\n",
            "iter: 635\n",
            "loss: 0.8422382897664272\n",
            "\n",
            "iter: 636\n",
            "loss: 0.8419944775215086\n",
            "\n",
            "iter: 637\n",
            "loss: 0.8417502580684326\n",
            "\n",
            "iter: 638\n",
            "loss: 0.8415056299320596\n",
            "\n",
            "iter: 639\n",
            "loss: 0.8412605916246372\n",
            "\n",
            "iter: 640\n",
            "loss: 0.8410151416457647\n",
            "\n",
            "iter: 641\n",
            "loss: 0.8407692784823569\n",
            "\n",
            "iter: 642\n",
            "loss: 0.8405230006086107\n",
            "\n",
            "iter: 643\n",
            "loss: 0.8402763064859723\n",
            "\n",
            "iter: 644\n",
            "loss: 0.8400291945631042\n",
            "\n",
            "iter: 645\n",
            "loss: 0.8397816632758541\n",
            "\n",
            "iter: 646\n",
            "loss: 0.8395337110472252\n",
            "\n",
            "iter: 647\n",
            "loss: 0.8392853362873478\n",
            "\n",
            "iter: 648\n",
            "loss: 0.8390365373934509\n",
            "\n",
            "iter: 649\n",
            "loss: 0.8387873127498343\n",
            "\n",
            "iter: 650\n",
            "loss: 0.8385376607278462\n",
            "\n",
            "iter: 651\n",
            "loss: 0.8382875796858562\n",
            "\n",
            "iter: 652\n",
            "loss: 0.8380370679692327\n",
            "\n",
            "iter: 653\n",
            "loss: 0.8377861239103216\n",
            "\n",
            "iter: 654\n",
            "loss: 0.8375347458284264\n",
            "\n",
            "iter: 655\n",
            "loss: 0.8372829320297871\n",
            "\n",
            "iter: 656\n",
            "loss: 0.8370306808075643\n",
            "\n",
            "iter: 657\n",
            "loss: 0.8367779904418206\n",
            "\n",
            "iter: 658\n",
            "loss: 0.8365248591995077\n",
            "\n",
            "iter: 659\n",
            "loss: 0.8362712853344507\n",
            "\n",
            "iter: 660\n",
            "loss: 0.8360172670873365\n",
            "\n",
            "iter: 661\n",
            "loss: 0.8357628026857036\n",
            "\n",
            "iter: 662\n",
            "loss: 0.8355078903439319\n",
            "\n",
            "iter: 663\n",
            "loss: 0.8352525282632353\n",
            "\n",
            "iter: 664\n",
            "loss: 0.8349967146316557\n",
            "\n",
            "iter: 665\n",
            "loss: 0.8347404476240576\n",
            "\n",
            "iter: 666\n",
            "loss: 0.8344837254021261\n",
            "\n",
            "iter: 667\n",
            "loss: 0.8342265461143648\n",
            "\n",
            "iter: 668\n",
            "loss: 0.8339689078960972\n",
            "\n",
            "iter: 669\n",
            "loss: 0.8337108088694671\n",
            "\n",
            "iter: 670\n",
            "loss: 0.8334522471434451\n",
            "\n",
            "iter: 671\n",
            "loss: 0.833193220813831\n",
            "\n",
            "iter: 672\n",
            "loss: 0.832933727963264\n",
            "\n",
            "iter: 673\n",
            "loss: 0.8326737666612315\n",
            "\n",
            "iter: 674\n",
            "loss: 0.8324133349640792\n",
            "\n",
            "iter: 675\n",
            "loss: 0.8321524309150258\n",
            "\n",
            "iter: 676\n",
            "loss: 0.8318910525441773\n",
            "\n",
            "iter: 677\n",
            "loss: 0.8316291978685448\n",
            "\n",
            "iter: 678\n",
            "loss: 0.8313668648920638\n",
            "\n",
            "iter: 679\n",
            "loss: 0.8311040516056137\n",
            "\n",
            "iter: 680\n",
            "loss: 0.8308407559870443\n",
            "\n",
            "iter: 681\n",
            "loss: 0.8305769760012007\n",
            "\n",
            "iter: 682\n",
            "loss: 0.8303127095999481\n",
            "\n",
            "iter: 683\n",
            "loss: 0.8300479547222064\n",
            "\n",
            "iter: 684\n",
            "loss: 0.8297827092939789\n",
            "\n",
            "iter: 685\n",
            "loss: 0.8295169712283893\n",
            "\n",
            "iter: 686\n",
            "loss: 0.8292507384257178\n",
            "\n",
            "iter: 687\n",
            "loss: 0.8289840087734401\n",
            "\n",
            "iter: 688\n",
            "loss: 0.828716780146271\n",
            "\n",
            "iter: 689\n",
            "loss: 0.8284490504062058\n",
            "\n",
            "iter: 690\n",
            "loss: 0.8281808174025705\n",
            "\n",
            "iter: 691\n",
            "loss: 0.8279120789720688\n",
            "\n",
            "iter: 692\n",
            "loss: 0.8276428329388352\n",
            "\n",
            "iter: 693\n",
            "loss: 0.8273730771144888\n",
            "\n",
            "iter: 694\n",
            "loss: 0.8271028092981922\n",
            "\n",
            "iter: 695\n",
            "loss: 0.8268320272767097\n",
            "\n",
            "iter: 696\n",
            "loss: 0.8265607288244713\n",
            "\n",
            "iter: 697\n",
            "loss: 0.8262889117036385\n",
            "\n",
            "iter: 698\n",
            "loss: 0.826016573664171\n",
            "\n",
            "iter: 699\n",
            "loss: 0.8257437124439003\n",
            "\n",
            "iter: 700\n",
            "loss: 0.8254703257686038\n",
            "\n",
            "iter: 701\n",
            "loss: 0.8251964113520791\n",
            "\n",
            "iter: 702\n",
            "loss: 0.8249219668962285\n",
            "\n",
            "iter: 703\n",
            "loss: 0.8246469900911394\n",
            "\n",
            "iter: 704\n",
            "loss: 0.824371478615171\n",
            "\n",
            "iter: 705\n",
            "loss: 0.8240954301350463\n",
            "\n",
            "iter: 706\n",
            "loss: 0.8238188423059416\n",
            "\n",
            "iter: 707\n",
            "loss: 0.8235417127715849\n",
            "\n",
            "iter: 708\n",
            "loss: 0.8232640391643536\n",
            "\n",
            "iter: 709\n",
            "loss: 0.8229858191053785\n",
            "\n",
            "iter: 710\n",
            "loss: 0.8227070502046484\n",
            "\n",
            "iter: 711\n",
            "loss: 0.8224277300611206\n",
            "\n",
            "iter: 712\n",
            "loss: 0.8221478562628325\n",
            "\n",
            "iter: 713\n",
            "loss: 0.8218674263870192\n",
            "\n",
            "iter: 714\n",
            "loss: 0.8215864380002323\n",
            "\n",
            "iter: 715\n",
            "loss: 0.821304888658463\n",
            "\n",
            "iter: 716\n",
            "loss: 0.8210227759072702\n",
            "\n",
            "iter: 717\n",
            "loss: 0.8207400972819104\n",
            "\n",
            "iter: 718\n",
            "loss: 0.8204568503074722\n",
            "\n",
            "iter: 719\n",
            "loss: 0.8201730324990135\n",
            "\n",
            "iter: 720\n",
            "loss: 0.8198886413617049\n",
            "\n",
            "iter: 721\n",
            "loss: 0.8196036743909734\n",
            "\n",
            "iter: 722\n",
            "loss: 0.8193181290726531\n",
            "\n",
            "iter: 723\n",
            "loss: 0.8190320028831385\n",
            "\n",
            "iter: 724\n",
            "loss: 0.8187452932895409\n",
            "\n",
            "iter: 725\n",
            "loss: 0.8184579977498497\n",
            "\n",
            "iter: 726\n",
            "loss: 0.8181701137130993\n",
            "\n",
            "iter: 727\n",
            "loss: 0.8178816386195357\n",
            "\n",
            "iter: 728\n",
            "loss: 0.8175925699007913\n",
            "\n",
            "iter: 729\n",
            "loss: 0.8173029049800622\n",
            "\n",
            "iter: 730\n",
            "loss: 0.8170126412722895\n",
            "\n",
            "iter: 731\n",
            "loss: 0.8167217761843446\n",
            "\n",
            "iter: 732\n",
            "loss: 0.8164303071152199\n",
            "\n",
            "iter: 733\n",
            "loss: 0.8161382314562219\n",
            "\n",
            "iter: 734\n",
            "loss: 0.8158455465911714\n",
            "\n",
            "iter: 735\n",
            "loss: 0.815552249896604\n",
            "\n",
            "iter: 736\n",
            "loss: 0.8152583387419773\n",
            "\n",
            "iter: 737\n",
            "loss: 0.8149638104898846\n",
            "\n",
            "iter: 738\n",
            "loss: 0.8146686624962682\n",
            "\n",
            "iter: 739\n",
            "loss: 0.8143728921106407\n",
            "\n",
            "iter: 740\n",
            "loss: 0.8140764966763088\n",
            "\n",
            "iter: 741\n",
            "loss: 0.8137794735306025\n",
            "\n",
            "iter: 742\n",
            "loss: 0.8134818200051103\n",
            "\n",
            "iter: 743\n",
            "loss: 0.8131835334259143\n",
            "\n",
            "iter: 744\n",
            "loss: 0.8128846111138348\n",
            "\n",
            "iter: 745\n",
            "loss: 0.8125850503846768\n",
            "\n",
            "iter: 746\n",
            "loss: 0.8122848485494819\n",
            "\n",
            "iter: 747\n",
            "loss: 0.8119840029147835\n",
            "\n",
            "iter: 748\n",
            "loss: 0.8116825107828683\n",
            "\n",
            "iter: 749\n",
            "loss: 0.8113803694520427\n",
            "\n",
            "iter: 750\n",
            "loss: 0.8110775762169005\n",
            "\n",
            "iter: 751\n",
            "loss: 0.8107741283686002\n",
            "\n",
            "iter: 752\n",
            "loss: 0.8104700231951416\n",
            "\n",
            "iter: 753\n",
            "loss: 0.8101652579816513\n",
            "\n",
            "iter: 754\n",
            "loss: 0.8098598300106719\n",
            "\n",
            "iter: 755\n",
            "loss: 0.8095537365624533\n",
            "\n",
            "iter: 756\n",
            "loss: 0.8092469749152515\n",
            "\n",
            "iter: 757\n",
            "loss: 0.8089395423456318\n",
            "\n",
            "iter: 758\n",
            "loss: 0.8086314361287749\n",
            "\n",
            "iter: 759\n",
            "loss: 0.8083226535387886\n",
            "\n",
            "iter: 760\n",
            "loss: 0.808013191849026\n",
            "\n",
            "iter: 761\n",
            "loss: 0.8077030483324052\n",
            "\n",
            "iter: 762\n",
            "loss: 0.8073922202617335\n",
            "\n",
            "iter: 763\n",
            "loss: 0.8070807049100421\n",
            "\n",
            "iter: 764\n",
            "loss: 0.8067684995509158\n",
            "\n",
            "iter: 765\n",
            "loss: 0.806455601458837\n",
            "\n",
            "iter: 766\n",
            "loss: 0.806142007909527\n",
            "\n",
            "iter: 767\n",
            "loss: 0.8058277161802957\n",
            "\n",
            "iter: 768\n",
            "loss: 0.8055127235503948\n",
            "\n",
            "iter: 769\n",
            "loss: 0.8051970273013751\n",
            "\n",
            "iter: 770\n",
            "loss: 0.8048806247174486\n",
            "\n",
            "iter: 771\n",
            "loss: 0.8045635130858559\n",
            "\n",
            "iter: 772\n",
            "loss: 0.8042456896972358\n",
            "\n",
            "iter: 773\n",
            "loss: 0.8039271518460018\n",
            "\n",
            "iter: 774\n",
            "loss: 0.8036078968307219\n",
            "\n",
            "iter: 775\n",
            "loss: 0.803287921954501\n",
            "\n",
            "iter: 776\n",
            "loss: 0.8029672245253721\n",
            "\n",
            "iter: 777\n",
            "loss: 0.8026458018566844\n",
            "\n",
            "iter: 778\n",
            "loss: 0.8023236512675052\n",
            "\n",
            "iter: 779\n",
            "loss: 0.8020007700830155\n",
            "\n",
            "iter: 780\n",
            "loss: 0.8016771556349191\n",
            "\n",
            "iter: 781\n",
            "loss: 0.8013528052618492\n",
            "\n",
            "iter: 782\n",
            "loss: 0.8010277163097803\n",
            "\n",
            "iter: 783\n",
            "loss: 0.8007018861324475\n",
            "\n",
            "iter: 784\n",
            "loss: 0.8003753120917646\n",
            "\n",
            "iter: 785\n",
            "loss: 0.8000479915582495\n",
            "\n",
            "iter: 786\n",
            "loss: 0.7997199219114512\n",
            "\n",
            "iter: 787\n",
            "loss: 0.7993911005403835\n",
            "\n",
            "iter: 788\n",
            "loss: 0.7990615248439565\n",
            "\n",
            "iter: 789\n",
            "loss: 0.7987311922314182\n",
            "\n",
            "iter: 790\n",
            "loss: 0.7984001001227947\n",
            "\n",
            "iter: 791\n",
            "loss: 0.7980682459493352\n",
            "\n",
            "iter: 792\n",
            "loss: 0.7977356271539624\n",
            "\n",
            "iter: 793\n",
            "loss: 0.7974022411917203\n",
            "\n",
            "iter: 794\n",
            "loss: 0.7970680855302332\n",
            "\n",
            "iter: 795\n",
            "loss: 0.7967331576501584\n",
            "\n",
            "iter: 796\n",
            "loss: 0.79639745504565\n",
            "\n",
            "iter: 797\n",
            "loss: 0.79606097522482\n",
            "\n",
            "iter: 798\n",
            "loss: 0.7957237157102044\n",
            "\n",
            "iter: 799\n",
            "loss: 0.7953856740392304\n",
            "\n",
            "iter: 800\n",
            "loss: 0.7950468477646879\n",
            "\n",
            "iter: 801\n",
            "loss: 0.7947072344552013\n",
            "\n",
            "iter: 802\n",
            "loss: 0.7943668316957053\n",
            "\n",
            "iter: 803\n",
            "loss: 0.79402563708792\n",
            "\n",
            "iter: 804\n",
            "loss: 0.7936836482508315\n",
            "\n",
            "iter: 805\n",
            "loss: 0.7933408628211704\n",
            "\n",
            "iter: 806\n",
            "loss: 0.7929972784538963\n",
            "\n",
            "iter: 807\n",
            "loss: 0.7926528928226796\n",
            "\n",
            "iter: 808\n",
            "loss: 0.7923077036203884\n",
            "\n",
            "iter: 809\n",
            "loss: 0.7919617085595707\n",
            "\n",
            "iter: 810\n",
            "loss: 0.7916149053729469\n",
            "\n",
            "iter: 811\n",
            "loss: 0.7912672918138965\n",
            "\n",
            "iter: 812\n",
            "loss: 0.7909188656569439\n",
            "\n",
            "iter: 813\n",
            "loss: 0.7905696246982533\n",
            "\n",
            "iter: 814\n",
            "loss: 0.7902195667561149\n",
            "\n",
            "iter: 815\n",
            "loss: 0.7898686896714381\n",
            "\n",
            "iter: 816\n",
            "loss: 0.7895169913082414\n",
            "\n",
            "iter: 817\n",
            "loss: 0.7891644695541423\n",
            "\n",
            "iter: 818\n",
            "loss: 0.7888111223208503\n",
            "\n",
            "iter: 819\n",
            "loss: 0.788456947544655\n",
            "\n",
            "iter: 820\n",
            "loss: 0.7881019431869174\n",
            "\n",
            "iter: 821\n",
            "loss: 0.787746107234559\n",
            "\n",
            "iter: 822\n",
            "loss: 0.7873894377005491\n",
            "\n",
            "iter: 823\n",
            "loss: 0.7870319326243955\n",
            "\n",
            "iter: 824\n",
            "loss: 0.7866735900726275\n",
            "\n",
            "iter: 825\n",
            "loss: 0.7863144081392822\n",
            "\n",
            "iter: 826\n",
            "loss: 0.7859543849463885\n",
            "\n",
            "iter: 827\n",
            "loss: 0.7855935186444495\n",
            "\n",
            "iter: 828\n",
            "loss: 0.7852318074129193\n",
            "\n",
            "iter: 829\n",
            "loss: 0.7848692494606857\n",
            "\n",
            "iter: 830\n",
            "loss: 0.7845058430265422\n",
            "\n",
            "iter: 831\n",
            "loss: 0.7841415863796629\n",
            "\n",
            "iter: 832\n",
            "loss: 0.7837764778200728\n",
            "\n",
            "iter: 833\n",
            "loss: 0.7834105156791158\n",
            "\n",
            "iter: 834\n",
            "loss: 0.783043698319919\n",
            "\n",
            "iter: 835\n",
            "loss: 0.7826760241378558\n",
            "\n",
            "iter: 836\n",
            "loss: 0.7823074915610017\n",
            "\n",
            "iter: 837\n",
            "loss: 0.7819380990505924\n",
            "\n",
            "iter: 838\n",
            "loss: 0.7815678451014716\n",
            "\n",
            "iter: 839\n",
            "loss: 0.7811967282425404\n",
            "\n",
            "iter: 840\n",
            "loss: 0.7808247470371995\n",
            "\n",
            "iter: 841\n",
            "loss: 0.780451900083787\n",
            "\n",
            "iter: 842\n",
            "loss: 0.7800781860160126\n",
            "\n",
            "iter: 843\n",
            "loss: 0.7797036035033893\n",
            "\n",
            "iter: 844\n",
            "loss: 0.7793281512516544\n",
            "\n",
            "iter: 845\n",
            "loss: 0.778951828003191\n",
            "\n",
            "iter: 846\n",
            "loss: 0.7785746325374422\n",
            "\n",
            "iter: 847\n",
            "loss: 0.7781965636713183\n",
            "\n",
            "iter: 848\n",
            "loss: 0.7778176202596009\n",
            "\n",
            "iter: 849\n",
            "loss: 0.77743780119534\n",
            "\n",
            "iter: 850\n",
            "loss: 0.777057105410244\n",
            "\n",
            "iter: 851\n",
            "loss: 0.7766755318750662\n",
            "\n",
            "iter: 852\n",
            "loss: 0.7762930795999816\n",
            "\n",
            "iter: 853\n",
            "loss: 0.7759097476349609\n",
            "\n",
            "iter: 854\n",
            "loss: 0.7755255350701326\n",
            "\n",
            "iter: 855\n",
            "loss: 0.7751404410361443\n",
            "\n",
            "iter: 856\n",
            "loss: 0.7747544647045115\n",
            "\n",
            "iter: 857\n",
            "loss: 0.7743676052879634\n",
            "\n",
            "iter: 858\n",
            "loss: 0.7739798620407768\n",
            "\n",
            "iter: 859\n",
            "loss: 0.7735912342591079\n",
            "\n",
            "iter: 860\n",
            "loss: 0.7732017212813104\n",
            "\n",
            "iter: 861\n",
            "loss: 0.772811322488251\n",
            "\n",
            "iter: 862\n",
            "loss: 0.7724200373036116\n",
            "\n",
            "iter: 863\n",
            "loss: 0.7720278651941894\n",
            "\n",
            "iter: 864\n",
            "loss: 0.7716348056701828\n",
            "\n",
            "iter: 865\n",
            "loss: 0.7712408582854712\n",
            "\n",
            "iter: 866\n",
            "loss: 0.7708460226378876\n",
            "\n",
            "iter: 867\n",
            "loss: 0.77045029836948\n",
            "\n",
            "iter: 868\n",
            "loss: 0.7700536851667641\n",
            "\n",
            "iter: 869\n",
            "loss: 0.7696561827609687\n",
            "\n",
            "iter: 870\n",
            "loss: 0.7692577909282697\n",
            "\n",
            "iter: 871\n",
            "loss: 0.7688585094900167\n",
            "\n",
            "iter: 872\n",
            "loss: 0.7684583383129487\n",
            "\n",
            "iter: 873\n",
            "loss: 0.7680572773094013\n",
            "\n",
            "iter: 874\n",
            "loss: 0.7676553264375028\n",
            "\n",
            "iter: 875\n",
            "loss: 0.767252485701361\n",
            "\n",
            "iter: 876\n",
            "loss: 0.766848755151243\n",
            "\n",
            "iter: 877\n",
            "loss: 0.7664441348837393\n",
            "\n",
            "iter: 878\n",
            "loss: 0.7660386250419249\n",
            "\n",
            "iter: 879\n",
            "loss: 0.7656322258155033\n",
            "\n",
            "iter: 880\n",
            "loss: 0.7652249374409436\n",
            "\n",
            "iter: 881\n",
            "loss: 0.7648167602016089\n",
            "\n",
            "iter: 882\n",
            "loss: 0.7644076944278734\n",
            "\n",
            "iter: 883\n",
            "loss: 0.7639977404972231\n",
            "\n",
            "iter: 884\n",
            "loss: 0.7635868988343585\n",
            "\n",
            "iter: 885\n",
            "loss: 0.7631751699112747\n",
            "\n",
            "iter: 886\n",
            "loss: 0.7627625542473361\n",
            "\n",
            "iter: 887\n",
            "loss: 0.7623490524093419\n",
            "\n",
            "iter: 888\n",
            "loss: 0.7619346650115788\n",
            "\n",
            "iter: 889\n",
            "loss: 0.7615193927158621\n",
            "\n",
            "iter: 890\n",
            "loss: 0.761103236231569\n",
            "\n",
            "iter: 891\n",
            "loss: 0.7606861963156594\n",
            "\n",
            "iter: 892\n",
            "loss: 0.7602682737726855\n",
            "\n",
            "iter: 893\n",
            "loss: 0.7598494694547918\n",
            "\n",
            "iter: 894\n",
            "loss: 0.7594297842617049\n",
            "\n",
            "iter: 895\n",
            "loss: 0.7590092191407108\n",
            "\n",
            "iter: 896\n",
            "loss: 0.7585877750866235\n",
            "\n",
            "iter: 897\n",
            "loss: 0.7581654531417399\n",
            "\n",
            "iter: 898\n",
            "loss: 0.7577422543957882\n",
            "\n",
            "iter: 899\n",
            "loss: 0.757318179985862\n",
            "\n",
            "iter: 900\n",
            "loss: 0.7568932310963465\n",
            "\n",
            "iter: 901\n",
            "loss: 0.7564674089588321\n",
            "\n",
            "iter: 902\n",
            "loss: 0.7560407148520188\n",
            "\n",
            "iter: 903\n",
            "loss: 0.75561315010161\n",
            "\n",
            "iter: 904\n",
            "loss: 0.7551847160801946\n",
            "\n",
            "iter: 905\n",
            "loss: 0.7547554142071206\n",
            "\n",
            "iter: 906\n",
            "loss: 0.7543252459483573\n",
            "\n",
            "iter: 907\n",
            "loss: 0.7538942128163484\n",
            "\n",
            "iter: 908\n",
            "loss: 0.7534623163698525\n",
            "\n",
            "iter: 909\n",
            "loss: 0.7530295582137762\n",
            "\n",
            "iter: 910\n",
            "loss: 0.7525959399989971\n",
            "\n",
            "iter: 911\n",
            "loss: 0.7521614634221744\n",
            "\n",
            "iter: 912\n",
            "loss: 0.7517261302255519\n",
            "\n",
            "iter: 913\n",
            "loss: 0.7512899421967506\n",
            "\n",
            "iter: 914\n",
            "loss: 0.7508529011685525\n",
            "\n",
            "iter: 915\n",
            "loss: 0.7504150090186739\n",
            "\n",
            "iter: 916\n",
            "loss: 0.7499762676695287\n",
            "\n",
            "iter: 917\n",
            "loss: 0.7495366790879845\n",
            "\n",
            "iter: 918\n",
            "loss: 0.749096245285108\n",
            "\n",
            "iter: 919\n",
            "loss: 0.7486549683158992\n",
            "\n",
            "iter: 920\n",
            "loss: 0.7482128502790243\n",
            "\n",
            "iter: 921\n",
            "loss: 0.7477698933165308\n",
            "\n",
            "iter: 922\n",
            "loss: 0.7473260996135562\n",
            "\n",
            "iter: 923\n",
            "loss: 0.7468814713980328\n",
            "\n",
            "iter: 924\n",
            "loss: 0.7464360109403807\n",
            "\n",
            "iter: 925\n",
            "loss: 0.7459897205531894\n",
            "\n",
            "iter: 926\n",
            "loss: 0.7455426025908962\n",
            "\n",
            "iter: 927\n",
            "loss: 0.7450946594494562\n",
            "\n",
            "iter: 928\n",
            "loss: 0.7446458935659986\n",
            "\n",
            "iter: 929\n",
            "loss: 0.7441963074184835\n",
            "\n",
            "iter: 930\n",
            "loss: 0.743745903525344\n",
            "\n",
            "iter: 931\n",
            "loss: 0.7432946844451241\n",
            "\n",
            "iter: 932\n",
            "loss: 0.7428426527761101\n",
            "\n",
            "iter: 933\n",
            "loss: 0.7423898111559506\n",
            "\n",
            "iter: 934\n",
            "loss: 0.7419361622612745\n",
            "\n",
            "iter: 935\n",
            "loss: 0.7414817088072985\n",
            "\n",
            "iter: 936\n",
            "loss: 0.7410264535474271\n",
            "\n",
            "iter: 937\n",
            "loss: 0.7405703992728498\n",
            "\n",
            "iter: 938\n",
            "loss: 0.7401135488121284\n",
            "\n",
            "iter: 939\n",
            "loss: 0.7396559050307777\n",
            "\n",
            "iter: 940\n",
            "loss: 0.739197470830843\n",
            "\n",
            "iter: 941\n",
            "loss: 0.7387382491504673\n",
            "\n",
            "iter: 942\n",
            "loss: 0.7382782429634553\n",
            "\n",
            "iter: 943\n",
            "loss: 0.7378174552788307\n",
            "\n",
            "iter: 944\n",
            "loss: 0.7373558891403871\n",
            "\n",
            "iter: 945\n",
            "loss: 0.7368935476262337\n",
            "\n",
            "iter: 946\n",
            "loss: 0.7364304338483358\n",
            "\n",
            "iter: 947\n",
            "loss: 0.73596655095205\n",
            "\n",
            "iter: 948\n",
            "loss: 0.7355019021156519\n",
            "\n",
            "iter: 949\n",
            "loss: 0.7350364905498626\n",
            "\n",
            "iter: 950\n",
            "loss: 0.7345703194973662\n",
            "\n",
            "iter: 951\n",
            "loss: 0.7341033922323246\n",
            "\n",
            "iter: 952\n",
            "loss: 0.7336357120598882\n",
            "\n",
            "iter: 953\n",
            "loss: 0.7331672823156984\n",
            "\n",
            "iter: 954\n",
            "loss: 0.7326981063653903\n",
            "\n",
            "iter: 955\n",
            "loss: 0.7322281876040879\n",
            "\n",
            "iter: 956\n",
            "loss: 0.7317575294558948\n",
            "\n",
            "iter: 957\n",
            "loss: 0.7312861353733815\n",
            "\n",
            "iter: 958\n",
            "loss: 0.7308140088370708\n",
            "\n",
            "iter: 959\n",
            "loss: 0.730341153354915\n",
            "\n",
            "iter: 960\n",
            "loss: 0.729867572461772\n",
            "\n",
            "iter: 961\n",
            "loss: 0.7293932697188775\n",
            "\n",
            "iter: 962\n",
            "loss: 0.7289182487133119\n",
            "\n",
            "iter: 963\n",
            "loss: 0.7284425130574657\n",
            "\n",
            "iter: 964\n",
            "loss: 0.7279660663885004\n",
            "\n",
            "iter: 965\n",
            "loss: 0.727488912367806\n",
            "\n",
            "iter: 966\n",
            "loss: 0.727011054680456\n",
            "\n",
            "iter: 967\n",
            "loss: 0.7265324970346574\n",
            "\n",
            "iter: 968\n",
            "loss: 0.7260532431612005\n",
            "\n",
            "iter: 969\n",
            "loss: 0.7255732968129035\n",
            "\n",
            "iter: 970\n",
            "loss: 0.7250926617640543\n",
            "\n",
            "iter: 971\n",
            "loss: 0.7246113418098513\n",
            "\n",
            "iter: 972\n",
            "loss: 0.7241293407658403\n",
            "\n",
            "iter: 973\n",
            "loss: 0.7236466624673479\n",
            "\n",
            "iter: 974\n",
            "loss: 0.7231633107689134\n",
            "\n",
            "iter: 975\n",
            "loss: 0.7226792895437206\n",
            "\n",
            "iter: 976\n",
            "loss: 0.7221946026830213\n",
            "\n",
            "iter: 977\n",
            "loss: 0.7217092540955631\n",
            "\n",
            "iter: 978\n",
            "loss: 0.721223247707011\n",
            "\n",
            "iter: 979\n",
            "loss: 0.7207365874593675\n",
            "\n",
            "iter: 980\n",
            "loss: 0.7202492773103921\n",
            "\n",
            "iter: 981\n",
            "loss: 0.719761321233018\n",
            "\n",
            "iter: 982\n",
            "loss: 0.7192727232147655\n",
            "\n",
            "iter: 983\n",
            "loss: 0.7187834872571578\n",
            "\n",
            "iter: 984\n",
            "loss: 0.7182936173751285\n",
            "\n",
            "iter: 985\n",
            "loss: 0.7178031175964351\n",
            "\n",
            "iter: 986\n",
            "loss: 0.7173119919610649\n",
            "\n",
            "iter: 987\n",
            "loss: 0.7168202445206421\n",
            "\n",
            "iter: 988\n",
            "loss: 0.7163278793378333\n",
            "\n",
            "iter: 989\n",
            "loss: 0.715834900485751\n",
            "\n",
            "iter: 990\n",
            "loss: 0.7153413120473573\n",
            "\n",
            "iter: 991\n",
            "loss: 0.7148471181148636\n",
            "\n",
            "iter: 992\n",
            "loss: 0.7143523227891322\n",
            "\n",
            "iter: 993\n",
            "loss: 0.7138569301790756\n",
            "\n",
            "iter: 994\n",
            "loss: 0.713360944401053\n",
            "\n",
            "iter: 995\n",
            "loss: 0.7128643695782704\n",
            "\n",
            "iter: 996\n",
            "loss: 0.7123672098401743\n",
            "\n",
            "iter: 997\n",
            "loss: 0.7118694693218487\n",
            "\n",
            "iter: 998\n",
            "loss: 0.7113711521634094\n",
            "\n",
            "iter: 999\n",
            "loss: 0.7108722625093996\n",
            "\n",
            "iter: 1000\n",
            "loss: 0.710372804508181\n",
            "\n",
            "\n",
            "Model results\n",
            "-------------\n",
            "res: 0.02182831815582451\n",
            "bc1: 2.9485092191406947e-05\n",
            "bc2: 0.028282121899724524\n",
            "bc3: 0.0041378518204213946\n",
            "bc4: 0.6555950053431667\n",
            "\n",
            "Model trained in 00:01:20.262 (wall time)\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXzU9Z348dd7JjMTCHc4JYQECEe4JYQbT5RVC25rt1S7atVFa+1uj93f2p+utvbQVbetv9a2UnVrW61Wq21atdYLuYUgiJBwhENIsBwJ4UxmMpn374/vQIZ0gAD5Zo68n48HD+Z7zby/JOSd9/dziapijDHGNOdJdADGGGOSkyUIY4wxcVmCMMYYE5clCGOMMXFZgjDGGBOXJQhjjDFxuZogRGS2iGwSkQoRuTvO8TtE5CMRWSsiS0SkMLo/T0TqovvXisjP3YzTGGPM3xO3xkGIiBfYDMwCKoFVwOdVtSzmnC6qeij6eg5wp6rOFpE84M+qOsqV4IwxxpyRmxVEMVChqttUNQQ8D8yNPeF4cojKAmzUnjHGJIkMF9+7P7ArZrsSmNT8JBH5MvB1wA9cGnMoX0TWAIeAe1V18ek+rGfPnpqXl3e+MRtjTLuyevXq/araK94xNxNEi6jq48DjInI9cC9wE/AJkKuq1SIyAfiDiIxsVnEgIvOB+QC5ubmUlpa2cfTGGJPaROTjUx1z8xFTFTAgZjsnuu9UngeuBVDVoKpWR1+vBrYCQ5tfoKoLVLVIVYt69YqbAI0xxpwjNxPEKqBARPJFxA/MA0piTxCRgpjNq4Et0f29oo3ciMggoADY5mKsxhhjmnHtEZOqhkXkLuANwAs8raobROQBoFRVS4C7RORyoAE4gPN4CWAm8ICINAAR4A5VrXErVmOMMX/PtW6uba2oqEibt0E0NDRQWVlJfX19gqJKDpmZmeTk5ODz+RIdijEmyYjIalUtincs4Y3UbqqsrKRz587k5eUhIokOJyFUlerqaiorK8nPz090OMaYFJLWU23U19eTnZ3dbpMDgIiQnZ3d7qsoY8zZS+sEAbTr5HCc/RsYY85F2icIY4wx58YSRArJy8tj//79532OMSYNNIbgwIeufoQlCGOMSTWN9bD4M/DmNNi3zLWPsQThsh07djB8+HBuvvlmhg4dyg033MBbb73FtGnTKCgoYOXKldTU1HDttdcyZswYJk+ezLp16wCorq7miiuuYOTIkdx2223Edkn+zW9+Q3FxMePGjeP222+nsbExUbdojGlL4aPw3qdg95/BEwBvwLWPSuturrH+5afujLP7xZ09znhORUUFL774Ik8//TQTJ07kueeeY8mSJZSUlPD973+fAQMGMH78eP7whz/wzjvvcOONN7J27Vq+/e1vM336dO677z5effVVnnrqKQDKy8t54YUXWLp0KT6fjzvvvJNnn32WG2+80ZV7NMYkiYZDsPBq2LcEMvvApW9BN/dWRWg3CSKR8vPzGT16NAAjR47ksssuQ0QYPXo0O3bs4OOPP+b3v/89AJdeeinV1dUcOnSIRYsW8fLLLwNw9dVX0717dwDefvttVq9ezcSJEwGoq6ujd+/eCbgzY0ybCdbAu1dCTSl0zIFL34YufzdFXatqNwmiJb/puyUQaCoBPR7PiW2Px0M4HD7rEc6qyk033cSDDz7YqnEaY5KURuDd2U5y6DTIqRw6uT/w1dogksCMGTN49tlnAVi4cCE9e/akS5cuzJw5k+eeew6A119/nQMHDgBw2WWX8dJLL7F3714Aampq+PjjU87Ya4xJdeKBUfdCt9Fw+aI2SQ7QjiqIZPatb32LW265hTFjxtCxY0eeeeYZAO6//34+//nPM3LkSKZOnUpubi4AhYWFfPe73+WKK64gEong8/l4/PHHGThwYCJvwxjT2iJh8ER/TOfMgQuuatpuA2k9WV95eTkjRoxIUETJxf4tjEkxB8vhvTkw+SnoPdO1jzndZH32iMkYY5LNgbXw1kVwpALKHklYGPaIyRhjksn+950G6YZa6HclTH8hYaFYBWGMMcliz3vwzuVOcsi5Fmb+ETI6JiwcSxDGGJMMdr8BC2dD+AgMvB6m/87VUdItYQnCGGOSQgS0EQbfBlN+BZ7ErwBpbRDGGJMMLvgHuHIldBsLSbKGi1UQLqutreWnP/3pWV/3y1/+kt27d5/Ytmm8jUlDW5+Cv73VtN19XNIkB3A5QYjIbBHZJCIVInJ3nON3iMhHIrJWRJaISGHMsW9Gr9skIle6GaebTpUgwuHwaa9rniCMMWlm42Pw/m3w3lw4VpXoaOJy7RGTiHiBx4FZQCWwSkRKVLUs5rTnVPXn0fPnAD8AZkcTxTxgJHAB8JaIDFXVlJvT+u6772br1q2MGzcOn89HZmYm3bt3Z+PGjfz1r3/lmmuuYf369QA8+uijHDlyhFGjRlFaWsoNN9xAhw4dWL58OQA//vGP+dOf/kRDQwMvvvgiw4cPT+StGWPO1Ybvw4f3OK/HPQgd+yc2nlNwsw2iGKhQ1W0AIvI8MBc4kSBU9VDM+VnA8WHdc4HnVTUIbBeRiuj7LT+viJ47TelW/AQMme+8rlgAK28/9bnXt3z0+UMPPcT69etZu3YtCxcu5Oqrr2b9+vXk5+ezY8eOuNdcd911/OQnP+HRRx+lqKhpgGPPnj354IMP+OlPf8qjjz7Kk08+2eI4jDFJQNVJDGUPAgKTfgGDb010VKfk5iOm/sCumO3K6L6TiMiXRWQr8DDwr2dzbSoqLi4mP//cJtr69Kc/DcCECRNOmVyMMUlKFVZ/1UkO4oWpzyZ1coAk6MWkqo8Dj4vI9cC9wE0tvVZE5gPzgRMT2Z1WS3/zHzK/qZpoZVlZWSdeZ2RkEIlETmzX19ef9trj04R7vd4ztmEYY5LMwTKoeAI8fmeMQ87cREd0Rm5WEFXAgJjtnOi+U3keuPZsrlXVBapapKpFvXr1Os9w3dG5c2cOHz4c91ifPn3Yu3cv1dXVBINB/vznP7foOmNMCuo2Ema8BDNLUiI5gLsVxCqgQETycX64zwOujz1BRApUdUt082rg+OsS4DkR+QFOI3UBsNLFWF2TnZ3NtGnTGDVqFB06dKBPnz4njvl8Pu677z6Ki4vp37//SY3ON998M3fcccdJjdTGmBTTGIQDH0LPYme7/zWJjecsuTrdt4hcBfwI8AJPq+r3ROQBoFRVS0TkMeByoAE4ANylqhui194D3AKEga+q6uun+yyb7vv07N/CmDYWPgaL/hH2LYKLX4c+Fyc6orhON923q20Qqvoa8FqzfffFvP6301z7PeB77kVnjDEuaTgEC6+BfYsh0Av83RMd0TlJeCO1McaklWANLPwHqF4JHS6AS9+Grqk5ZintE4SqIkk0dD0R0mXVQGOSXv1eeGcW1K6DrDy47G3oNCjRUZ2ztJ6LKTMzk+rq6nb9A1JVqa6uJjMzM9GhGJPeNOIs9FO7DjoPhVmLUzo5QJpXEDk5OVRWVrJv375Eh5JQmZmZ5OTkJDoMY9KbeGDsg7Duv+CiP0GHPme+JsmldYLw+XznPGrZGGNapLEevNEK/YIrod8sJ1mkgfS4C2OMSYSa1VAyBD55s2lfmiQHsARhjDHnZu9iePtSqKuCrb9IdDSusARhjDFna/df4N0rnfEOuf8EU36T6IhcYQnCGGPOxs6XYNEcaKxz1o+e+hx4/YmOyhWWIIwxpqW2/xqWfg4iDTD861C8ADzeREflmrTuxWSMMa2q4wBnuu7Cb8Ko/0qq9aPdYAnCGGNaqs/FcHU5dMpLdCRtwh4xGWPMqajCmv+Eqqa1WtpLcgBLEMYYE1+kEVbOh/KHYen1ziR87Yw9YjLGmOYaQ7D8Rtj5gjNKevoLEOiR6KjanCUIY4yJFa6DJZ+F3a9CRmdnXqU+FyU6qoSwBGGMMcc1HIL35sDe98DfAy55A7LjLrbWLliCMMaY4w5thupV0KEfXPImdBuZ6IgSyhKEMcYcl10EF/8Zsgam/FoOrcF6MRlj2rcjO6DqtabtPpdYcoiyBGGMab8OboQ3p8Pia2HvokRHk3RcTRAiMltENolIhYjcHef410WkTETWicjbIjIw5lijiKyN/ilxM05jTDtUsxremuFM1509CbqNTXRESce1NggR8QKPA7OASmCViJSoalnMaWuAIlU9JiJfAh4GPhc9Vqeq49yKzxjTju1Z6PRWCh+GfrNhxu8ho2Oio0o6blYQxUCFqm5T1RDwPDA39gRVfVdVj0U3VwC2cLIxxl2Vf4R3ZzvJYeA8mPlHSw6n4GaC6A/sitmujO47lVuB12O2M0WkVERWiMi18S4QkfnRc0r37dt3/hEbY9Jb6CCs+CJEglDwJWehnzRdy6E1JEU3VxH5AlAExA5XHKiqVSIyCHhHRD5S1a2x16nqAmABQFFRkbZZwMaY1OTvCjNedh4xjb4/7afrPl9uJogqYEDMdk5030lE5HLgHuAiVQ0e36+qVdG/t4nIQmA8sLX59cYYc1qqUPshdI82afa52PljzsjNR0yrgAIRyRcRPzAPOKk3koiMB54A5qjq3pj93UUkEH3dE5gGxDZuG2PMmUUaYdWd8Jeik6fsNi3iWgWhqmERuQt4A/ACT6vqBhF5AChV1RLgEaAT8KI4pd5OVZ0DjACeEJEIThJ7qFnvJ2OMOb3YGVk9AdBIoiNKOaKaHo/ui4qKtLS0NNFhGGOSQfgoLPo0/O2v7X5G1jMRkdWqGndGwqRopDbGmFYTrIH3roH9yyHQCy75C/S4MNFRpSRLEMaY9KEKi//RSQ4dc+HSN6HL0ERHlbJsLiZjTPoQgbEPQo8JcMVSSw7nySoIY0zqCx10xjgA9JoKV66yMQ6twCoIY0xq27cU/jQYdv6+aZ8lh1ZhCcIYk7qqXoN3ZkGwGnb9/sznm7NiCcIYk5q2PQOL5kBjHQy+Fab8OtERpR1LEMaY1KIKZQ/DiptBG6Hwm1D8C/B4Ex1Z2rFGamNMaln/AHz0LUBgwo9g2L8mOqK0ZRWEMSa19L8G/D1g6nOWHFxmFYQxJvlFGpseIfWYAHO3g69LYmNqB6yCMMYkt/p98OZU2P5s0z5LDm3CEoQxJnkd2Q5vToPqlbDhO84MrabN2CMmY0xyOvChs3Z0/d+cxX4uft2WB21jliCMMclnz0JYNBcaDkGfS2HmK/ZYKQHsEZMxJrlUlsC7VzrJIfezcPFrlhwSxBKEMSa5dMoHbwco+DJM/S14A4mOqN2yR0zGmOTSbTRc9aGznoNNupdQVkEYYxIrEoaVt0PFk037sgZackgCliCMMYkTPgqLroWKBfDBV6F+b6IjMjFcTRAiMltENolIhYjcHef410WkTETWicjbIjIw5thNIrIl+ucmN+M0xiRA3R5462LY/SoEsuGSNyGzd6KjMjFcSxAi4gUeB/4BKAQ+LyKFzU5bAxSp6hjgJeDh6LU9gPuBSUAxcL+IdHcrVmNMGzu02RkdXVMKWfkwaxn0mpLoqEwzblYQxUCFqm5T1RDwPDA39gRVfVdVj0U3VwA50ddXAm+qao2qHgDeBGa7GKsxpq3sX+kkhyPboEcRXLHc1o5OUm4miP7Arpjtyui+U7kVeP1srhWR+SJSKiKl+/btO89wjTFtwt/N+fuCq+HyhdChT0LDMaeWFN1cReQLQBFw0dlcp6oLgAUARUVF6kJoxpjW1mWo80ip0yDwJMWPIHMKblYQVcCAmO2c6L6TiMjlwD3AHFUNns21xpgUoBFYezds/FHTvi5DLTm0kj21jZRWuDOJoZtfoVVAgYjk4/xwnwdcH3uCiIwHngBmq2ps/7Y3gO/HNExfAXzTxViNMW5oDMKKW+Dj58DjhwGfgawBZ77OnFawQSndGmJpeZAtn4TxZ8DI3O508Lfu2BHXEoSqhkXkLpwf9l7gaVXdICIPAKWqWgI8AnQCXhRnUMxOVZ2jqjUi8h2cJAPwgKrWuBWrMcYFoVpY/GnY8y5kdILpL1lyOA+qyrY9YZaWh1hZESTY4Oz3Z0DRED/BBm31BCGq6fHovqioSEtLSxMdhjEG4OguWHgVHFwPmX2dCfd6jE90VCnp0LEIyzcFWboxyCcHIif2D+6TwbQRfoqGBM4rMYjIalUtinfMHgIaY1pX7QZnNta6KugyHC75izN1hmmxxoiyfmcDS8qDfPRxA43RvNC5gzBlWIDpwwP06+F1PQ5LEMaY1uXrBNoIvabDzD9CoEeiI0oZnxxoZOnGICs2BTl4zHm64xEYm+dj+ogAo3J9ZHjbbo4qSxDGmNah6kywlzXQGd+QNRC8mYmOKunVNyilFU6Dc8Xfwif29+3mYdqIAJOHBuiWlZhp8yxBGGPOj0Zgzf8Bf3cYdY+zr8uwxMaU5FSVHXsbWVweZOWWpgbnQLTBefqIAIP7ZiAJntHWEoQx5tyFj8Hyf4ZdL4PHB3k3QKe8REeVtI7WR1ixOcTisiBVNY0n9g/um8GMEQEmDPGT6Uueac4tQRhjzk3dHlg0B6pXgq8rzHjZkkMcEVU27w6zuCzIB9tChKN5oVOmMHVYgOkj2qbB+VxYgjDGnL2DZU431qMfQ1YeXPwqdG0+WXP7Vns0wrKNQZaUB9l3yOmGJEDhgAxmjMhkXH7bNjifC0sQxpizs28pLLwaGg5CdjHMLLEJ96KOd09dXOZ0T41Eh5l1z/IwbYSfacMD9OySnNVCPJYgjDFnJysffJ2h7+Uw5VeQ0THRESXcvoONLCl3BrMd757q9cCF+U731JEDfHg8yV0txGMJwhhzZhodqSUe6HiBs4ZDhwuc7XaqIays2R5iSXmQ8sqm7qm9u3qYURhgyrAAXTum9r+PJQhjzOmF62DFF6HzYBj7PWdfx5zTX5PGqqrDLC4PsmJTiKNBp1rweWHCYD8zCgMU9Et899TWYgnCGHNqdZ/AomudnkoZnaHgy04F0c7UNyirtoRYUl7Ptj1N3VMH9PQyozDApAI/HQOpXS3Ec8YEISKZqlrfFsEYY5JIzRqnG+uxSmdU9EV/alfJQVXZvreRJWXBk2ZP7eAXigv8zBgRILeXN22qhXhaUkGsFJE3gZ+paoXbARljksCuV2DZF6DxGPScCjNfgczeiY6qTRytj7B8k9O2EDuYbUi/6GC2wX4CSTSYzU0tSRDjgKuBH4qIB/gZ8KqmyzzhxpiT7XgOlt3gvM6/EYoXgDeQ2JhcpqpsStHBbG5qSYLoBmwAvg2MBR4GfgzkuxiXMSZR+s6CToNhyHwY8R/OBHxp6nCdM5htUVmQvQebBrONHOBjRmGAsXnJP5jNTS1JEPuB5cBS4DCwADjkZlDGmDZWvx/83Zx1ojN7wVXr0nZ8g6qysaqpWji+1kK3LGH6CKdayO7c/qqFeFqSIIqArwCjgSeBV1Q1cvpLjDEp48A6eO9TMODTMOGHzr40TA7HV2Y7qVoQGD3Qx0UjnbUWvCk4mM1NZ0wQqvoB8EURyQZuAxaJyGuq+n3XozPGuKuyBJZdD+GjsH+FM+Yho0Oio2o1EVU2VYVZtCHImu0nVwszRgSYZtXCabWkm+t7QBZw/FeKCHAdYAnCmFSlCuWPwNq7AXWm6Z70ZNos8HPoWIRlm4IsblYtjBnoY6ZVCy3WkkdMNwK1wKGz7bkkIrOBxwAv8KSqPtTs+EzgR8AYYJ6qvhRzrBH4KLq5U1XnnM1nG2NOobEeVt4O23/lbI/5Loz8vynfGH2qaqF7lofphQGmD/fTw6qFs9KSR0wfn8sbi4gXeByYBVQCq0SkRFXLYk7bCdwM/Huct6hT1XHn8tnGmNP48F4nOXg7OpPt5X4m0RGdl1NVC2PznJ5IVi2cOzen2igGKlR1G4CIPA/MBU4kCFXdET1mjd7GtJVR90DtRzD+Yeg+NtHRnJPj1cJ7G4KstWrBNW4miP7ArpjtSmDSWVyfKSKlQBh4SFX/0PwEEZkPzAfIzc09j1CNSXO7XoELrgav31k7+tI3Eh3ROTl0LMLSjU61cGIRnphqYXRuak6rnaySebK+gapaJSKDgHdE5CNV3Rp7gqouwBmXQVFRkY3sNqa5xhB88FXY8jMYcgcU/yzREZ21iCobK8MsKju5WujRycP0aE+kHp3Sb6K8ZOBmgqgCBsRs50T3tYiqVkX/3iYiC4HxwNbTXmSMaVK/F5Z8FvYuAk8Aep5NAZ94p6sWZkbbFqxacJebCWIVUCAi+TiJYR5wfUsuFJHuwDFVDYpIT2AazhQfxpiWqFnjTNN9bKezsM+MV6BncaKjOqOmaqGetdsbTqoWZhQGmDrcqoW25FqCUNWwiNwFvIHTzfVpVd0gIg8ApapaIiITgVeA7sCnROTbqjoSGAE8EW289uC0QZSd4qOMMbF2PA/v3wKNdZA9GWa+DB36JTqq0zp4zJkTKV61cNHI1F2yM9W52gahqq8BrzXbd1/M61U4j56aX7cMZ2oPY8zZUIWdzzvJYfCtUPR40s7EeqZqYdrwAN2tWkioZG6kNsacLRFnbMOuV5ypupNw8Fu8asEjMC7faVuwaiF5WIIwJtVVr4Kyh2Dqs85UGb4uMOimREd1kogq5bucauHDHVYtpApLEMaksopfQOldEAnBpseg8D8THdFJDh6LsLQ8yOLyIPutWkg5liCMSUWN9U5i2PqUs13wZRj2tcTGFHWmamH6iADdsqxaSAWWIIxJNUd2wJLroGa180hp4hMw6MZER8XBYxGWlAdZEqdauGhkgMIcqxZSjSUIY1LJke3wlyII1UCnQTDj99A9cXNaRlQp29XA4rLgSdVCdmfPifUWrFpIXZYgjEklWXnQ9zJngZ+pv3HmVUqA2qNNo5yrDzdVC+PznfUWrFpID5YgjEl2oQPQcBiycp1uq5OfccY2SNv+Zn68Wli0wakWItHZz7I7N/VEsmohvViCMCaZVZc68yn5u8MVy5w2hzZeErT2aFNPpNhq4cJBzgyqhQN8eJJwvIU5f5YgjElGqrD5cVjzDacLayDbqSTaaMoMqxYMWIIwJvmEDsLKf4GdLzrbBV+GC/+nTabMiFcteD1wYb6PmYWZjBiQYdVCO2IJwphkcmAtLP4sHKmAjM4w6UkY+E+ufuSpqoWeXaLrLVi10G5ZgjAmmexd4iSHbmNh+ovQpcC1j7JqwZyJJQhjEk21aVK9oV92lgXN+2dXGqNPVy3MGOGst2DVgjnOEoQxiVT7Ebx/mzPRXuchTqIYMr/1P8aqBXMOLEEYkwiqUPEEfPA1Z16ldffDtGdb9SOsWjDnyxKEMW0tWOP0Utr1srM96ItQ9JNWe3urFkxrsQRhTFvauxiW3QDHdjnrNkz8OeR9/rzf1qoF4wZLEMa0lfr98O5saDwG2ZNg2nPOhHvnwaoF4yZLEMa0lcyeMO6/oW43jPk2eHzn9DZWLZi24mqCEJHZwGOAF3hSVR9qdnwm8CNgDDBPVV+KOXYTcG9087uq+oybsRrjil0vOw3SuZ9xtofddc5vZdWCaWuuJQgR8QKPA7OASmCViJSoalnMaTuBm4F/b3ZtD+B+oAhQYHX02gNuxWtMqwofgw++7vRUyugMPSdDx/5n/TZWLZhEcrOCKAYqVHUbgIg8D8wFTiQIVd0RPRZpdu2VwJuqWhM9/iYwG/iti/Ea0zqqS2H5F+DQJvD4Yez3oMMFZ/UWVi2YZOBmgugP7IrZrgQmnce1f/frl4jMB+YD5ObmnluUxrSWSBg2PAjrHwANQ9dCZwBcC1d8s2rBJJuUbqRW1QXAAoCioiJNcDimvXv/NtgebSob9lUY96CzfsMZWLVgkpWbCaIKGBCznRPd19JrL2527cJWicoYtwz7CuxdBJN+4SwLehpWLZhU4GaCWAUUiEg+zg/8ecD1Lbz2DeD7InJ8wd0rgG+2fojGnIe6Pc6aDcd7JvWYAJ/aDJ5T/7eyasGkEtcShKqGReQunB/2XuBpVd0gIg8ApapaIiITgVeA7sCnROTbqjpSVWtE5Ds4SQbggeMN1sYkhco/wvv/AsF9Tu+kAf/o7I+THCIRZf3OBhaXB1ln1YJJIaKaHo/ui4qKtLS0NNFhmHQXqnUm2Nv2S2e7z2Uw+X8ha8DfnVp9uJEl5UGWloc4cLRpLedxVi2YJCIiq1W1KN6xlG6kNqZNVb3mTLJXtxs8AWdU9LCvgDT99h9uVNbtcKqFDTsbOP7rV68uzlrOU4cH6NrRqgWTGixBGNMS238Ny290Xvec4lQNXYadOLyntpHF5UGWbQxyuM5JCxkeuHCwnxmFAYZeYNWCST2WIIxpiZxroctwGHyb04XV46UhrHywLcTisiCbdodPnNqvu5eZhQEmD/PTKdOqBZO6LEEYE0/ogDPobfT9kJEFvs5w1Trw+KiqDrO47CjLN4c4FnSqBX8GTBziVAuD+mQgVi2YNGAJwpjmql6FlfOdtoZIA0z4IcEGZVVFhMVlB9m2p/HEqbm9vMwYEaC4wE/HgFULJr1YgjDmuPp9Tg+lHc7Sn9pzClU9buPdhUdZuSVIfYNzWge/MKnAz/TCAAN72X8hk77su9sYVdj+K2f21VAN6slka5/7+O3e29n5OkAQgMF9M5hRGKBosJ+Azx4hmfRnCcKYfUtgxc0AVPkvYcGBR9hdkw9AVkCYMsypFvr3sP8upn2x73jTPqmCCIfrIizfXUR3buKjo8Usr/0cIAzvn8H0wgAX5vvxZVi1YNonSxCm3YnsX0lw2Z28lvEYb+0cQTgC8AO6dBD+YVSA6YUBenf1JjpMYxLOEoRpN2pqDlC74r/IO/AzOkiEvNBDNEaeYXSuj+mFAcYM9JHhtWrBmOMsQZi0Fm5UPtweYt9Hv2HK0XsY5NlDBA/vRe7iyIj7eaiwKz06W7VgTDyWIExaqqoOs6Q8yOYtW/is5ytM8C0BD+zJmMiRUT9hxvCJeDxWLRhzOpYgTNo4FoywqiLE0vIg2/c6g9m6io/8rmsJeXsSGfsgfYbdQh+xAW3GtIQlCJPSVJXNu8Ms3Rhk9dYQobAyKuMtsvyXUlTQkekjhuFvfBnpMQECPRIdrjEpxRKESUm1RyMs2xhk6cYgew86ay3082zklh73khd5l4ZxjzGhNBkAABDgSURBVOEr/Nfo2bMSF6gxKcwShEkZ4UZl3ccNLCkPsn5nA8fXurog6wA3Zj/KoMO/QCKN4O+OL7NrYoM1Jg1YgjBJ75OaRpZsDLJ8U9NaC14PjM+DT3X7Jf2qvoscOuAs3DPkDhjzAGT2SmzQxqQBSxAmKdWH9ESD89Y9J6+1MH2En8lDA3TZ+wIs+4ZzoM9lMOGH0G10giI2Jv1YgjBJQ1XZ+jene2ppRYhgNC8EfFA8JMC0EQEGdT+MBDo4B3L/CXa9DPn/DP0/BbYGgzGtytUEISKzgccAL/Ckqj7U7HgA+BUwAagGPqeqO0QkDygHNkVPXaGqd7gZq0mcmsONLN8cYllMgzPAkH4ZTB8RYMJgP5nhv8FH34JFv4WrN0DWAPB4YcaLiQvcmDTnWoIQES/wOE4XkkpglYiUqGpZzGm3AgdUdYiIzAP+G/hc9NhWVR3nVnwmsUJhZc12JymU7woTbW+ma0dh8rAA04cH6NvdCw2HoOw7sPGH0HgMxAt73oZBNycyfGPaBTcriGKgQlW3AYjI88BcIDZBzAW+FX39EvATsbUa05aqsm1PI8s2BllVEaIu5KSFDA+MzfczbbifwgE+vB6BxiBs/Als+C4E9ztvkPOPMPb70HV4Au/CmPbDzQTRH9gVs10JTDrVOaoaFpGDQHb0WL6IrAEOAfeq6mIXYzUuOnAkworNQZZtDPK32qZHSAN7eZk2PMDEAj+dMpuNbl71Jdj2v87rXtNh3MPQa0obRm2MSdZG6k+AXFWtFpEJwB9EZKSqHoo9SUTmA/MBcnNzExCmOZWGsLJ2e4hlm0Js2NU0ZqFzB2HK0ABTh/vpnx3z7acKoQNNo52H/RtUr3Iqhv7XWAO0MQngZoKoAgbEbOdE98U7p1JEMoCuQLWqKtF1HlV1tYhsBYYCpbEXq+oCYAFAUVGRYhJKVdmx13mEtLIixLFg05iFsfk+pg4PMHJAsym1VWH3q7DufvB3h8vecvZ3HwtXrbPEYEwCuZkgVgEFIpKPkwjmAdc3O6cEuAlYDlwHvKOqKiK9gBpVbRSRQUABsM3FWM15qD0a4f3NQZZuDPHJgcYT+3N7eZk6LEBxgZ/OHZo9QlKFT/4KH90H1SudfR36Qd0e6NDH2bbkYExCuZYgom0KdwFv4HRzfVpVN4jIA0CpqpYATwG/FpEKoAYniQDMBB4QkQYgAtyhqjVuxWrOXkOjsm5HA0vLg6xv9ghp0lA/04YFyOkZ59tLFfa8A+vug/3LnH2ZvaHwmzDkdsjo0HY3YYw5LVFNjyczRUVFWlpaeuYTzTk7/ghp+aYg7285+RHS6IE+pg0PMCr3DKuyhWrhDzkQPgqBbBjxnzD0TsjIaqO7MMbEEpHVqloU71iyNlKbJFJ9uJEVm0Is3xxkT0wvpJxspxfSpKFxHiEdFwk7o51zrgWvH/zdoPBuZ96koV8BX+c2ugtjzNmyBGHiOhaMsHprAys2B9m8u2kupM4dhEkFfqYMC5Db6zTfPo31sO0ZKH8YjmyDSU/B4FucY6PudTl6Y0xrsARhTmiMKGW7Gli+KcTa7SEaou3NPi+My/czZVjMQLZTCR2AigWw6TGo+8TZ12mIUzkYY1KKJYh2TlXZtd9pV1i5JcShuqY2qaEXZDBlWIALB/noGGjBMp1ljzjzJTUec7a7jYGR/xcGXOfMm2SMSSmWINqpA0civL8lyPJNIXbXNHVN7dvNw+ShTrtCzy5n+KGuCpEgeDOd7UAPJzn0nQXDvwb9ZltXVWNSmCWIdqS+QVmzLcTyTUE2VjZNkNcpU5hY4GfK0AB5vb2ccTqscB3s/B1s+hH0vgQm/MDZn3cDZE+CbqNcvQ9jTNuwBJHmIhFlY1WY5ZuCrNnWtMZChgfG5PmYMqwFXVOPO7QZtvwctv/SaWsAaDgC4x9xHiF5My05GJNGLEGkIVVl5/5GVm4OsbIiSO3RpnaFIX0zmDzMT9FgP1nNJ8g7lepVsPZuZ4DbcT2KoOBOyPu8tS8Yk6YsQaSRvQcbWbklxPubT541tVcXD5OH+pk8LEDvri38YR6uixnVLE5y8HaAvOuddZ+z446rMcakEUsQKe7QsQilW52ksG1PU2Nz5w5C0RA/kwr8DOqTceZ2BXAW5/n4d84jJDwwa5GzP7sIpvwG+l9t3VWNaUcsQaSg+pCzGtvKLSHKdjUQiT5BCmTA+EF+igv8jMhpYbtCpBH2vgvbfumMeG6sc/ZndIL6/ZDZ09nOv8GVezHGJC9LECki3Khs2NXA+5tDfLgjRCja2Oz1wJhcH8VD/YzL8xPwnUW30upSWHQt1MXMwt77Ymc5zwGfAV+n1rwFY0yKsQSRxCKqVHwSZuWWEKUVIY4GYxqb+2UwqcDPhMGnmQcpliocWANHd8CATzv7Ohc4y3lm5cOgmyD/RuiU787NGGNSjiWIJFRZHWbl5hDvbwlRc6Spsbl/Dy+ThvqZOKQFg9jAeXxU/T5UlcDO38ORCmcG1f6fAo8P/F3hqo+g8xAb0GaM+TuWIJLEntpGVlU4lUJVzMjmHp08FBf4mTTUT052C79cBzdC+SOw+89Qv7dpf2ZvZ9qL8BFn9TaALgWteBfGmHRiCSKBqg87SWFVRYid+5qSQlZAmDDYSQpD+mXgOd1v96pwuAIaaiF7orOvsQ62PR19szzImQv950DvmeCxL7kxpmXsp0Ubqz0aobQixKqKk7ulZvqcHkgTh7SgB1KwxhmX8Mlf4W9vOu0KPafAFdEV2rqPg/GPQr8roOsoe3xkjDknliDawKFjET7Y5lQKW3Y3zYHkz4CxeU5SGJXrw5dxhh/kO1+EDQ9B7VrQprYJ/D2chmaNOAvxiMCIb7h2P8aY9sEShEuO1kf4YFsDpRVBNlaFT4xVyPDC6FwfE4f4GROvW2okDIfKnekt9i11ehz1v9o51hiEAx84Dcy9ZjoVQt9Z0H28TXdhjGl1liBaUV1I+XB7iJUVzgC2xugv+V4PjB7gY2KBn3H5fjr4myWFj19wkkFNKRxY2zRYDcDjb0oQ/WbDpW9Dz0m2hrMxxnWuJggRmQ08BniBJ1X1oWbHA8CvgAlANfA5Vd0RPfZN4FagEfhXVX3DzVjPVV1IWfdxiNUVIdbvbDixCpsIjMjJYOIQP+PzoFNoKxwqg40bnAphyq+dNZoBNv/YSRDHdRrkTIaXPQn6zWran9kT+l7aZvdmjGnfXEsQIuIFHgdmAZXAKhEpUdWymNNuBQ6o6hARmQf8N/A5ESkE5gEjgQuAt0RkqKo2kgSOBSN8uKOB1VtDbNgVwhc5RFfZQ1cCdO83mIkFfib2KKNT2ddg8zZYWwXoyW8y+n7oWui8HnwbXHCNM+dR9wudhXeMMSbB3KwgioEKVd0GICLPA3OB2AQxF/hW9PVLwE/EmVVuLvC8qgaB7SJSEX2/5a5EWrcHatdBpAG0wfn7+J/wERhyO0dDwtodDWRsuJ8OR9bSR/Yzz7OXrp324pMgAPW5t5A5/SnnPWt9sDc62Z14oGMedB3pJIWuIyGzT9PnD7rZldsyxpjz4WaC6A/sitmuBCad6hxVDYvIQSA7un9Fs2v7uxbp3oWwdN4pD/904xzW7e5IYwS+lrWcQt+ik0/I6ASZfcnMym7a17kALn3Lmbqi4wCnYdkYY1JISjdSi8h8YD5Abm7uub9Rhwug7+UgPhrUS22dl+qjXmqPeanXLDbWholE2xRCve/hWK+v0rFLb8js61QC8Sa182ZC38vOPSZjjEkwNxNEFTAgZjsnui/eOZUikgF0xWmsbsm1qOoCYAFAUVGRNj/eUrVZ01jT+0+s3hpi8+4wGn0nj8DwnAw+O9jpfeRMinfFuX6MMcakFDcTxCqgQETycX64zwOub3ZOCXATTtvCdcA7qqoiUgI8JyI/wGmkLgBWuhHkwvX1PLfo2IkmZK8HCgf4mDDYz9h8H51auiynMcakGdcSRLRN4S7gDZxurk+r6gYReQAoVdUS4Cng19FG6BqcJEL0vN/hNGiHgS+71YMpv08GXg+MzI0mhTwfHQOWFIwxRlTP+clMUikqKtLS0tKzvk5VqW/g7wevGWNMOyAiq1U17iLzKd1I3RpEhA7+REdhjDHJx56lGGOMicsShDHGmLgsQRhjjInLEoQxxpi4LEEYY4yJyxKEMcaYuCxBGGOMiSttBsqJyD7g4/N4i57A/lYKJ1W0t3tub/cLds/txfnc80BV7RXvQNokiPMlIqWnGk2YrtrbPbe3+wW75/bCrXu2R0zGGGPisgRhjDEmLksQTRYkOoAEaG/33N7uF+ye2wtX7tnaIIwxxsRlFYQxxpi42lWCEJHZIrJJRCpE5O44xwMi8kL0+Psiktf2UbauFtzz10WkTETWicjbIjIwEXG2pjPdc8x5nxERFZGU7/HSknsWkX+Kfq03iMhzbR1ja2vB93auiLwrImui399XJSLO1iIiT4vIXhFZf4rjIiL/L/rvsU5ELjzvD1XVdvEHZ1W7rcAgwA98CBQ2O+dO4OfR1/OAFxIddxvc8yVAx+jrL7WHe46e1xlYBKwAihIddxt8nQuANUD36HbvRMfdBve8APhS9HUhsCPRcZ/nPc8ELgTWn+L4VcDrgACTgffP9zPbUwVRDFSo6jZVDQHPA3ObnTMXeCb6+iXgMhFJ5aXmznjPqvquqh6Lbq4Acto4xtbWkq8zwHeA/wbq2zI4l7Tknv8FeFxVDwCo6t42jrG1teSeFegSfd0V2N2G8bU6VV2EszTzqcwFfqWOFUA3Eel3Pp/ZnhJEf2BXzHZldF/cc1Q1DBwEstskOne05J5j3YrzG0gqO+M9R0vvAar6alsG5qKWfJ2HAkNFZKmIrBCR2W0WnTtacs/fAr4gIpXAa8BX2ia0hDnb/+9n1O6XHDUOEfkCUARclOhY3CQiHuAHwM0JDqWtZeA8ZroYp0pcJCKjVbU2oVG56/PAL1X1f0RkCvBrERmlqpFEB5Yq2lMFUQUMiNnOie6Le46IZOCUpdVtEp07WnLPiMjlwD3AHFUNtlFsbjnTPXcGRgELRWQHzrPakhRvqG7J17kSKFHVBlXdDmzGSRipqiX3fCvwOwBVXQ5k4sxZlK5a9P/9bLSnBLEKKBCRfBHx4zRClzQ7pwS4Kfr6OuAdjbb+pKgz3rOIjAeewEkOqf5cGs5wz6p6UFV7qmqequbhtLvMUdXSxITbKlryvf0HnOoBEemJ88hpW1sG2cpacs87gcsARGQEToLY16ZRtq0S4MZob6bJwEFV/eR83rDdPGJS1bCI3AW8gdMD4mlV3SAiDwClqloCPIVThlbgNAbNS1zE56+F9/wI0Al4Mdoev1NV5yQs6PPUwntOKy285zeAK0SkDGgE/kNVU7Y6buE9fwP4hYh8DafB+uZU/oVPRH6Lk+R7RttV7gd8AKr6c5x2lquACuAY8MXz/swU/vcyxhjjovb0iMkYY8xZsARhjDEmLksQxhhj4rIEYYwxJi5LEMYYY+KyBGGMMSYuSxDGGGPisgRhjIui6xHMir7+roj8ONExGdNS7WYktTEJcj/wgIj0BsYDKTtK3bQ/NpLaGJeJyHs405lcrKqHEx2PMS1lj5iMcZGIjAb6ASFLDibVWIIwxiXR1byexVnp60gaLNJj2hlLEMa4QEQ6Ai8D31DVcpwlTu9PbFTGnB1rgzDGGBOXVRDGGGPisgRhjDEmLksQxhhj4rIEYYwxJi5LEMYYY+KyBGGMMSYuSxDGGGPisgRhjDEmrv8PuAeCdLJjtUoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf8AAAFGCAYAAABkNJYDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxc9X3v/9dnVu3yJm/yItvyjm0MsoGYBAgEzGITICHQ9tEsJCS9pb1p8ru3yf01CUl7m7S32UhJUzcQ0t4UQghJMDskUELYvALGq7zv8iprn+17/zhjWwjL1kgjzejM+/l4zEOac86c+Tg56H2+3/M932POOURERKRwBHJdgIiIiAwshb+IiEiBUfiLiIgUGIW/iIhIgVH4i4iIFBiFv4iISIEJ5bqAgTJixAhXU1OT6zJEREQGxKpVqw4756rOtK5gwr+mpoaVK1fmugwREZEBYWY7u1unbn8REZECo/AXEREpMAp/ERGRAqPwFxERKTAKfxERkQKj8BcRESkwg/JWPzP7MHA9UAHc55x7NscliYiIDBp50/I3s/vNrMHM1nVZvtjMNplZvZl9CcA592vn3GeAzwEfy0W9IiIig1XehD/wALC48wIzCwL3AtcCs4DbzWxWp03+Jr1eREREeihvuv2dcy+ZWU2XxQuBeufcNgAzewi40cw2AN8CnnLOre7J/k+0pnh2bVvW6g0HjZKo9yqOGCXRwKn34SCYWda+S0REJJvyJvy7UQ3s7vR+D3AR8BfAVUClmdU65350pg+b2Z3AnQAjJszjF69kL/zPJhTg9IlB1CiNBtInCJ1enU4YOq8rjhihoE4cRESk/+R7+J+Rc+4e4J4ebLcMWAYwZcYF7qp50azVEE9Aa4ejLeZo6UjR1uFoTb8SKTjR5jjR5tJbJzPadzTE6RODqFHa5QShJBpInzycPsE4+b4oYgTU6yAiImeR7+G/Fxjf6f249LKMDS0L8LFFpVkp6lxiCe8koK0jfWIQO31i0NrhaI2dfV1HAjoSKY61ZP7dBt7JQNSoLA4wtKzTqzTA0DJjaGmAytIAwYBOEkREClG+h/8KYKqZTcIL/duAP8pkB2a2BFhSW1vbD+WdWSRkRELGkFKAYEafdc7RHofWdG9CS7p34fQJQorWTu+7rvM+670/fCIFB8/8PWZQWWKdTgo6/Uz/PqQ0oEsQIiI+ZM65c281AMzsQeByYAReZH3NOXefmV0HfA8vRe93zv3v3uy/rq7OFcIjfZMpR3vMO2k43pLiWHOKYyd/dvr9RKujJ//PVxTbGXoPAgwrCzCyMkhliWlwo4hIHjKzVc65ujOuy5fw72+FEv49lUg6Glvfe1Jw8ufR5hSNrY5zHR7FEWP0kACjhwYZPSTI6KFBxgwJUlWpXgMRkVw6W/jne7d/n+Wi238wCAWN4eVBhpd3f1kimXI0tjqONadO9yKkTwyONCc5eDxFa4dje0OS7Q3vHtQYMKiqDLzrhGD0UO99aVE+TS8hIlJ41PKXXnPO0dTmOHA8yYFjSfYfT3LgWIoDx5McOZHq9rJCRbGd6imYWBVk4sgQ1cOC6ikQEcmigm75S/8xMypKjIqSANPGht+1LpZwNBx/9wnBgWNJDhxPpm+DTLB5X+LU9qEAjBsRpKYqxISqEDUjg4wZqhMCEZH+4PvwV7d/bkRCxrgRIcaNePchlnLeZYQDx1LsO5ZkZ0OCnYcSHDieYkdDkh0NSaADgHAQxo/wegZqqkJMrPJOCAK6RVFEpE/U7S95obUjxe7DSXY0JNh5KMnOQwkaGlPv2S4SgimjQ0wfG2ZadYhJI0PqHRAROQN1+0veK4kGmF4dYHr16csHLe0pdp06IUiwoyHJkaYUG/Yk2LDHu2Rw8mRg2tgw08aGmDQqRFgnAyIiZ+X78Fe3/+BVWhRg5rgAM8edPiE40Zpiy/4Em/bG2bQvwb6jyXedDISDMDndMzC9OsSU0SHNZCgi0oW6/WVQa2pLsXlfgk374mzem2Dv0XffclgSNWaPDzN3YpjzJoYp022GIlIg1O0vvlVeHODCKREunBIBvJOBLemTgfW74xw4nmJFfYwV9THMYPKoEHMneicD1cODmp1QRAqSWv7iaw2NSd7aEeftnTE27UuQ7DSGcFhZgLkTw1xYG2HamJDuIhARX9H0vij8BdpjjvV74ry1I8bbO+OdHrnsPeTowikRFtRGmDw6pMcii8igV9Dh32nA32e2bNmS63IkT6ScY9ehJKu3xVhZH+PQidNdAsPKAtRNiVBXG6FmpC4NiMjgVNDhf5Ja/tId5xw7DyVZsSXGyq0xjjafPhGoqgiwaEaU982IMrRMgwVFZPBQ+KPwl55JOce2AwlW1MdYtTVGY6v334cZnDc+zKUzo8ytCWtiIRHJewp/FP6SuVTK8c7uOC9v6ODNHfFTgwXLi41LpkW5dGaUMcO6fyqiiEgu6VY/kV4IBIw5EyPMmRihqS3Fa5tjvLyhg31Hkzz7ZjvPvtnOrHEhPji3iDkTwxokKCKDhu9b/hrwJ9nknGN7Q5KXN3Tw+uYOYukHE1ZVBPjgnCLeNyNCSVRjA0Qk99Ttj7r9Jfta2lO8vKGDF9Z1cKTJuyYQDcP7ZkS5el4RIyp0SUBEckfhj8Jf+k8q5XhzR5zfvtXOpn1eV0DAYOHUCNdeUMxYjQsQkRzQNX+RfhQIGPMnR5g/OcKewwmeWdvOG1tivLbZe50/Kcx1FxQzaZT+cxOR/KCWv0g/OHQiybNr2nl5YweJ9LOGZo0P8eGFJToJEJEBoW5/FP6SG42tKZ5/s50X17XTHveWzasJc+PCYsaP0EmAiPQfhT8Kf8mt5vYUz65p57dvt5+6Q6BuSoSlC4sZM1RjAkQk+wo6/HWrn+STE60pnlrdxovveJcDAgYfmB1l6YJiyot1i6CIZE9Bh/9JavlLPjnalOTxVe28vKED56A4Ylx7QRFXzS0iHNJkQSLSdwp/FP6Sn/YeSfDIq22s2+UNCBheHuCWS4qpmxLR0wRFpE/OFv7qZxTJoerhIf77DeV8/oZyqocFOdKUYtmzLXx3eRP7jyZzXZ6I+JTCXyQPzJ4Q5qu3VvAnl5VQGjU27Enw9YcbeeSVVtpjhdE7JyIDR+EvkicCAeOy2UX87R9V8v5ZUVIpeGZtO1958Dhrt8dyXZ6I+IjCXyTPlBcH+NPLS/nyLRVMrApyvMVx71PN/OiZJhpbU7kuT0R8QOEvkqcmjQrxv26p4GOLSoiGYNXWOF99sJHfr++gUAbqikj/UPiL5LFAwLhqXhFfv72SORPCtHY4/v3FFv75yWb1AohIryn8RQaB4eVB/uL6Mj59VSklUeOtnXHufqiR1ds0FkBEMuf78DezJWa2rLGxMdeliPSJmXHRtChf+1gls8aFaG53/MvTzfzkd8206Y4AEcmAJvkRGYRSzvHC2x388tVW4klvcqBPfrCU6dXhXJcmInlCk/yI+EzAjCvnFvGVj1YyscqbHOjbv2niF6+0Ek8Wxgm9iPSewl9kEBszLMiXbq7ghroiMHh2bTv/+KsTHGnS7IAi0j2Fv8ggFwoaNy4s4a9vqmB4eYAdDUm+8fAJ3t6pwYAicmYKfxGfmDI6xFc+WsGcid4tgfc80cyvXm8lldJlABF5N4W/iI+UFgW467oybrqoGDN4clU7313exAnNCSAinSj8RXwmYMZ1FxbzhaXlVBQbG/cm+MbDjWzeF891aSKSJxT+Ij41ozrMV26tZNrYEI2tju881sRL69tzXZaI5AGFv4iPDSkN8IWl5Vw9r4hkCv7jxVZ+/nILSY0DECloCn8RnwsGjI8uKuETV5QSDMDzb3Xwz08209qhcQAihUrhL1IgFs2M8oWl5ZQVGet2xfnWoydoaNR8ACKFaFCGv5lNNrP7zOyRXNciMphMGxvmf32kgrHDguw/luLvHznBpr0aCChSaPIm/M3sfjNrMLN1XZYvNrNNZlZvZl8CcM5tc87dkZtKRQa3qgpvVsA5E8O0dDi+t7yJFVs6cl2WiAygvAl/4AFgcecFZhYE7gWuBWYBt5vZrIEvTcRfiiPGXdeWceWcKIkULHuuheff1J0AIoUib8LfOfcScLTL4oVAfbqlHwMeAm4c8OJEfCgQMD52aQkfuaQYgJ//oZVHXmklVSBP+hQpZHkT/t2oBnZ3er8HqDaz4Wb2I2C+mX25uw+b2Z1mttLMVh46dKi/axUZdMyMa+YX86krvTsBnlnbzv3Pt5DQkwFFfC2U6wJ6wzl3BPhcD7ZbBiwDqKur018zkW5cMj1KRXGAf3m6ide3xGhqS/Fn15ZTFLZclyYi/SDfW/57gfGd3o9LLxORLJs9Icz/9+EKyouN9XsSfG95k+YCEPGpfA//FcBUM5tkZhHgNuCxTHZgZkvMbFljY2O/FCjiJzUjQ/z1TRUMKwuw9UCC7zzWRHO7TgBE/CZvwt/MHgReBaab2R4zu8M5lwDuAp4BNgAPO+feyWS/zrnlzrk7Kysrs1+0iA+NGhLkf95UTlVFgJ2HkvzTr5to1FMBRXzFXIGM7K2rq3MrV67MdRkig8bxlhTfeewE+4+lGFkZ4ItLyxlWHsx1WSLSQ2a2yjlXd6Z1edPy7y/q9hfpnSGlAf7HhysYPyJIQ2OKf/x1E0eaNB2wiB/4PvzV7S/Se+XFXot/0qggR5pS/NNvmjiqEwCRQc/34S8ifVNaFODzN5RTMzLI4RMpvv1YE8eaNQZAZDDzffir21+k70qi3gnAhCrvEsC3HzvB8RadAIgMVr4Pf3X7i2RHaVGALywpZ/yIIAePp/j2b07oLgCRQcr34S8i2VNaFOCvlpRTPSzIgeMpvre8iRbNAyAy6Cj8RSQj5cUBvrC0nNFDAuw5kuQHTzTTES+MW4ZF/ML34a9r/iLZV1Hi9QAMKwuw9WCCHz7dRFwPAxIZNHwf/rrmL9I/hpUH+aul5d6zAHYnuO/5FlIpnQCIDAa+D38R6T+jhwT5/A3lFEeMVVtj/OylVgpl1lCRwUzhLyJ9MqEqxF3XlREOwkvrO3hqdXuuSxKRc/B9+Ouav0j/mzY2zKevKsOAX73exuubO3Jdkoiche/DX9f8RQbGBVMi3LqoBIAHftfCpr3xHFckIt3xffiLyMC5al4RV86NkkjBD59uZv9RPQdAJB8p/EUkq259XwnzJ4Vp7XB8/4kmzQIokocU/iKSVYGAccdVZaeeBPiDJ5o0CZBInvF9+GvAn8jAi4aNu64tp6oiwM5DSe57vpmUbgEUyRu+D38N+BPJjYqSAH+ZngNgzfY4T6zULYAi+cL34S8iuTN6SJA7ry7FgMdWtLF2eyzXJYkICn8R6WfnTYhw08XFAPz4+Wb2Hk3kuCIRUfiLSL9bPL+IBbUROuLww6ea9RhgkRxT+ItIvzMzPn5FKeNHBGloTLHsuWY9BEgkhxT+IjIgomHjz68to6zIewrgo6+35bokkYLl+/DXrX4i+WN4eZDPXVNGwOCZNe0aACiSI74Pf93qJ5JfpleHuTk9APAnv2vhSJOmABYZaL4PfxHJPx86v4i5E70pgP/1mWYSSV3/FxlICn8RGXABMz55ZSnDygJsb0jyy9dac12SSEFR+ItITpQVBbjz6jKCAXj+zQ7WbNP1f5GBovAXkZyZMjp06vr/Ay+0cOiErv+LDASFv4jk1IfmFTGvxrv+/+Pnmknq/n+RfqfwF5GcMjM++cFShpQa2w4meWq1HgAk0t8U/iKSc6VFAT75wTIAlq9oY/tBzf8v0p8U/iKSF2aND3Pl3CgpB/f/tpmOuLr/RfqL78NfM/yJDB43X1zCmKFBDhxP8ctXdfufSH/xffhrhj+RwSMSMj59VSnBALywroN1u3T7n0h/CJ1rAzP700x26Jz7996XIyKFbkJViBsXFvPoa2088LsWvn5biNIi37dTRAbUOcMfeCCD/TlA4S8ifXLN+UW8tSNO/YEED7/SemowoIhkR0/Cf1K/VyEi0kkgYHz8ilK+/nAjr2yMcdHUOLPGh3NdlohvnLMvzTm3M5PXQBQtIv43emiQJXXe7H///mIL7Rr9L5I1upAmInnr6vOLGD8iyJGmFL95XaP/RbIl4/A3s6vN7Fdmtt7MtnV99UeRIlKYQkHj45eXYga/fauDrQc0+Y9INmQU/mZ2HfAUUALMADYCu4DxQAr4r2wXKCKFbeLIEFefX+SNJn6xhURS3f8ifZVpy/8rwL3Aden3f+OcuxyYDQTxTgxERLJq6YJiRlYG2Hc0yXNvau5/kb7KNPxnAMvxWvmO9N0CzrnNwN14JwciIlkVCRl//IFSAB5f2cbR5lSOKxIZ3DIN/xSQcM454BAwodO6fcCUbBUmItLZrPFhLpgcJpaAR17R4D+Rvsg0/DcBNenfVwKfN7MxZlYFfBHYkb3SRETe7dZFJURCsKI+xsa98VyXIzJoZRr+PwNmpn//Gt61/j3AAeCDwFezV1r3zKzUzH5qZv9mZn88EN8pIrk3vDzIdRd49/7/50utGvwn0ksZhb9z7l7n3P9M/74KmAN8Fvgr4Hzn3CO9LcTM7jezBjNb12X5YjPbZGb1Zval9OKbgUecc58Blvb2O0Vk8Ln6/CKqKgLsP5bkhXUduS5HZFDq0yQ/zrk9zrkfO+fucc6t72MtDwCLOy8wsyDe3QXXArOA281sFjAO2J3eLNnH7xWRQSQcMm5/fwkAj73RyolWDf4TyVTezPDnnHsJONpl8UKg3jm3zTkXAx4CbsS71DAuvU3e/BtEZGDMmRhh7sQw7XFYvqIt1+WIDDqZTvKTMrPk2V5Zrq+a0y188EK/GngUuMXM/gXv1sPu6r3TzFaa2cpDhw5luTQRyaWPXFKCGby0voMDx9QBKJKJnjzVr7Nv4N3f39lw4GogSmaP/+0151wL8MkebLcMWAZQV1enkUEiPjJmWJD3z4zy0voOHn2tlf92bXmuSxIZNDIKf+fc3Wdanr42vxxozEJNne3Fmzr4pHHpZT1mZkuAJbW1tdmsS0TywJIFxby2uYM12+Ns2R9n6hg99lekJ7Jyvdw5lwR+CHw+G/vrZAUw1cwmmVkEuA14LMPaljvn7qysrMxyaSKSa0NKA1x9fhHgTfzjzT8mIueSzcFyUWBYbz9sZg8CrwLTzWyPmd3hnEsAdwHPABuAh51z72SlWhHxhWvOL6a82Nh2MMnqbZr4R6QnMur2N7MJZ1gcAc4DvoU361+vOOdu72b5k8CTvd2vuv1F/K0oYixdUMzPXmrlV6+3Mn9SmEDAcl2WSF7LtOW/A9je5bUJb/Q9wJ9nrbIsUbe/iP9dOjNKVUWAg8dTvLEllutyRPJepqP9P8V7R/u3AzuBFelr/yIiAyoUNK6/sJgHXmhh+co2FkyNEFTrX6RbmY72f6Cf6ug36vYXKQwXT4/wxKo2Ghq91v8l06O5Lkkkb/l+djx1+4sUhmDAuL7Oe+jP4yvbSKY08l+kO+ds+ZvZ7zLYn3POXdmHekREeu3iaRGeWOm1/l/fHON9M9T6FzmTnrT8A4B1es0ALgdqgOL0z8uB6en1IiI5EQwYN6Rb/0+sUutfpDvnDH/n3OXOuSucc1cA3wfiwCXOucnOuUucc5OBS9LLv9+/5WbOzJaY2bLGxmxPPigi+eiiaRFGVgZoaEyxol4j/0XOJNNr/n8LfMU593rnhen3dwN/l6W6skbX/EUKSzBgXDPfm/XvmTXtmvVP5AwyDf+pQHePx2sANKReRHLukmlRKoqNPUeSbNiTyHU5Inkn0/DfDny2m3WfxZsESEQkp8Ih44NzT7b+23JcjUj+yXSSn68DPzOzdcAjwEFgFPARvIGAf5zd8vpO9/mLFKbLZ0d5alUb6/ck2HUowYSqTP/cifhXRi1/59xDwDV4j+79MnBv+udx4Brn3M+zXmEf6Zq/SGEqLQrw/lnerX7PrG3PcTUi+SXjSX6cc8875xbh3eY3Gih2zl3qnPtt1qsTEemDq+YVETBYWR/jSJNmHxc5qdcz/DnnUs65BudcKpsFiYhky/DyIHW1EVIOfvdWR67LEckbPZnh76vAj51z+9K/n41zzv1tdkoTEem7q+YW8caWGC9v7GDpwmKiYc1FJtKTETB3A08D+9K/n43DmwtARCQvTBoVomZkkB0NSVbUx7h0pqb8FenJDH8B59wbnX4/2yvY/yVnRjP8icgV53m3/b2wTpP+iICe6iciBWBBbYSyImPXoSTbDmrgn0hG4W9m08xsYaf3xWb2TTNbbmZ3Zb88EZG+C4eMRenu/hfX6bY/kUxb/v+MN6HPSf8b+CIwFviumf15tgoTEcmmy2ZHMbzb/k606iYlKWyZhv884A8AZhYA/hT4a+fchXgP9bkzu+WJiGRHVUWQORPDJFLwh4267U8KW6bhXwkcSf8+HxiKN80vwIvA5OyUJSKSfZfN9rr+X97QoYF/UtAyDf+DnH5y39XAVufc7vT7MiDvHp+l0f4ictLsCWEqS4yGxhRbD+TdnyuRAZNp+D8GfNPM/gnvWv8vOq2bA2zLVmHZotH+InJSMGBcPN1r/f9hYyzH1YjkTqbh/yXgcbyH+zwG/H2ndUuBZ7NUl4hIv1g0wwv/FfUddMTV9S+FKaNnXDrnWoDPdLPufVmpSESkH40ZGmTKqBBbDyZYtTXG+2Zoxj8pPL2a5MfMRpjZDWb2cTMbll5WlL4DQEQkr71vZgSAVzTqXwpUppP8mJn9H2APXrf//UBNevVvgP8/q9WJiPSDBbVRIiHYtC/BoUbN+CeFJ9OW+peBu4BvABcBnR+PtRy4IUt1iYj0m+KIccFkr/X/6mYN/JPCk2n4fxr4hnPu74HVXdbVA1OyUpWISD+7aJoX/iu26J5/KTyZhn818Fo362JAad/KEREZGDOqw5QVGQeOp9hzRF3/UlgyDf+9wHndrJsHbO9bOSIiAyMUNOqmnGz9q+tfCkum4f8L4KtmtqjTMmdm0/Am/Xkoa5VliWb4E5HuLJjqhf8b9TF1/UtByTT87wY2Ai8BW9LLfgG8nX7/raxVliWa4U9EulM7JsSQUuNIU4ptB9X1L4Wjx+FvZhHgP4GvAZ8AXgGeB1bgPc3vQ8459Z2JyKARMKOuNt31X697/qVw9Dj808F+FZByzv2Hc+5PnHNXO+dud8791Dmnp2SIyKCzsNab4W9lfYxUSl3/Uhgy7fb/A3BxfxQiIpILNSODVFUEaGx1bNmvNowUhkzD/4vAHWZ2l5mNM7OgmQU6v/qjSBGR/mJmzE9P+LN2h65cSmHINKzfxpvI5/vATrx7++OdXvovR0QGnfmTwgCs3R7XqH8pCBk91Q9vWl/9lyEivjJ5VIjyYuPwiRR7jyYZNzzTP40ig0umj/S9u5/qEBHJmUDAOL8mwu83dLB2e1zhL76na/QiIsD56a7/Ndt19VL8T+EvIgLMHBcmGoJdh5IcbdKEP+JvCn8RESAcMmZPSA/82xHPcTUi/UvhLyKSNn9S+pY/df2Lzw3K8DezyWZ2n5k9kutaRMQ/5kwMYwab9yVoj+nGJvGvAQ9/M7vfzBrMbF2X5YvNbJOZ1ZvZl862D+fcNufcHf1bqYgUmtKiAJNHhUimYONedf2Lf+Wi5f8AsLjzAjMLAvcC1wKzgNvNbJaZzTGzx7u8Rg58ySJSKM5LX/d/Z5fCX/xrwG9mdc69ZGY1XRYvBOqdc9sAzOwh4Ebn3DeBGwa2QhEpZLPHh/nNG228vcub7c/Mcl2SSNblyzX/amB3p/d70svOyMyGm9mPgPlm9uWzbHenma00s5WHDh3KXrUi4lsTRwYpKzKONKU42JjKdTki/SJfwj8jzrkjzrnPOeempHsHuttumXOuzjlXV1VVNZAlisggFTBj1nh1/Yu/5Uv47wXGd3o/Lr2sz8xsiZkta2xszMbuRKQAzE6H/zqFv/hUvoT/CmCqmU0yswhwG/BYNnbsnFvunLuzsrIyG7sTkQJwcrKfzfvixBK65U/8Jxe3+j0IvApMN7M9ZnaHcy4B3AU8A2wAHnbOvTPQtYmIAFSWBJgwIkgs4Z0AiPhNLkb7397N8ieBJ7P9fWa2BFhSW1ub7V2LiI/NHh9m1+EkG/ckOG9CJNfliGRVvnT79xt1+4tIb0yv9rr+NdmP+JHvw19EpDdqx4QIBmDX4SStHbrlT/zF9+Gv0f4i0hvRsDFpZAjnYMv+RK7LEckq34e/uv1FpLemV3vDotT1L37j+/AXEemtk9f9N+1Vy1/8ReEvItKNKaNDhIKw53CSlnZd9xf/8H3465q/iPRWJGRMGRXCAZv2qfUv/uH78Nc1fxHpi9Nd/7ruL/7h+/AXEemLk4P+dN1f/EThLyJyFpNGedf99x3V/f7iH74Pf13zF5G+CAeNmirvuv/WA2r9iz/4Pvx1zV9E+mrKGK/rv16T/YhP+D78RUT6qnZ0OvzV8hefUPiLiJzDlHT472hIkEi6HFcj0ncKfxGRcygvDjB6SIBYwnvQj8hgp/AXEemB2jHe/f71+3W/vwx+vg9/jfYXkWw4dd1fg/7EB3wf/hrtLyLZcGrE/4EEzum6vwxuvg9/EZFsGFUZoLzYaGpzHDqhyX5kcFP4i4j0gJkxeZTX+t92UF3/Mrgp/EVEemjSSC/8tyv8ZZBT+IuI9NCkUQp/8QeFv4hID9WMDGLA7sNJ4prsRwYx34e/bvUTkWwpiQYYPTRAIgV7NNmPDGK+D3/d6ici2VQzUoP+ZPDzffiLiGTTZF33Fx9Q+IuIZODUoL8Ghb8MXgp/EZEMVA8LEg5CQ2OKlnZN9iODk8JfRCQDoaAxsUqtfxncFP4iIhk6fb+/RvxL3yWSjn/6zQl+v76dVGpgbiFV+IuIZGjSyCCgQX+SHb/f0MGmvQmeXtPOAGW/wl9EJFOdB/3pCX/SF20xx/IVbQDccnEJoaANyPf6Pvw1yY+IZNvwcu8Jf83tesKf9M0za9poanNMGR1i/uTwgH2v78Nfk/yISLZ1fsKfBv1Jbx1pSvLcm+0AfOSSYswGptUPBRD+IiL9oUZP+JM+evgPrcQScOGUCLVjBq7VDwp/EZFeOdny36ER/9IL63bFWL0tTvDFCPUAABHcSURBVDQEty4qGfDvV/iLiPRCTXrE/87DCRJ6wp9kIJ5wPPj7VgCWLChmWNnAR7HCX0SkF0qiAUYPCZBIwt4jav1Lz/3mjTYaGlOMGRrgyrlFOalB4S8i0kunnvCnQX/SQ/X74zy7th0z+PgVZQN2a19XCn8RkV6apCf8SQbaY477f9uCA66dX8SU0aGc1aLwFxHppUka8S895JzjgReaOXQixbjhQZYsKM5pPQp/EZFeGjciSCgAB46naO3QZD/SvWfXtrNqa5ziiPHZa3LX3X+Swl9EpJfCQWN8lTfqf0eDBv3Jmb25I8YvX/Om8P3UlaWMHhLMcUUKfxGRPjnV9a9Bf3IGWw/EWfZsM87BDXVFnD8pkuuSAIW/iEifaNCfdGfX4QQ/eKKZWAIunRllaY6v83eWu6GGfWBmHwauByqA+5xzz+a4JBEpUJ0H/TnnBnR+dslf2w4m+P7jTbR2OObVhPmTy0ry6tgY8Ja/md1vZg1mtq7L8sVmtsnM6s3sS2fbh3Pu1865zwCfAz7Wn/WKiJzNyMoAZUXGiTY94U887+yK853HTtDa4Zg/KcxnrykjGMif4IfcdPs/ACzuvMDMgsC9wLXALOB2M5tlZnPM7PEur5GdPvo36c+JiOSEmVE7xmv9b9mvrv9C5pzj+Tfb+f4TTXTEYeHUCJ+9poxwjkf2n8mAd/s7514ys5ouixcC9c65bQBm9hBwo3Pum8ANXfdhXt/Jt4CnnHOr+7diEZGzmzomxNrtcbbsi7NoRjTX5UgOtMcc//n7Fl7dFAPg+guLWLqwmEAedfV3li/X/KuB3Z3e7wEuOsv2fwFcBVSaWa1z7kdn2sjM7gTuBJgwYUKWShURebepY8JAm1r+BWrbwQQ/fs6bwCcSgk9cUcqCqfl9Epgv4Z8R59w9wD092G4ZsAygrq5Oj90SkX4xfkSQaAgaGlM0tqaoLNGNVIWgI+5YvrKN59a2k3IwbniQOz9Uxphhub+P/1zyJfz3AuM7vR+XXtZnZrYEWFJbW5uN3YmIvEcoaEweHWLDngSb98VZUJvfrT7pG+cca7fHeejlVo42pzDgQ/OKuOni4ry8vn8m+XJ6ugKYamaTzCwC3AY8lo0dO+eWO+furKyszMbuRETOaOa4MOCN9Bb/qt8f5//8uokfPt3M0eYUE6qCfPmWCm5dVDJogh9y0PI3sweBy4ERZrYH+Jpz7j4zuwt4BggC9zvn3hno2kREemvOhDCPvtbGul1x3e/vQ1sPxHliVTtv7/RO7kqjxpIFxVxxXpRAnt3G1xO5GO1/ezfLnwSezPb3qdtfRAZC9fAgQ0qN4y2O3UeSTBiRL1dVpbdSKcea7XGeW9vO1vQMjtGw18X/oXlFlETzpfM8c74/Op1zy4HldXV1n8l1LSLiX2bGeRMivLyhg3U74wr/QezwiSSvbOzglU0xjjR5EzeVRI3LZkf50LwiyosHb+ifpKNTRCRL5kwM8/KGDlZtjXHdhfkzj7ucW0t7irU74ry2qYONe0/fsllVEeCqeUUsmhElGh583fvdUfiLiGTJnAlhSqLGrsNJdh1OqPWf5060plizPcbqrTE27UuQTM/OHA7CBZMjLJoZZXp1KG8n6ukL3x+ZuuYvIgMlHDIumhbhhbc7eGVDBxPe7/s/sYNKIunYdjDB+t1x1u+Os6MhyckJYAIGM8eFuHBKhAW1kUF9Pb8nfH9k6pq/iAykS2dEeeHtDl7dHOPGi0oojpy51bjrUILfr+9g/Z44R5tSBAJQPSzI/MkRLpsd9X34DATnvIctvbMrzju742zaG6e9052YoQDMHB/mwskR5taEfXEtv6d8H/4iIgNpQlWIqWNCbNmf4KlVbdx8Scm71h84luRXr7eyeluX+QBSsL0hyfaGNp5e086NC4u5/LyoL7uc+1NLe4qNe9Ot+z1xDnd50uKYoQFmjQ8ze3yYqWPDFPnoOn4mfB/+6vYXkYF2yyXF/MOjTTy9tp3xVUEW1EY5eDzJU6vbeHVTjJTzrit/YFaUhdOiVA8Lkkg6tuxP8Nyb7Wzel+DB37fy5vYYn7yyjCGlhdMizVQ86dh6IMGGdNjvPJTEdZrMvTRqzBof9l7jQgwrz/+pdweCOVcYU97X1dW5lStX5roMESkQj73RyvKV7QAUR4y2mPe3NmCwaEaUGxYUM6zsvaHunHdv+X+82EJzu2NIqXHXteVMHOn7tlqPOOfYeyTJ+j3xU9Mpxzo9TykYgNrRIWaOCzN7QpgJI4KDchKebDCzVc65ujOt09EkItIPliwopiQa4PGVbbR0OIojxvmTwtxQV8zIyu5bn2bGBZMjTB4V4t+ea2bzvgT/+OsTfOrKMi6cEhnAf0H+iCUcG/fEeWtnnLd2xDnW8u6u/OphQWaODzFrXJhpY8O+uiWvv6jlLyLSj5IpR1Obo6LYMm6BJpKO//tfLfxho/eM+JsvLmbx/KKCmDr4eEuKt3bEeGtnnA173t26ryxJd+WPCzNzfFhPUeyGWv4iIjkSDBhDSnsX1qGg8fErShk9NMgvX23j0dfaONKU4vb3lxD0YVf28ZYUq7bGWLk1Rv3+xLvWTawKMndimLk1ESZUBTUQso98H/4a8Ccig5mZsXh+MVUVQX78fDP/9U4HR5tT3Hl1mS9Gqp9oTbF6W4yV9TE270ucuu8+HIRZ48PMrQkzd2JEgx6zTN3+IiKDRP3+OPc+1Uxzu2NCVZC/vL58UHZ5N7enWLMtxor6GBv3Jk6Nzg8FYPaEMAtqI8yriVDUzRwJ0jNn6/ZX+IuIDCIHjie55/EmDp1IMbw8wF9eX87YYfl/+1prR4o12+OsrI+xYU/81FS6wQDMGhemrjbC+ZPCmtwoixT+KPxFxD+a2lL84Mkmth9MUhI1/tviMqZXh3Nd1nu0xRxvbvda+O/sPh34AYMZ40LUTYlyweQwpUUK/P6gAX8iIj5SXhzgi0sruO/5ZtZsj/Pd5U3cdFExH5pXlPN72ttijjd3eNfw39kVJ5EOfDOYXh1iQW2ECyZHCmoq3Xzk+5Z/pwF/n9myZUuuyxERyZpUyvGLV1t5/s0OACaNCnLbpaVMHjWw7bqmthTrdsVZsy3G27viJJLecgOmjg1RNyXCBVMig3J8wmCmbn/U7S8i/vX2zhj//mILx1u8v+cXTvEmExo3vH9OApIpx+7DSdbtivP2zhjbD55+Op4BtWNC1KVb+BqlnzsKfxT+IuJvrR0pnl7dzvNvtRNPt7ynjQ2dGjk/9AxTCfdEIuk9GW//0SS7jyTYeiDBtgMJOjrdhh8KwLTqEHMnRrhwigI/Xyj8UfiLSGE42pzi6TVtvLKxg45ODw4cUmpMrAoxvDzA0LIARWEjHDTMvIfjxBIQizuaO1I0tjgaW1Mcb0lxpCl1aqBeZyMrA0yvDjN3YpgZ4wr36Xj5TAP+REQKxLCyAH/0/lJuuqiEVVtjrN0eY9PeOMdbHMdb4ufewRkMLw8wdmiQscOCTB4donZ0iApdvx/UFP4iIj5UHDEunRnl0plRUs7RcDzF3qNJjjanONacoiPuSKQcqRREQkYkBOGQURo1KksCVJYGGFISYFh5QA/K8SGFv4iIzwXMGD00yOih+T8ZkAwM3/fbmNkSM1vW2NiY61JERETygu/D3zm33Dl3Z2VlZa5LERERyQu+D38RERF5N4W/iIhIgVH4i4iIFBiFv4iISIFR+IuIiBQYhb+IiEiBUfiLiIgUGIW/iIhIgfH99L5mtgRYApwwsy3dbFYJdDcFYHfrRgCH+15hvzrbvysf9t/bz/f0cz3Z7lzb6NjI3f57s49MPtPX46O36/L9+NCx4Z+/HRO7XeOcK/gXsCzTdcDKXNfdl39XPuy/t5/v6ed6st25ttGxkbv992YfmXymr8dHH9bl9fGhY6Mw/nao29+zvJfr8l1/197X/ff28z39XE+2O9c2OjZyt//e7COTz/T1+NCxkbv95/uxca71OT8+LH22IRkys5XOubpc1yH5R8eGnI2OD+nOQB4bavn33rJcFyB5S8eGnI2OD+nOgB0bavmLiIgUGLX8RURECozCX0REpMAo/EVERAqMwj9LzKzUzH5qZv9mZn+c63okf5jZZDO7z8weyXUtkl/M7MPpvxk/N7Orc12P5A8zm2lmPzKzR8zsz7K9f4X/WZjZ/WbWYGbruixfbGabzKzezL6UXnwz8Ihz7jPA0gEvVgZUJseGc26bc+6O3FQqAy3DY+PX6b8ZnwM+lot6ZeBkeGxscM59DrgVWJTtWhT+Z/cAsLjzAjMLAvcC1wKzgNvNbBYwDtid3iw5gDVKbjxAz48NKSwPkPmx8Tfp9eJvD5DBsWFmS4EngCezXYjC/yyccy8BR7ssXgjUp1tzMeAh4EZgD94JAOh/V9/L8NiQApLJsWGefwCecs6tHuhaZWBl+nfDOfeYc+5aIOuXkhVSmavmdAsfvNCvBh4FbjGzfyEPpm6UnDjjsWFmw83sR8B8M/tybkqTHOvu78ZfAFcBHzGzz+WiMMm57v5uXG5m95jZv9IPLX/fP9VvoDjnWoBP5roOyT/OuSN413RF3sU5dw9wT67rkPzjnHsReLG/9q+Wf+b2AuM7vR+XXiaiY0O6o2NDupOTY0Phn7kVwFQzm2RmEeA24LEc1yT5QceGdEfHhnQnJ8eGwv8szOxB4FVgupntMbM7nHMJ4C7gGWAD8LBz7p1c1ikDT8eGdEfHhnQnn44NPdhHRESkwKjlLyIiUmAU/iIiIgVG4S8iIlJgFP4iIiIFRuEvIiJSYBT+IiIiBUbhL+JjZna3mbn070PS7y/IYT3np2sYdoZ1zszuzkFZIgVH4S/ibz8GLkn/PgT4GpCz8AfOT9fwnvDHq/PHA1uOSGHSg31EfMw5twfvKWH9wswMCKcfRdonzrnXslCSiPSAWv4iPnay29/MaoDt6cX/ll7mzOwTnba92cxeM7NWMztuZr8wswld9rfDzP6vmX3KzDYCMeD69Lqvm9lqMzthZofN7HdmdnGnz34C+En67ZZONdSk17+n29/MFpvZq2bWZmaNZvZrM5veZZsXzexlM7sq/f2tZrbOzG7q4/98Ir6l8BcpDPuBm9O/fxOvi/0S4AmA9LPkfwmsBz4CfBY4D/gvMyvvsq8rgC8AXwcWA2+ll1cD3wVuBD4BNAAvmdmc9PongL9L//7RTjXsP1PBZrY4/Zlm4GPAn6VretnMqrtsPgX4PvCd9L9zP/ALM6s96/8qIgVK3f4iBcA512Fma9Jvt3XuYjezMuAfgJ845z7VafkbwCbgDuB7nXY3FLjQOXegy3d8utNng8DTwDvAp4H/7pw7ZGZb05usdc7Vn6PsvwO2AdemH36Cmb0KbAa+iHcCctII4APOuS3p7VbjnQDcCvz9Ob5HpOCo5S8ilwAVwM/MLHTyBewGNgIf6LL9a12DHyDd7f6CmR0BEkAcmAZM77rtuZhZKd7AxJ+fDH4A59x24A/AZV0+suVk8Ke3a8DreZiAiLyHWv4iMjL98/lu1h/r8v493fTp2wefxHss6R3pbZJ4o/eLelHTUMDO9F3AAWBil2VHz7BdRy+/W8T3FP4iciT98xN43fRdNXV5f6bngN+C19q/2TkXP7nQzIYCx3tR07H094w+w7rRnDnsRaSHFP4ihaMj/bO4y/JX8AK+1jn3017uuwSvpX/qxMDMPojX7b6903bd1fAuzrkWM1sFfNTM7nbOJdP7nAi8D/hBL+sUERT+IoXkIF4r/zYzewtoAbY7546Y2f8A7jWzKuApoBFv9P5lwIvOuf88x76fBj4PPGBmP8G71v8VYG+X7danf/65mf0Ub1zAW93ME/AVvNH+j5vZD4EyvDsMGoFvZ/DvFpEuNOBPpEA451J4I++H4l3fXwEsSa/7V2Ap3uC8/8C7fn83XgNhbQ/2/Qzwl8Ai4HHgU8CfAvVdtnszvd8lwMvpGsZ2s8+n8eYQGAI8DPwI2ABc6pzb18N/toicgTl3pst3IiIi4ldq+YuIiBQYhb+IiEiBUfiLiIgUGIW/iIhIgVH4i4iIFBiFv4iISIFR+IuIiBQYhb+IiEiBUfiLiIgUmP8H3LcndET+qeUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1296x432 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    }
  ]
}