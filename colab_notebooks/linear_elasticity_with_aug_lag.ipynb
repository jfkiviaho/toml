{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "linear_elasticity_with_aug_lag.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPCcJLsTwGtZXEHaH8gQyZK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jfkiviaho/toml/blob/master/linear_elasticity_with_aug_lag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrfBxKpUT0rI"
      },
      "source": [
        "'''\n",
        "Linear elastic isotropic material in tension\n",
        "    \n",
        "    div(sigma(u)) + f = 0 in rectangular domain\n",
        "                   ux = 0 on left boundary\n",
        "            sigma . n = 0 on bottom boundary\n",
        "            sigma . n = 0 on top boundary\n",
        "            sigma . n = t on right boundary\n",
        "\n",
        "'''\n",
        "# Load required modules\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "import numpy as np\n",
        "import numpy.linalg as la\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from math import ceil\n",
        "from functools import reduce\n",
        "\n",
        "# Set floating point precision in Keras to double\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "# Set global random seed\n",
        "# Note: this is a shortcut for getting reproducible results.\n",
        "#tf.random.set_seed(1)\n",
        "\n",
        "# Auxiliary function\n",
        "def compose2(f, g):\n",
        "    return lambda x: f(g(x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxdiGeehT7E_"
      },
      "source": [
        "# Define model\n",
        "class Model(keras.Sequential):\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Initialize model with parameters, collocation points, and network \n",
        "        topology\n",
        "\n",
        "        '''\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        ########################################################################\n",
        "        # Equation and discretization parameters\n",
        "        ########################################################################\n",
        "        self.K       = 1.0  # Bulk modulus\n",
        "        self.G       = 1.0  # Shear modulus\n",
        "        self.L       = 1.0  # length of rectangular domain\n",
        "        self.h       = 1.0  # height of rectangular domain\n",
        "        self.sig_app = 1.0  # applied stress\n",
        "\n",
        "        self.lam = self.K - 2*self.G/3  # 1st Lame parameter\n",
        "        self.mu  = self.G               # 2nd Lame parameter\n",
        "\n",
        "        self.num_x_pts = 100\n",
        "        self.num_y_pts = 100\n",
        "\n",
        "        ########################################################################\n",
        "        # Augmented Lagrangian parameters\n",
        "        ########################################################################\n",
        "        self.num_cons = 4\n",
        "\n",
        "        self.max_con_iters  = 10**2  # max. num. of constrained iterations\n",
        "        self.max_unc_iters  = 30**1  # max. num. of unconstrained iterations\n",
        "\n",
        "        self.tol_unc        = 1e-7   # loss gradient tolerance\n",
        "        self.tol_con        = 1e-8   # constraint tolerance\n",
        "\n",
        "        self.penalty        = 1.0    # penalty parameter\n",
        "        self.max_penalty    = 1e5    # maximum penalty \n",
        "        self.penalty_growth = 2.0    # penalty growth\n",
        "\n",
        "        self.lambdas  = np.zeros(self.num_cons, dtype=np.float64)  # multipliers\n",
        "\n",
        "        ########################################################################\n",
        "        # Unconstrained optimization parameters\n",
        "        ########################################################################\n",
        "        # The unconstrained optimization that is performed within the\n",
        "        # constrained optimization in the Augmented Lagrangian framework will\n",
        "        # be undertaken by a derivative of the stochastic gradient descent\n",
        "        # method (SGD). The challenge addressed here is how to batch the\n",
        "        # multiple datasets (domain and boundary sample points) and how to\n",
        "        # repeat those datasets so that, during each epoch of training, every\n",
        "        # batch is seen at least once\n",
        "        self.optimizer = tf.optimizers.Adam()\n",
        "\n",
        "        self.num_pts = [\n",
        "                self.num_x_pts*self.num_y_pts,  # for domain\n",
        "                self.num_y_pts,                 # for left boundary\n",
        "                self.num_x_pts,                 # for bottom boundary\n",
        "                self.num_y_pts,                 # for right boundary\n",
        "                self.num_x_pts                  # for top boundary\n",
        "            ]\n",
        "\n",
        "        self.batch_sizes = [\n",
        "                0.02*self.num_pts[0],  \n",
        "                0.50*self.num_pts[1],  \n",
        "                0.50*self.num_pts[2],  \n",
        "                0.50*self.num_pts[3],  \n",
        "                0.50*self.num_pts[4]   \n",
        "            ]\n",
        "        #self.batch_sizes = [10, 5, 5, 5, 5]\n",
        "        self.batch_sizes = list(map(round, self.batch_sizes))\n",
        "        print(self.batch_sizes)\n",
        "\n",
        "        num_batches_per_epoch = [\n",
        "                self.num_pts[0]/self.batch_sizes[0],\n",
        "                self.num_pts[1]/self.batch_sizes[1],\n",
        "                self.num_pts[2]/self.batch_sizes[2],\n",
        "                self.num_pts[3]/self.batch_sizes[3],\n",
        "                self.num_pts[4]/self.batch_sizes[4]\n",
        "            ]\n",
        "        num_batches_per_epoch = list(map(ceil, num_batches_per_epoch))\n",
        "        print(num_batches_per_epoch)\n",
        "\n",
        "        max_num_batches  = reduce(max, num_batches_per_epoch)\n",
        "\n",
        "        self.num_repeats = [\n",
        "                max_num_batches/num_batches_per_epoch[0],\n",
        "                max_num_batches/num_batches_per_epoch[1],\n",
        "                max_num_batches/num_batches_per_epoch[2],\n",
        "                max_num_batches/num_batches_per_epoch[3],\n",
        "                max_num_batches/num_batches_per_epoch[4]\n",
        "            ]\n",
        "        self.num_repeats = list(map(ceil, self.num_repeats))\n",
        "        print(self.num_repeats)\n",
        "\n",
        "        ########################################################################\n",
        "        # Collocation points\n",
        "        ########################################################################\n",
        "        x0 = np.array([0.0])\n",
        "        x1 = np.array([self.L])\n",
        "        x = np.linspace(x0, x1, self.num_x_pts)\n",
        "\n",
        "        y0 = np.array([0.0])\n",
        "        y1 = np.array([self.h])\n",
        "        y = np.linspace(y0, y1, self.num_y_pts)\n",
        "\n",
        "        x_mesh, y_mesh = np.meshgrid(x, y)\n",
        "\n",
        "        # Compute normalization constants\n",
        "        self.mu_x = x.mean()\n",
        "        self.sig_x = x.std()\n",
        "\n",
        "        self.mu_y = y.mean()\n",
        "        self.sig_y = y.std()\n",
        "\n",
        "        self.x_coords = [\n",
        "                x_mesh.flatten().reshape(-1,1),\n",
        "                x0*np.ones(self.num_pts[1]).reshape(-1,1),\n",
        "                np.linspace(x0, x1, self.num_pts[2]),\n",
        "                x1*np.ones(self.num_pts[3]).reshape(-1,1),\n",
        "                np.linspace(x0, x1, self.num_pts[4])\n",
        "            ]\n",
        "        self.X_coords = list(map(self.normalize_x, self.x_coords))\n",
        "\n",
        "        self.y_coords = [\n",
        "                y_mesh.flatten().reshape(-1,1),\n",
        "                np.linspace(y0, y1, self.num_pts[1]),\n",
        "                y0*np.ones(self.num_pts[2]).reshape(-1,1),\n",
        "                np.linspace(y0, y1, self.num_pts[3]),\n",
        "                y1*np.ones(self.num_pts[4]).reshape(-1,1)\n",
        "            ]\n",
        "        self.Y_coords = list(map(self.normalize_y, self.y_coords))\n",
        "\n",
        "        self.all_pts = tuple(\n",
        "                zip(\n",
        "                    map(tf.constant, self.X_coords), \n",
        "                    map(tf.constant, self.Y_coords)\n",
        "                )\n",
        "            )\n",
        "\n",
        "       ########################################################################\n",
        "        # Neural network topology\n",
        "        ########################################################################\n",
        "        # Add first layer\n",
        "        input_layer = keras.layers.Dense(units=2, input_shape=[None,2])\n",
        "        self.add(input_layer)\n",
        "\n",
        "        # Add hidden layers\n",
        "        num_neurons_per_hidden_layer = [20]\n",
        "\n",
        "        for num_neurons in num_neurons_per_hidden_layer:\n",
        "            self.add(\n",
        "                    keras.layers.Dense(\n",
        "                        units=num_neurons, \n",
        "                        activation=tf.nn.tanh\n",
        "                    )\n",
        "                )\n",
        "        \n",
        "        # Add last layer\n",
        "        output_layer = keras.layers.Dense(units=2)\n",
        "        self.add(output_layer)\n",
        "\n",
        "        ########################################################################\n",
        "        # Logging\n",
        "        ########################################################################\n",
        "\n",
        "        # Optimization history arrays\n",
        "        self.iters = [0]\n",
        "        #self.res_hist = [self.eval_res().numpy()]\n",
        "        #self.bc_hist = [\n",
        "        #        [self.eval_bnd_cnd(0).numpy()],\n",
        "        #        [self.eval_bnd_cnd(1).numpy()],\n",
        "        #        [self.eval_bnd_cnd(2).numpy()],\n",
        "        #        [self.eval_bnd_cnd(3).numpy()]\n",
        "        #    ]\n",
        "\n",
        "        return\n",
        "\n",
        "    def normalize_x(self, pts):\n",
        "        return (pts - self.mu_x)/self.sig_x\n",
        "\n",
        "    def normalize_y(self, pts):\n",
        "        return (pts - self.mu_y)/self.sig_y\n",
        "\n",
        "    def make_datasets(self):\n",
        "        '''\n",
        "        Make a list of appropriately shuffled, batched, and repeated datasets\n",
        "        for the domain and boundary points\n",
        "        '''\n",
        "        ds_list = [\n",
        "                tf.data.Dataset.from_tensor_slices(\n",
        "                        (X, Y)\n",
        "                    ).shuffle(buf_size).repeat(num_rpt).batch(bat_size)\n",
        "                for X, Y, buf_size, num_rpt, bat_size in \n",
        "                zip(\n",
        "                        self.X_coords,\n",
        "                        self.Y_coords,\n",
        "                        self.num_pts,\n",
        "                        self.num_repeats,\n",
        "                        self.batch_sizes\n",
        "                    )\n",
        "            ]\n",
        "\n",
        "        return ds_list\n",
        "\n",
        "    def eval_strain(self, X, Y):\n",
        "        '''\n",
        "        Evaluate strain at given coordinates\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: TensorFlow tensor\n",
        "            contains N number of normalized x coordinates at which to evalaute\n",
        "        Y: TensorFlow tensor\n",
        "            contains N number of normalized y coordinates at which to evalaute\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        eps: NumPy array\n",
        "            the strain tensor at each point in a (N, 2, 2)-shape array\n",
        "        '''\n",
        "        dXdx = 1.0/self.sig_x\n",
        "        dYdy = 1.0/self.sig_y\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as t2:\n",
        "            t2.watch(X)\n",
        "            t2.watch(Y)\n",
        "\n",
        "            uvec = self.__call__(tf.stack([X, Y], axis=2))\n",
        "            u = tf.slice(uvec, [0,0,0], [-1,-1,1])\n",
        "            v = tf.slice(uvec, [0,0,1], [-1,-1,1])\n",
        "        \n",
        "        # Compute displacement derivatives (gradient)\n",
        "        u_x = t2.gradient(u, X)*dXdx\n",
        "        u_y = t2.gradient(u, Y)*dYdy\n",
        "        v_x = t2.gradient(v, X)*dXdx\n",
        "        v_y = t2.gradient(v, Y)*dYdy\n",
        "\n",
        "        # Compute strain from displacement derivatives\n",
        "        epsxx = 0.5*(u_x + u_x)\n",
        "        epsyx = 0.5*(v_x + u_y)\n",
        "        epsxy = 0.5*(u_y + v_x)\n",
        "        epsyy = 0.5*(v_y + v_y)\n",
        "\n",
        "        # Stack the strain components so that there's a 2D tensor for each\n",
        "        # point\n",
        "        eps = tf.squeeze(\n",
        "                tf.stack(\n",
        "                    [\n",
        "                        tf.stack([epsxx, epsyx], axis=2),\n",
        "                        tf.stack([epsxy, epsyy], axis=2)\n",
        "                    ], \n",
        "                    axis=3\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return eps\n",
        "\n",
        "    def eval_stress(self, X, Y):\n",
        "        '''\n",
        "        Evaluate stress at given coordinates\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: TensorFlow tensor\n",
        "            contains N number of normalized x coordinates at which to evalaute\n",
        "        y: TensorFlow tensor\n",
        "            contains N number of normalized y coordinates at which to evalaute\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        sig: NumPy array\n",
        "            the stress tensor at each point in a (n, 2, 2)-shape array\n",
        "        '''\n",
        "        # Evaluate the strain\n",
        "        eps = self.eval_strain(X, Y)\n",
        "        if len(eps.shape) == 3:\n",
        "            epsxx = tf.slice(eps, [0,0,0], [-1,1,1])\n",
        "            epsyx = tf.slice(eps, [0,1,0], [-1,1,1])\n",
        "            epsxy = tf.slice(eps, [0,0,1], [-1,1,1])\n",
        "            epsyy = tf.slice(eps, [0,1,1], [-1,1,1])\n",
        "        elif len(eps.shape) == 4:\n",
        "            epsxx = tf.slice(eps, [0,0,0,0], [-1,-1,1,1])\n",
        "            epsyx = tf.slice(eps, [0,0,1,0], [-1,-1,1,1])\n",
        "            epsxy = tf.slice(eps, [0,0,0,1], [-1,-1,1,1])\n",
        "            epsyy = tf.slice(eps, [0,0,1,1], [-1,-1,1,1])\n",
        "        else:\n",
        "            error_msg = 'Strain tensor array has incorrect shape.'\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "        # Compute the stress from the strain\n",
        "        sigxx = self.lam*(epsxx + epsyy) + 2.0*self.mu*epsxx\n",
        "        sigyx =                      0.0 + 2.0*self.mu*epsyx\n",
        "        sigxy =                      0.0 + 2.0*self.mu*epsxy\n",
        "        sigyy = self.lam*(epsxx + epsyy) + 2.0*self.mu*epsyy\n",
        "\n",
        "        # Stack the stress components so that there's a 2D tensor for each\n",
        "        # point\n",
        "        sig = tf.squeeze(\n",
        "                tf.stack(\n",
        "                    [\n",
        "                        tf.stack([sigxx, sigyx], axis=2),\n",
        "                        tf.stack([sigxy, sigyy], axis=2)\n",
        "                    ], \n",
        "                    axis=3\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return sig\n",
        "\n",
        "    def eval_res(self, X, Y):\n",
        "        '''\n",
        "        Evaluate residual of governing equation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: TensorFlow tensor\n",
        "            normalized x coordinates at which to evalaute\n",
        "        y: TensorFlow tensor\n",
        "            normalized y coordinates at which to evalaute\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        res: TensorFlow tensor\n",
        "            evaluation of residual\n",
        "        '''\n",
        "        dXdx = 1.0/self.sig_x\n",
        "        dYdy = 1.0/self.sig_y\n",
        "\n",
        "        # Evaluate residual of governing equation\n",
        "        with tf.GradientTape(persistent=True) as t1:\n",
        "            t1.watch(X)\n",
        "            t1.watch(Y)\n",
        "\n",
        "            # Evaluate the stress\n",
        "            sig = self.eval_stress(X, Y)\n",
        "            sigxx = tf.slice(sig, [0,0,0], [-1,1,1])\n",
        "            sigyx = tf.slice(sig, [0,1,0], [-1,1,1])\n",
        "            sigxy = tf.slice(sig, [0,0,1], [-1,1,1])\n",
        "            sigyy = tf.slice(sig, [0,1,1], [-1,1,1])\n",
        "\n",
        "        # Compute derivatives (divergence) of stress\n",
        "        sigxx_x = t1.gradient(sigxx, X)*dXdx\n",
        "        sigxy_y = t1.gradient(sigxy, Y)*dYdy\n",
        "        sigyx_x = t1.gradient(sigyx, X)*dXdx\n",
        "        sigyy_y = t1.gradient(sigyy, Y)*dYdy\n",
        "\n",
        "        # Compute residual\n",
        "        N = np.product(X.shape)\n",
        "        resx = (1/N)*tf.square(sigxx_x + sigxy_y)\n",
        "        resy = (1/N)*tf.square(sigyx_x + sigyy_y)\n",
        "\n",
        "        res = tf.reduce_sum(resx + resy)\n",
        "        \n",
        "        return res\n",
        "\n",
        "    def eval_bnd_cnd(self, X, Y, k):\n",
        "        '''\n",
        "        Evaluate boundary conditions\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: TensorFlow tensor\n",
        "            normalized x coordinates at which to evalaute\n",
        "        Y: TensorFlow tensor\n",
        "            normalized y coordinates at which to evalaute\n",
        "        k: int\n",
        "            boundary indicator (LEFT = 0, BOTTOM = 1, RIGHT = 2, TOP = 3)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        bnd_cnd: TensorFlow tensor\n",
        "            boundary condition violation\n",
        "\n",
        "        '''\n",
        "        N = np.product(X.shape)\n",
        "\n",
        "        # Left fixed-displacement (Dirichlet) boundary condition \n",
        "        # u = 0 at x = 0\n",
        "        if k == 0:\n",
        "            # Evaluate predicted displacement at boundary points\n",
        "            uvec = self.__call__(tf.stack([X, Y], axis=2))\n",
        "            u = tf.slice(uvec, [0,0,0], [-1,1,1])\n",
        "\n",
        "            # Compute measure of boundary condition violation\n",
        "            bnd_cnd = (1/N)*tf.reduce_sum(tf.square(u))\n",
        "\n",
        "        # Top and bottom traction-free (Neumann) boundary conditions\n",
        "        # sigxy = sigyy = 0 at y = 0\n",
        "        elif k == 1 or k == 3:\n",
        "            # Evaluate the stress\n",
        "            sig = self.eval_stress(X, Y)\n",
        "            sigxx = tf.slice(sig, [0,0,0], [-1,1,1])\n",
        "            sigyx = tf.slice(sig, [0,1,0], [-1,1,1])\n",
        "            sigxy = tf.slice(sig, [0,0,1], [-1,1,1])\n",
        "            sigyy = tf.slice(sig, [0,1,1], [-1,1,1])\n",
        "\n",
        "            # Compute measure of boundary condition violation\n",
        "            bnd_cnd = (1/N)*tf.reduce_sum(\n",
        "                    tf.square(sigxy) + \n",
        "                    tf.square(sigyy)\n",
        "                )\n",
        "\n",
        "        # Right non-zero traction (Neumann) boundary condition\n",
        "        # sigyx = 0, sigxx = sig_app at x = L\n",
        "        elif k == 2:\n",
        "            # Evaluate the stress\n",
        "            sig = self.eval_stress(X, Y)\n",
        "            sigxx = tf.slice(sig, [0,0,0], [-1,1,1])\n",
        "            sigyx = tf.slice(sig, [0,1,0], [-1,1,1])\n",
        "            sigxy = tf.slice(sig, [0,0,1], [-1,1,1])\n",
        "            sigyy = tf.slice(sig, [0,1,1], [-1,1,1])\n",
        "\n",
        "            bnd_cnd = (1/N)*tf.reduce_sum(\n",
        "                    tf.square(sigxx - self.sig_app) + \n",
        "                    tf.square(sigyx)\n",
        "                )\n",
        "\n",
        "        else:\n",
        "            error_msg = 'No boundary matching index specified.'\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "        return bnd_cnd\n",
        "\n",
        "    def eval_loss(self, pts):\n",
        "        '''\n",
        "        Evaluate loss function for given set of domain and boundary points\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        pts: tuple\n",
        "            tuple containing tuples of TensorFlow tensors containing x, y\n",
        "            coordinates for the domain + boundaries\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        loss: TensorFlow tensor\n",
        "            loss function evaluation \n",
        "        '''\n",
        "        # Add objective, i.e. residual of governing equations\n",
        "        # in domain, to the loss function (Augmented Lagrangian)\n",
        "        X_dom, Y_dom = pts[0]\n",
        "        loss = self.eval_res(X_dom, Y_dom)\n",
        "\n",
        "        # Add multiplier terms and penalty terms for\n",
        "        # constraints, i.e. boundary conditions, to the\n",
        "        # Augmented Lagrangian\n",
        "        for k in range(self.num_cons):\n",
        "            X_bnd, Y_bnd = pts[k+1]\n",
        "            bnd_cnd = self.eval_bnd_cnd(\n",
        "                    X_bnd, Y_bnd, k\n",
        "                )\n",
        "            loss += self.lambdas[k]*bnd_cnd \n",
        "            loss += self.penalty*tf.square(bnd_cnd)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def train(self):\n",
        "        '''\n",
        "        Train the neural network using a Augmented Lagrangian approach to the\n",
        "        optimization of the residual and the enforcement of the boundary\n",
        "        conditions. The unconstrained optimization in the inner loop of the\n",
        "        Augment Lagrangian procedure is performed by the Adam optimizer from\n",
        "        TensorFlow\n",
        "        \n",
        "        '''\n",
        "        # Counters \n",
        "        it = 0      # total iterations\n",
        "        it_con = 0  # constrained iterations\n",
        "        it_unc = 0  # unconstrained iterations\n",
        "\n",
        "        self.lambdas = np.zeros(self.num_cons, dtype=np.float64)  # multipliers\n",
        "        self.penalty = 1.0                                        # penalty \n",
        "        \n",
        "        norm_loss_grad = 1.0  # norm of the loss function gradient\n",
        "\n",
        "        # Perform constrained optimization using Augmented Lagrangian framework\n",
        "        while it_con < self.max_con_iters:\n",
        "            # Perform unconstrained optimization\n",
        "            it_unc = 0\n",
        "            while it_unc < self.max_unc_iters:\n",
        "                # Create datasets\n",
        "                ds_list = self.make_datasets()\n",
        "\n",
        "                # Loop over batches in datasets and update parameters using\n",
        "                # gradient information\n",
        "                print('Epoch', it_unc+1)\n",
        "                for i, batches in enumerate(zip(*ds_list)):\n",
        "                    # Evaluate loss function and gradient\n",
        "                    with tf.GradientTape() as t:\n",
        "                        loss = self.eval_loss(batches)\n",
        "\n",
        "                    loss_grad = t.gradient(loss, self.trainable_variables)\n",
        "\n",
        "                    print('batch', i, 'loss', loss.numpy())\n",
        "\n",
        "                    # Check for None-valued components in the gradient. Replace\n",
        "                    # these with zero-valued tensors\n",
        "                    for i, comp in enumerate(loss_grad):\n",
        "                        if comp is None:\n",
        "                            loss_grad[i] = tf.zeros(\n",
        "                                    self.trainable_variables[i].shape,\n",
        "                                    dtype=tf.float64\n",
        "                                )\n",
        "\n",
        "                    # Apply optimization step to model parameters\n",
        "                    self.optimizer.apply_gradients(\n",
        "                            zip(\n",
        "                                loss_grad, \n",
        "                                self.trainable_variables\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "                # Evaluate norm of loss function gradient and determine\n",
        "                # whether to exit unconstrained optimization\n",
        "                with tf.GradientTape() as t:\n",
        "                    loss = self.eval_loss(self.all_pts)\n",
        "\n",
        "                loss_grad = t.gradient(loss, self.trainable_variables)\n",
        "\n",
        "                # Check for None-valued components in the gradient. Replace\n",
        "                # these with zero-valued tensors\n",
        "                for i, comp in enumerate(loss_grad):\n",
        "                    if comp is None:\n",
        "                        loss_grad[i] = tf.zeros(\n",
        "                                self.trainable_variables[i].shape,\n",
        "                                dtype=tf.float64\n",
        "                            )\n",
        "\n",
        "                norm_loss_grad = tf.sqrt(\n",
        "                        reduce(\n",
        "                            lambda x, y: x + y,\n",
        "                            map(\n",
        "                                compose2(tf.reduce_sum, tf.square),\n",
        "                                loss_grad\n",
        "                            )\n",
        "                        )\n",
        "                    ).numpy()\n",
        "\n",
        "                res, bcs = self.eval_model()\n",
        "\n",
        "                print()\n",
        "                print('res:', res)\n",
        "                for k, bc in enumerate(bcs):\n",
        "                    print('bc{}:'.format(k), bc)\n",
        "                print()\n",
        "                print()\n",
        "\n",
        "                if loss.numpy() < self.tol_unc and norm_loss_grad < self.tol_unc:\n",
        "                    break\n",
        "\n",
        "                it_unc += 1\n",
        "                it += 1\n",
        "\n",
        "            # Evaluate the constraints and determine whether to exit\n",
        "            # constrained optimization\n",
        "            res, cons = self.eval_model()\n",
        "\n",
        "            norm_cons = la.norm(cons)\n",
        "\n",
        "            print('finished unconstrained optimization')\n",
        "            print('res:', res)\n",
        "            for k, bc in enumerate(cons):\n",
        "                print('bc{}:'.format(k), bc)\n",
        "            print()\n",
        "            print()\n",
        "\n",
        "            if norm_cons < self.tol_con:\n",
        "                break\n",
        "\n",
        "            # Update the multipliers and penalty parameter\n",
        "            self.lambdas += 2.0*self.penalty*cons\n",
        "            self.penalty = min(self.penalty_growth*self.penalty, self.max_penalty)\n",
        "            print(self.lambdas)\n",
        "            print(self.penalty)\n",
        "\n",
        "            it_con += 1\n",
        "\n",
        "        return\n",
        "\n",
        "    def eval_model(self):\n",
        "        '''\n",
        "        Evaluate residual over all domain points and boundary condition\n",
        "        violations over all boundary points\n",
        "        '''\n",
        "        X_dom = tf.constant(self.X_coords[0])\n",
        "        Y_dom = tf.constant(self.Y_coords[0])\n",
        "        res = model.eval_res(X_dom, Y_dom).numpy()\n",
        "\n",
        "        bcs = np.empty(self.num_cons, dtype=np.float64)\n",
        "        for k in range(self.num_cons):\n",
        "            X_bnd = tf.constant(self.X_coords[k+1])\n",
        "            Y_bnd = tf.constant(self.Y_coords[k+1])\n",
        "            bcs[k] = model.eval_bnd_cnd(X_bnd, Y_bnd, k).numpy()\n",
        "\n",
        "        return res, bcs\n",
        "\n",
        "    def predict_displacement(self, x, y):\n",
        "        '''\n",
        "        Return the stress tensor at each point specified\n",
        "\n",
        "        One can easily generate the appropriately shaped array of points using\n",
        "        the meshgrid function from NumPy.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: NumPy array\n",
        "            the x coordinates arranged in a (ny, nx)-shape array\n",
        "        y: NumPy array\n",
        "            the y coordinates arranged in a (ny, nx)-shape array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        disp: NumPy array\n",
        "            the displacements arranged in a (nx, ny, 2)-shape array\n",
        "\n",
        "        '''\n",
        "        # Extract coordinates and normalize\n",
        "        X = tf.constant(self.normalize_x(x))\n",
        "        Y = tf.constant(self.normalize_y(y))\n",
        "\n",
        "        # Evaluate displacements at specified coordinates\n",
        "        disp = tf.squeeze(self.__call__(np.stack([X, Y], axis=2)))\n",
        "\n",
        "        return disp.numpy()\n",
        "\n",
        "    def predict_stress_and_strain(self, x, y):\n",
        "        '''\n",
        "        Return the stress and strain tensors at each point specified\n",
        "\n",
        "        One can easily generate the appropriately shaped array of points using\n",
        "        the meshgrid function from NumPy.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: NumPy array\n",
        "            the x coordinates arranged in a (ny, nx)-shape array\n",
        "        y: NumPy array\n",
        "            the y coordinates arranged in a (ny, nx)-shape array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        sig: NumPy array\n",
        "            the stress tensor at each point in a (ny, nx, 2, 2)-shape array\n",
        "        eps: NumPy array\n",
        "            the strain tensor at each point in a (ny, nx, 2, 2)-shape array\n",
        "        '''\n",
        "        # Extract coordinates and normalize\n",
        "        X = tf.constant(self.normalize_x(x))\n",
        "        Y = tf.constant(self.normalize_y(y))\n",
        "\n",
        "        # Evaluate stress and strain\n",
        "        sig = self.eval_stress(X, Y)\n",
        "        eps = self.eval_strain(X, Y)\n",
        "        \n",
        "        return sig.numpy(), eps.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKfYzCayUJRO",
        "outputId": "ce456983-366f-4bbc-9edc-dff8954a1f28"
      },
      "source": [
        "# Solve problem of linear elastic material in tension and plot results\n",
        "if __name__ == '__main__':\n",
        "    ############################################################################\n",
        "    # Train the model and print residual and violations\n",
        "    ############################################################################\n",
        "    model = Model()\n",
        "    model.train()\n",
        "    res, bcs = model.eval_model()\n",
        "\n",
        "    print()\n",
        "    print('Model results')\n",
        "    print('-------------')\n",
        "    print('res:', res)\n",
        "    for k, bc in enumerate(bcs):\n",
        "        print('bc{}:'.format(k), bc)\n",
        "\n",
        "    # Create directory for storing plots\n",
        "    plot_dir = './test/'\n",
        "    Path(plot_dir).mkdir(exist_ok=True)\n",
        "\n",
        "    ############################################################################\n",
        "    # Plot displacements, stresses, strains\n",
        "    ############################################################################\n",
        "    # Generate new set of coordinates for plotting results\n",
        "    nx = 100\n",
        "    ny = 20\n",
        "    x0 = np.array([0.0])\n",
        "    x1 = np.array([model.L])\n",
        "    y0 = np.array([0.0])\n",
        "    y1 = np.array([model.h])\n",
        "    x = np.linspace(x0, x1, nx)\n",
        "    y = np.linspace(y0, y1, ny)\n",
        "    X, Y = np.meshgrid(x, y) \n",
        "\n",
        "    # Predict the displacements from the model\n",
        "    disp = model.predict_displacement(X, Y)\n",
        "    U = disp[:,:,0]\n",
        "    V = disp[:,:,1]\n",
        "\n",
        "    # Since the model is not constrained in v, any translation up or down is\n",
        "    # possible. Remove this bias for plotting\n",
        "    V = V - V.mean()\n",
        "\n",
        "    # Predict the stresses and strains from the model\n",
        "    sig, eps = model.predict_stress_and_strain(X, Y)\n",
        "\n",
        "    # Plot displacements\n",
        "    fig, ax = plt.subplots(figsize=(18,6))\n",
        "\n",
        "    ax.scatter(\n",
        "            X, Y, \n",
        "            color='cornflowerblue',\n",
        "            label='reference'\n",
        "        )\n",
        "    ax.scatter(\n",
        "            X + U, Y + V, \n",
        "            color='orange',\n",
        "            label='deformed'\n",
        "        )\n",
        "\n",
        "    ax.set_xlabel(r'x', fontsize=16)\n",
        "    ax.set_ylabel(r'y', fontsize=16)\n",
        "    ax.legend()\n",
        "    plt.axis('equal')\n",
        "    fig.savefig(plot_dir + 'disps.png')\n",
        "\n",
        "    # Plot stresses\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(18,6))\n",
        "    plt.subplots_adjust(\n",
        "            hspace=0.5,  # height of blank space between subplots?\n",
        "            wspace=0.5   # width of blank space between subplots?\n",
        "        )\n",
        "\n",
        "    for k in range(4):\n",
        "        # Do some binary math to organize plots into a 2 x 2 grid\n",
        "        i = (k & 1) >> 0\n",
        "        j = (k & 2) >> 1\n",
        "\n",
        "        num_lvls = 20\n",
        "        stress = sig[:,:,i,j]\n",
        "        lvls = np.linspace(stress.min(), stress.max(), num_lvls)\n",
        "        cset = ax[i,j].contourf(X, Y, stress, lvls)\n",
        "        fig.colorbar(cset, ax=ax[i,j])\n",
        "\n",
        "        ax[i,j].set_xlabel(r'$x$', fontsize=16)\n",
        "        ax[i,j].set_ylabel(r'$y$', fontsize=16)\n",
        "        lbl_str = 'xy'\n",
        "        ax[i,j].set_title(\n",
        "                r'$\\sigma_{{{0}{1}}}$'.format(lbl_str[i], lbl_str[j]), \n",
        "                fontsize=16\n",
        "            )\n",
        "        \n",
        "    fig.savefig(plot_dir + 'stresses.png')\n",
        "\n",
        "    # Plot strains\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(18,6))\n",
        "    plt.subplots_adjust(\n",
        "            hspace=0.5,  # height of blank space between subplots?\n",
        "            wspace=0.5   # width of blank space between subplots?\n",
        "        )\n",
        "\n",
        "    for k in range(4):\n",
        "        # Do some binary math to organize plots into a 2 x 2 grid\n",
        "        i = (k & 1) >> 0\n",
        "        j = (k & 2) >> 1\n",
        "\n",
        "        num_lvls = 20\n",
        "        strain = eps[:,:,i,j]\n",
        "        lvls = np.linspace(strain.min(), strain.max(), num_lvls)\n",
        "        cset = ax[i,j].contourf(X, Y, strain, lvls)\n",
        "        fig.colorbar(cset, ax=ax[i,j])\n",
        "\n",
        "        ax[i,j].set_xlabel(r'$x$', fontsize=16)\n",
        "        ax[i,j].set_ylabel(r'$y$', fontsize=16)\n",
        "        lbl_str = 'xy'\n",
        "        ax[i,j].set_title(\n",
        "                r'$\\epsilon_{{{0}{1}}}$'.format(lbl_str[i], lbl_str[j]), \n",
        "                fontsize=16\n",
        "            )\n",
        "\n",
        "    fig.savefig(plot_dir + 'strains.png')\n",
        "\n",
        "    '''\n",
        "    ############################################################################\n",
        "    # Plot optimization/training history\n",
        "    ############################################################################\n",
        "    # Plot residual history\n",
        "    fig, ax = plt.subplots(figsize=(8,5))\n",
        "    ax.loglog(\n",
        "            model.iters, model.res_hist,\n",
        "            color='cornflowerblue',\n",
        "            linewidth=2,\n",
        "        )\n",
        "    ax.set_xlabel(r'iteration', fontsize=16)\n",
        "    ax.set_ylabel(r'residual', fontsize=16)\n",
        "\n",
        "    fig.savefig(plot_dir + 'residual.png')\n",
        "\n",
        "    # Plot boundary condition violation histories\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(18,6))\n",
        "    plt.subplots_adjust(\n",
        "            hspace=0.5,  # height of blank space between subplots?\n",
        "            wspace=0.5   # width of blank space between subplots?\n",
        "        )\n",
        "\n",
        "    for k in range(4):\n",
        "        # Do some binary math to organize plots into a 2 x 2 grid\n",
        "        i = (k & 1) >> 0\n",
        "        j = (k & 2) >> 1\n",
        "\n",
        "        ax[i,j].loglog(\n",
        "                model.iters, model.bc_hist[k],\n",
        "                color='cornflowerblue',\n",
        "                linewidth=2,\n",
        "            )\n",
        "        ax[i,j].set_xlabel(r'iteration', fontsize=16)\n",
        "        ax[i,j].set_ylabel(r'boundary condition {}'.format(k+1), fontsize=16)\n",
        "\n",
        "    fig.savefig(plot_dir + 'bnd_cnds.png')\n",
        "    '''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "batch 36 loss 0.0018187448645569058\n",
            "batch 37 loss 0.001969311785286787\n",
            "batch 38 loss 0.0018984560245701757\n",
            "batch 39 loss 0.0017453037066225323\n",
            "batch 40 loss 0.0018972832156207405\n",
            "batch 41 loss 0.0018225182187581606\n",
            "batch 42 loss 0.0017327130530367027\n",
            "batch 43 loss 0.0016184120802929642\n",
            "batch 44 loss 0.001544193343250093\n",
            "batch 45 loss 0.001998804777625863\n",
            "batch 46 loss 0.0014173311353093214\n",
            "batch 47 loss 0.00179765245474973\n",
            "batch 48 loss 0.0018877456715044303\n",
            "batch 49 loss 0.001965531823475091\n",
            "\n",
            "res: 0.0008844433674141575\n",
            "bc0: 0.004200893642244834\n",
            "bc1: 0.011843426610829622\n",
            "bc2: 0.024536692526638888\n",
            "bc3: 0.011697880466585297\n",
            "\n",
            "\n",
            "Epoch 20\n",
            "batch 0 loss 0.001835164614467266\n",
            "batch 1 loss 0.0020398406486512215\n",
            "batch 2 loss 0.0018480508631938182\n",
            "batch 3 loss 0.0019267502787261332\n",
            "batch 4 loss 0.001876328189873162\n",
            "batch 5 loss 0.0017162408710610292\n",
            "batch 6 loss 0.0018104511821727869\n",
            "batch 7 loss 0.0014583427122156197\n",
            "batch 8 loss 0.0017291652146466702\n",
            "batch 9 loss 0.0016102196019875407\n",
            "batch 10 loss 0.001794380849947771\n",
            "batch 11 loss 0.0013870922183583457\n",
            "batch 12 loss 0.0018287951589935744\n",
            "batch 13 loss 0.0016089359830735253\n",
            "batch 14 loss 0.0018806126740380522\n",
            "batch 15 loss 0.0018816292330840496\n",
            "batch 16 loss 0.0016506240618434206\n",
            "batch 17 loss 0.0017222243999412792\n",
            "batch 18 loss 0.00161038190952513\n",
            "batch 19 loss 0.0015363972796425544\n",
            "batch 20 loss 0.0017319794983806972\n",
            "batch 21 loss 0.0015561906247499012\n",
            "batch 22 loss 0.0016382626898044884\n",
            "batch 23 loss 0.0018960635805825839\n",
            "batch 24 loss 0.00183679370544062\n",
            "batch 25 loss 0.001672868614308943\n",
            "batch 26 loss 0.0015085924738507596\n",
            "batch 27 loss 0.0014650213890911945\n",
            "batch 28 loss 0.0017640486392497873\n",
            "batch 29 loss 0.001869443833515231\n",
            "batch 30 loss 0.001622721453546663\n",
            "batch 31 loss 0.0017433142245202832\n",
            "batch 32 loss 0.0015268739257961112\n",
            "batch 33 loss 0.001583315401022342\n",
            "batch 34 loss 0.0017088334773599418\n",
            "batch 35 loss 0.001549612193616424\n",
            "batch 36 loss 0.0016491065552448978\n",
            "batch 37 loss 0.0014893826044688223\n",
            "batch 38 loss 0.0015459092462468999\n",
            "batch 39 loss 0.0017821837716510253\n",
            "batch 40 loss 0.001737188458890379\n",
            "batch 41 loss 0.0013907231176958645\n",
            "batch 42 loss 0.0016746143668616251\n",
            "batch 43 loss 0.0015760424180165224\n",
            "batch 44 loss 0.0016133296044121526\n",
            "batch 45 loss 0.0017647715847559635\n",
            "batch 46 loss 0.001604891460908898\n",
            "batch 47 loss 0.001522518192987134\n",
            "batch 48 loss 0.0015558627955995982\n",
            "batch 49 loss 0.0016248069378130543\n",
            "\n",
            "res: 0.0007565163243535582\n",
            "bc0: 0.004191912173964474\n",
            "bc1: 0.011622140294843741\n",
            "bc2: 0.02320069014544604\n",
            "bc3: 0.01148322043116133\n",
            "\n",
            "\n",
            "Epoch 21\n",
            "batch 0 loss 0.0017763444752483706\n",
            "batch 1 loss 0.001463167968990601\n",
            "batch 2 loss 0.0017652726867350762\n",
            "batch 3 loss 0.0015075619362756696\n",
            "batch 4 loss 0.00138516439686714\n",
            "batch 5 loss 0.0016608125327556601\n",
            "batch 6 loss 0.001282294845394899\n",
            "batch 7 loss 0.0013254254114710485\n",
            "batch 8 loss 0.0018379423807763512\n",
            "batch 9 loss 0.0015653821950127776\n",
            "batch 10 loss 0.0015771204229923783\n",
            "batch 11 loss 0.0016698862394824371\n",
            "batch 12 loss 0.0015310702346934607\n",
            "batch 13 loss 0.0016251497939537309\n",
            "batch 14 loss 0.0015155197215509335\n",
            "batch 15 loss 0.0016497967859555958\n",
            "batch 16 loss 0.0015874809883383392\n",
            "batch 17 loss 0.0019856582969159865\n",
            "batch 18 loss 0.0016438602441271855\n",
            "batch 19 loss 0.0016796495610531578\n",
            "batch 20 loss 0.0014523883486877419\n",
            "batch 21 loss 0.001421631776452392\n",
            "batch 22 loss 0.0013676122000192826\n",
            "batch 23 loss 0.0015633732385579087\n",
            "batch 24 loss 0.0015532798589489188\n",
            "batch 25 loss 0.0013074274114554126\n",
            "batch 26 loss 0.0014540475904168918\n",
            "batch 27 loss 0.0014719264835751733\n",
            "batch 28 loss 0.0014342534152722787\n",
            "batch 29 loss 0.0017032698034418262\n",
            "batch 30 loss 0.0015945828679533067\n",
            "batch 31 loss 0.0014749550429215125\n",
            "batch 32 loss 0.001593392824889438\n",
            "batch 33 loss 0.0011600781333704051\n",
            "batch 34 loss 0.001458229104564712\n",
            "batch 35 loss 0.001274905370414745\n",
            "batch 36 loss 0.0014815487499923229\n",
            "batch 37 loss 0.0013633946573645306\n",
            "batch 38 loss 0.0012540806690247651\n",
            "batch 39 loss 0.0013846968918949643\n",
            "batch 40 loss 0.0014524229533360894\n",
            "batch 41 loss 0.0015437826213639952\n",
            "batch 42 loss 0.001353166746874065\n",
            "batch 43 loss 0.0015141663218781898\n",
            "batch 44 loss 0.0011885404187523191\n",
            "batch 45 loss 0.0014580229097166898\n",
            "batch 46 loss 0.0012546014480362756\n",
            "batch 47 loss 0.0014342579621948906\n",
            "batch 48 loss 0.0015817385216450961\n",
            "batch 49 loss 0.0012960597017101913\n",
            "\n",
            "res: 0.000659028919338153\n",
            "bc0: 0.004189796767706128\n",
            "bc1: 0.011450319666153846\n",
            "bc2: 0.02190298051269803\n",
            "bc3: 0.01134544976371753\n",
            "\n",
            "\n",
            "Epoch 22\n",
            "batch 0 loss 0.001309357081950565\n",
            "batch 1 loss 0.0014122535546800524\n",
            "batch 2 loss 0.0012001934733700786\n",
            "batch 3 loss 0.0013424576272233265\n",
            "batch 4 loss 0.0015186362328572986\n",
            "batch 5 loss 0.0014762083294613268\n",
            "batch 6 loss 0.0015146963257592796\n",
            "batch 7 loss 0.0012965868661274252\n",
            "batch 8 loss 0.0013068604726580143\n",
            "batch 9 loss 0.0011778999607505647\n",
            "batch 10 loss 0.0016375973506903027\n",
            "batch 11 loss 0.0012586978768165682\n",
            "batch 12 loss 0.0014876655100035281\n",
            "batch 13 loss 0.001313244282901082\n",
            "batch 14 loss 0.0014118964753097535\n",
            "batch 15 loss 0.001403007480413211\n",
            "batch 16 loss 0.0013176719947433795\n",
            "batch 17 loss 0.001580422305329834\n",
            "batch 18 loss 0.0013185686524148734\n",
            "batch 19 loss 0.001362632231184285\n",
            "batch 20 loss 0.0012005868865130342\n",
            "batch 21 loss 0.0014438594705126256\n",
            "batch 22 loss 0.001391639766304379\n",
            "batch 23 loss 0.0014753439776873402\n",
            "batch 24 loss 0.0011767409572510633\n",
            "batch 25 loss 0.0011536301142241977\n",
            "batch 26 loss 0.001596407363874082\n",
            "batch 27 loss 0.0013515529655198219\n",
            "batch 28 loss 0.0013405783121414985\n",
            "batch 29 loss 0.0014935053398686033\n",
            "batch 30 loss 0.0013297345284065995\n",
            "batch 31 loss 0.0015005630093369656\n",
            "batch 32 loss 0.0013682232740998888\n",
            "batch 33 loss 0.0010962082412469434\n",
            "batch 34 loss 0.0014698967312869139\n",
            "batch 35 loss 0.0011919607346972923\n",
            "batch 36 loss 0.0014612981736276624\n",
            "batch 37 loss 0.0010818740821076085\n",
            "batch 38 loss 0.0015086279256036623\n",
            "batch 39 loss 0.0011728820859016051\n",
            "batch 40 loss 0.0013407363591232025\n",
            "batch 41 loss 0.0012364099214143961\n",
            "batch 42 loss 0.0012301714354065072\n",
            "batch 43 loss 0.0013719695131480896\n",
            "batch 44 loss 0.0013010983417697226\n",
            "batch 45 loss 0.001251517909408481\n",
            "batch 46 loss 0.0011774534255545063\n",
            "batch 47 loss 0.0013957272226072092\n",
            "batch 48 loss 0.0012013011522861725\n",
            "batch 49 loss 0.0014650243938351983\n",
            "\n",
            "res: 0.0005823122932654054\n",
            "bc0: 0.004197654223441134\n",
            "bc1: 0.011249325796731506\n",
            "bc2: 0.020763837106455522\n",
            "bc3: 0.011151637441018077\n",
            "\n",
            "\n",
            "Epoch 23\n",
            "batch 0 loss 0.0013647176721591821\n",
            "batch 1 loss 0.001423146729653012\n",
            "batch 2 loss 0.0012831911073912094\n",
            "batch 3 loss 0.00121981205647668\n",
            "batch 4 loss 0.001131049939782401\n",
            "batch 5 loss 0.0014628483817130244\n",
            "batch 6 loss 0.0014275055438438115\n",
            "batch 7 loss 0.0010701251796085397\n",
            "batch 8 loss 0.001257136722395968\n",
            "batch 9 loss 0.0012950201444418767\n",
            "batch 10 loss 0.001280251103262945\n",
            "batch 11 loss 0.001294512098034097\n",
            "batch 12 loss 0.0012481476172551864\n",
            "batch 13 loss 0.0013467937726606533\n",
            "batch 14 loss 0.0010773770624800062\n",
            "batch 15 loss 0.001176803778661352\n",
            "batch 16 loss 0.00112799030960806\n",
            "batch 17 loss 0.0009836171894577375\n",
            "batch 18 loss 0.001093536389455135\n",
            "batch 19 loss 0.0012445458800997124\n",
            "batch 20 loss 0.0011352247825574299\n",
            "batch 21 loss 0.0013948774104552619\n",
            "batch 22 loss 0.0011582365752091186\n",
            "batch 23 loss 0.0011121462470071405\n",
            "batch 24 loss 0.001162079828830387\n",
            "batch 25 loss 0.0011676446233116707\n",
            "batch 26 loss 0.0013791946667947221\n",
            "batch 27 loss 0.0016246511689191137\n",
            "batch 28 loss 0.0012418683800589586\n",
            "batch 29 loss 0.001199890013794402\n",
            "batch 30 loss 0.0012048843428693673\n",
            "batch 31 loss 0.0012692111330825893\n",
            "batch 32 loss 0.0011808026752691823\n",
            "batch 33 loss 0.00121101653327669\n",
            "batch 34 loss 0.0011373701365620245\n",
            "batch 35 loss 0.0010158738208182047\n",
            "batch 36 loss 0.0013253252881668298\n",
            "batch 37 loss 0.0011571551021556372\n",
            "batch 38 loss 0.0010510489009590122\n",
            "batch 39 loss 0.0013745993654701457\n",
            "batch 40 loss 0.0012593580137135803\n",
            "batch 41 loss 0.0013830579331105435\n",
            "batch 42 loss 0.001186753302287311\n",
            "batch 43 loss 0.0011354036702867533\n",
            "batch 44 loss 0.0011128116139832646\n",
            "batch 45 loss 0.001137777475658682\n",
            "batch 46 loss 0.0012000629069947775\n",
            "batch 47 loss 0.0013667070463968863\n",
            "batch 48 loss 0.0010460276413063636\n",
            "batch 49 loss 0.001160742505304765\n",
            "\n",
            "res: 0.0005164363027739618\n",
            "bc0: 0.004200713075639185\n",
            "bc1: 0.011044318052541835\n",
            "bc2: 0.01988048899594624\n",
            "bc3: 0.010955524838124257\n",
            "\n",
            "\n",
            "Epoch 24\n",
            "batch 0 loss 0.0011752810437659414\n",
            "batch 1 loss 0.0011968139533837373\n",
            "batch 2 loss 0.0014459822094812804\n",
            "batch 3 loss 0.001218230343727154\n",
            "batch 4 loss 0.0009591860020606528\n",
            "batch 5 loss 0.0010839532796250403\n",
            "batch 6 loss 0.0010213372792196587\n",
            "batch 7 loss 0.0010417480641535144\n",
            "batch 8 loss 0.0011209534552613918\n",
            "batch 9 loss 0.0011269217817234446\n",
            "batch 10 loss 0.0013671740925980083\n",
            "batch 11 loss 0.001063962695303393\n",
            "batch 12 loss 0.000988172263838203\n",
            "batch 13 loss 0.0011732094624556392\n",
            "batch 14 loss 0.00126822244359931\n",
            "batch 15 loss 0.0013408746767117107\n",
            "batch 16 loss 0.0011074922616044352\n",
            "batch 17 loss 0.001339251392896235\n",
            "batch 18 loss 0.0010792305958953533\n",
            "batch 19 loss 0.001133512081791171\n",
            "batch 20 loss 0.0010465845200494897\n",
            "batch 21 loss 0.0009922569196225107\n",
            "batch 22 loss 0.0008724493382104507\n",
            "batch 23 loss 0.0011347998248038512\n",
            "batch 24 loss 0.001288018740229326\n",
            "batch 25 loss 0.0010045439726874175\n",
            "batch 26 loss 0.0012563583758747623\n",
            "batch 27 loss 0.0012493343492835996\n",
            "batch 28 loss 0.0012399322589285203\n",
            "batch 29 loss 0.0011607448430417926\n",
            "batch 30 loss 0.0012354745895106157\n",
            "batch 31 loss 0.001060352637120267\n",
            "batch 32 loss 0.001021950983483259\n",
            "batch 33 loss 0.0012784713533894115\n",
            "batch 34 loss 0.0009785467332401512\n",
            "batch 35 loss 0.0013629078160505683\n",
            "batch 36 loss 0.0010042853859419186\n",
            "batch 37 loss 0.0009465081023629579\n",
            "batch 38 loss 0.0011520535853114613\n",
            "batch 39 loss 0.0012162705418090173\n",
            "batch 40 loss 0.0010158654438470272\n",
            "batch 41 loss 0.000998399590742703\n",
            "batch 42 loss 0.001000971615815027\n",
            "batch 43 loss 0.0009588159465683205\n",
            "batch 44 loss 0.0009555929978112974\n",
            "batch 45 loss 0.0010396454316883405\n",
            "batch 46 loss 0.0010970477707522768\n",
            "batch 47 loss 0.0009773663700369808\n",
            "batch 48 loss 0.001402755357610361\n",
            "batch 49 loss 0.00100021145317215\n",
            "\n",
            "res: 0.00046546876323615106\n",
            "bc0: 0.004215941054602863\n",
            "bc1: 0.010931222461951488\n",
            "bc2: 0.01888063578982883\n",
            "bc3: 0.01085065873954639\n",
            "\n",
            "\n",
            "Epoch 25\n",
            "batch 0 loss 0.0009932310253677344\n",
            "batch 1 loss 0.0011984717396683955\n",
            "batch 2 loss 0.0012259452396391629\n",
            "batch 3 loss 0.0009945422780320953\n",
            "batch 4 loss 0.0012154819441980477\n",
            "batch 5 loss 0.0008636148535709476\n",
            "batch 6 loss 0.000965592769055184\n",
            "batch 7 loss 0.0009843719005392423\n",
            "batch 8 loss 0.0009417275894714919\n",
            "batch 9 loss 0.000993637713127021\n",
            "batch 10 loss 0.0010319011256763597\n",
            "batch 11 loss 0.0009683013993690256\n",
            "batch 12 loss 0.001201519143681379\n",
            "batch 13 loss 0.0010429626417544585\n",
            "batch 14 loss 0.0010288047561977155\n",
            "batch 15 loss 0.0011781256448576557\n",
            "batch 16 loss 0.0012356967442191022\n",
            "batch 17 loss 0.0011186879323839213\n",
            "batch 18 loss 0.0009507621321562573\n",
            "batch 19 loss 0.0011809143235657813\n",
            "batch 20 loss 0.0010971727558806053\n",
            "batch 21 loss 0.0009456743676388628\n",
            "batch 22 loss 0.0009064216135481488\n",
            "batch 23 loss 0.0009444081041617916\n",
            "batch 24 loss 0.001087958586980235\n",
            "batch 25 loss 0.0009661140518497138\n",
            "batch 26 loss 0.0009623459828183625\n",
            "batch 27 loss 0.0009782864087165516\n",
            "batch 28 loss 0.001304822739222416\n",
            "batch 29 loss 0.0010849896065634464\n",
            "batch 30 loss 0.0010497318354215452\n",
            "batch 31 loss 0.0008746623274174013\n",
            "batch 32 loss 0.00104711071137023\n",
            "batch 33 loss 0.0009662993740725453\n",
            "batch 34 loss 0.0012083007556015302\n",
            "batch 35 loss 0.0010329997327309587\n",
            "batch 36 loss 0.0009475243410477572\n",
            "batch 37 loss 0.0010136425937369226\n",
            "batch 38 loss 0.0008838735442942148\n",
            "batch 39 loss 0.0010665712459110705\n",
            "batch 40 loss 0.0010265495963817015\n",
            "batch 41 loss 0.0010339413657872673\n",
            "batch 42 loss 0.0010024002990482463\n",
            "batch 43 loss 0.0010317516727301511\n",
            "batch 44 loss 0.0009191157031536382\n",
            "batch 45 loss 0.0009896775915543775\n",
            "batch 46 loss 0.0009901975606883425\n",
            "batch 47 loss 0.0010271848733639853\n",
            "batch 48 loss 0.0011707980979945758\n",
            "batch 49 loss 0.0009994174906019905\n",
            "\n",
            "res: 0.000424358622573168\n",
            "bc0: 0.004232488056071851\n",
            "bc1: 0.010821917755312582\n",
            "bc2: 0.017966685821737417\n",
            "bc3: 0.010748756449906152\n",
            "\n",
            "\n",
            "Epoch 26\n",
            "batch 0 loss 0.0010309307564968086\n",
            "batch 1 loss 0.0009215591477814529\n",
            "batch 2 loss 0.0009407680838795107\n",
            "batch 3 loss 0.001078092514977967\n",
            "batch 4 loss 0.0009232041466784442\n",
            "batch 5 loss 0.0008194234911152942\n",
            "batch 6 loss 0.0011754220819438852\n",
            "batch 7 loss 0.0011155472108512665\n",
            "batch 8 loss 0.0009144836750563524\n",
            "batch 9 loss 0.0010281585668002022\n",
            "batch 10 loss 0.0009585110799733702\n",
            "batch 11 loss 0.0009166707210652352\n",
            "batch 12 loss 0.0010234939104425732\n",
            "batch 13 loss 0.0009437641271626047\n",
            "batch 14 loss 0.0011096220413273747\n",
            "batch 15 loss 0.0009611354914914557\n",
            "batch 16 loss 0.0010281989336136037\n",
            "batch 17 loss 0.001012320452762377\n",
            "batch 18 loss 0.0010966428592325862\n",
            "batch 19 loss 0.0009652900642566059\n",
            "batch 20 loss 0.0010803741568818671\n",
            "batch 21 loss 0.0009486819240579272\n",
            "batch 22 loss 0.0010185012987044147\n",
            "batch 23 loss 0.0010614490201922243\n",
            "batch 24 loss 0.001115979554970115\n",
            "batch 25 loss 0.0009976837694037188\n",
            "batch 26 loss 0.0009864029938406037\n",
            "batch 27 loss 0.0008863907466402504\n",
            "batch 28 loss 0.0008617406699185759\n",
            "batch 29 loss 0.0009421803768378829\n",
            "batch 30 loss 0.0009465410097202924\n",
            "batch 31 loss 0.0009227261966192032\n",
            "batch 32 loss 0.000867952479983305\n",
            "batch 33 loss 0.0009543317823000821\n",
            "batch 34 loss 0.0008642236701646899\n",
            "batch 35 loss 0.0010280954245173181\n",
            "batch 36 loss 0.0009500100468604218\n",
            "batch 37 loss 0.0011561255447904892\n",
            "batch 38 loss 0.0008760704932955694\n",
            "batch 39 loss 0.0007779864792917689\n",
            "batch 40 loss 0.0008726103392872922\n",
            "batch 41 loss 0.000801866838866974\n",
            "batch 42 loss 0.0009554697844230859\n",
            "batch 43 loss 0.0009621300759096627\n",
            "batch 44 loss 0.0007913015510508474\n",
            "batch 45 loss 0.0010049330840879896\n",
            "batch 46 loss 0.0009348585272767132\n",
            "batch 47 loss 0.0008252183801450234\n",
            "batch 48 loss 0.0008994132367692138\n",
            "batch 49 loss 0.0009413600614007445\n",
            "\n",
            "res: 0.0003896753825883274\n",
            "bc0: 0.004253009520572283\n",
            "bc1: 0.010772241449356887\n",
            "bc2: 0.017066004367381274\n",
            "bc3: 0.010709765806418598\n",
            "\n",
            "\n",
            "Epoch 27\n",
            "batch 0 loss 0.0008916016573140211\n",
            "batch 1 loss 0.0010242354876096292\n",
            "batch 2 loss 0.0008837517818915658\n",
            "batch 3 loss 0.0009239012811388264\n",
            "batch 4 loss 0.0009198805005998574\n",
            "batch 5 loss 0.0008764092949484456\n",
            "batch 6 loss 0.0008491706574366747\n",
            "batch 7 loss 0.0007853779376447114\n",
            "batch 8 loss 0.0008953049985532762\n",
            "batch 9 loss 0.0007800775524644223\n",
            "batch 10 loss 0.0009975397881324883\n",
            "batch 11 loss 0.0010675742810863998\n",
            "batch 12 loss 0.0008626491633141888\n",
            "batch 13 loss 0.0011184048127242275\n",
            "batch 14 loss 0.0008334859468182958\n",
            "batch 15 loss 0.0009950413504599166\n",
            "batch 16 loss 0.000896055897206148\n",
            "batch 17 loss 0.0008909826296894972\n",
            "batch 18 loss 0.0009320520401306218\n",
            "batch 19 loss 0.000864181287804215\n",
            "batch 20 loss 0.000987067179760235\n",
            "batch 21 loss 0.0009888634531412647\n",
            "batch 22 loss 0.00084170533499805\n",
            "batch 23 loss 0.000899588878566727\n",
            "batch 24 loss 0.0009091555952753578\n",
            "batch 25 loss 0.0009132118599955224\n",
            "batch 26 loss 0.0008491686611090103\n",
            "batch 27 loss 0.000971892148202294\n",
            "batch 28 loss 0.0009354227531759458\n",
            "batch 29 loss 0.0007861167341181876\n",
            "batch 30 loss 0.0010170355706378468\n",
            "batch 31 loss 0.0009109902913549346\n",
            "batch 32 loss 0.0008498284012441779\n",
            "batch 33 loss 0.0008109552361596806\n",
            "batch 34 loss 0.0008590100108085625\n",
            "batch 35 loss 0.000984282782777763\n",
            "batch 36 loss 0.0008743329058559641\n",
            "batch 37 loss 0.0008388569943374705\n",
            "batch 38 loss 0.0008812477619046318\n",
            "batch 39 loss 0.0009092294499929245\n",
            "batch 40 loss 0.0008653035554412116\n",
            "batch 41 loss 0.0008382970370319402\n",
            "batch 42 loss 0.0009633169621449442\n",
            "batch 43 loss 0.0008308581573319103\n",
            "batch 44 loss 0.0008417240704334757\n",
            "batch 45 loss 0.0008434886192121096\n",
            "batch 46 loss 0.0007561780253591975\n",
            "batch 47 loss 0.0008025207041439776\n",
            "batch 48 loss 0.001022274248610567\n",
            "batch 49 loss 0.0009525800860810751\n",
            "\n",
            "res: 0.0003588629834126392\n",
            "bc0: 0.0042691031316303574\n",
            "bc1: 0.010649131222430552\n",
            "bc2: 0.0163763278867564\n",
            "bc3: 0.010594240921888463\n",
            "\n",
            "\n",
            "Epoch 28\n",
            "batch 0 loss 0.0010263197947642166\n",
            "batch 1 loss 0.0007400915579817895\n",
            "batch 2 loss 0.0008927053175824273\n",
            "batch 3 loss 0.0008844148689746289\n",
            "batch 4 loss 0.0007901351208799742\n",
            "batch 5 loss 0.0010035996576195768\n",
            "batch 6 loss 0.0008568846036920532\n",
            "batch 7 loss 0.0009302589954814966\n",
            "batch 8 loss 0.0009340042658138326\n",
            "batch 9 loss 0.0007290249199546904\n",
            "batch 10 loss 0.0008055476061771739\n",
            "batch 11 loss 0.0008594232816754573\n",
            "batch 12 loss 0.0008866892432409949\n",
            "batch 13 loss 0.0009678317466671799\n",
            "batch 14 loss 0.0008818771528354256\n",
            "batch 15 loss 0.0009191015597263035\n",
            "batch 16 loss 0.001137003162974407\n",
            "batch 17 loss 0.0008081318170544223\n",
            "batch 18 loss 0.0009580338949629723\n",
            "batch 19 loss 0.0008710830973575114\n",
            "batch 20 loss 0.0009263207596287013\n",
            "batch 21 loss 0.0007430071388080856\n",
            "batch 22 loss 0.0008301462179198488\n",
            "batch 23 loss 0.0009255228201000671\n",
            "batch 24 loss 0.0009689343222818413\n",
            "batch 25 loss 0.0007447432925791371\n",
            "batch 26 loss 0.0008073975894691613\n",
            "batch 27 loss 0.0007691383465438575\n",
            "batch 28 loss 0.000757385937506831\n",
            "batch 29 loss 0.0008382181095896101\n",
            "batch 30 loss 0.0007820727747624067\n",
            "batch 31 loss 0.0008256510459191413\n",
            "batch 32 loss 0.0008441164589722389\n",
            "batch 33 loss 0.0008033867283777585\n",
            "batch 34 loss 0.00102666309905361\n",
            "batch 35 loss 0.0009090385471563828\n",
            "batch 36 loss 0.0007354499870532528\n",
            "batch 37 loss 0.0007748360029312154\n",
            "batch 38 loss 0.0008139378953764141\n",
            "batch 39 loss 0.000738434604254215\n",
            "batch 40 loss 0.000848942533173552\n",
            "batch 41 loss 0.0008716262865233356\n",
            "batch 42 loss 0.0006874369462530358\n",
            "batch 43 loss 0.0008283773437057775\n",
            "batch 44 loss 0.000670128079619075\n",
            "batch 45 loss 0.0006987328347357392\n",
            "batch 46 loss 0.0009382928975087817\n",
            "batch 47 loss 0.0008317795137579956\n",
            "batch 48 loss 0.0008373000018569846\n",
            "batch 49 loss 0.0006457998025118476\n",
            "\n",
            "res: 0.00033452377268725357\n",
            "bc0: 0.004288706109135139\n",
            "bc1: 0.010604738020198201\n",
            "bc2: 0.015600049563216842\n",
            "bc3: 0.010555468285493688\n",
            "\n",
            "\n",
            "Epoch 29\n",
            "batch 0 loss 0.0008230806604258567\n",
            "batch 1 loss 0.0009203258297715219\n",
            "batch 2 loss 0.0009806139342730812\n",
            "batch 3 loss 0.0007833876619717787\n",
            "batch 4 loss 0.0007034625834637349\n",
            "batch 5 loss 0.0008033537217521801\n",
            "batch 6 loss 0.0008549835509642515\n",
            "batch 7 loss 0.0008578595016005597\n",
            "batch 8 loss 0.0007535799784204761\n",
            "batch 9 loss 0.0009858401659314827\n",
            "batch 10 loss 0.0008321294149324654\n",
            "batch 11 loss 0.0006778560500060784\n",
            "batch 12 loss 0.0007167817861753541\n",
            "batch 13 loss 0.0008278241356576335\n",
            "batch 14 loss 0.0006740185556163908\n",
            "batch 15 loss 0.0009744998107319582\n",
            "batch 16 loss 0.0009026724657171723\n",
            "batch 17 loss 0.000866856346684478\n",
            "batch 18 loss 0.0008004090858470408\n",
            "batch 19 loss 0.0007891945638731569\n",
            "batch 20 loss 0.0008448527463481597\n",
            "batch 21 loss 0.0008756119392029372\n",
            "batch 22 loss 0.0010185288323285393\n",
            "batch 23 loss 0.0006578958645038841\n",
            "batch 24 loss 0.0008141043130682037\n",
            "batch 25 loss 0.0007870880654013107\n",
            "batch 26 loss 0.0007206499262727819\n",
            "batch 27 loss 0.000704698786703219\n",
            "batch 28 loss 0.000762618142073138\n",
            "batch 29 loss 0.0007165071809920782\n",
            "batch 30 loss 0.0006472174272044675\n",
            "batch 31 loss 0.0008189805446839872\n",
            "batch 32 loss 0.0008315493066459939\n",
            "batch 33 loss 0.0008070174687636545\n",
            "batch 34 loss 0.000768032299349913\n",
            "batch 35 loss 0.0008114348303667961\n",
            "batch 36 loss 0.0006999760825167349\n",
            "batch 37 loss 0.0007479461246815461\n",
            "batch 38 loss 0.000686113122689533\n",
            "batch 39 loss 0.0007174915836349757\n",
            "batch 40 loss 0.0008027139495270707\n",
            "batch 41 loss 0.0008559526375018705\n",
            "batch 42 loss 0.0007896094754690314\n",
            "batch 43 loss 0.0007963412389668812\n",
            "batch 44 loss 0.0007468548322729865\n",
            "batch 45 loss 0.0007305686945247515\n",
            "batch 46 loss 0.0009299009103960174\n",
            "batch 47 loss 0.0007382719762347265\n",
            "batch 48 loss 0.0006923048647547789\n",
            "batch 49 loss 0.0008716598826627618\n",
            "\n",
            "res: 0.00031369931638609405\n",
            "bc0: 0.004306420651039064\n",
            "bc1: 0.010522082740935369\n",
            "bc2: 0.014940033437374708\n",
            "bc3: 0.010479315842688375\n",
            "\n",
            "\n",
            "Epoch 30\n",
            "batch 0 loss 0.000835390159340163\n",
            "batch 1 loss 0.0007767041433310347\n",
            "batch 2 loss 0.0006829788557472366\n",
            "batch 3 loss 0.0007358454513004778\n",
            "batch 4 loss 0.0007155417597603796\n",
            "batch 5 loss 0.0008619967834312796\n",
            "batch 6 loss 0.0007053662958444983\n",
            "batch 7 loss 0.0007109422871223426\n",
            "batch 8 loss 0.0007285330219322999\n",
            "batch 9 loss 0.0007153928066118192\n",
            "batch 10 loss 0.000682315815367329\n",
            "batch 11 loss 0.0008000785959155149\n",
            "batch 12 loss 0.0006907064807821482\n",
            "batch 13 loss 0.0007146553605656728\n",
            "batch 14 loss 0.0010715936797162243\n",
            "batch 15 loss 0.0008216437769237251\n",
            "batch 16 loss 0.0006505074024053247\n",
            "batch 17 loss 0.000838852556068591\n",
            "batch 18 loss 0.000672764302474896\n",
            "batch 19 loss 0.0007788288139999633\n",
            "batch 20 loss 0.0008569905070720153\n",
            "batch 21 loss 0.0007597932066449184\n",
            "batch 22 loss 0.000725038239912883\n",
            "batch 23 loss 0.0007406039878557694\n",
            "batch 24 loss 0.0006982374843334237\n",
            "batch 25 loss 0.000888826757753106\n",
            "batch 26 loss 0.0007316245394554061\n",
            "batch 27 loss 0.0008428912427036462\n",
            "batch 28 loss 0.0006470391741126027\n",
            "batch 29 loss 0.0007979233988825992\n",
            "batch 30 loss 0.000741363247735496\n",
            "batch 31 loss 0.0006477996512061743\n",
            "batch 32 loss 0.0007450026126798092\n",
            "batch 33 loss 0.0007053776897874422\n",
            "batch 34 loss 0.0007375484771752196\n",
            "batch 35 loss 0.0007529965368270288\n",
            "batch 36 loss 0.0008370476347078444\n",
            "batch 37 loss 0.0008853218908773633\n",
            "batch 38 loss 0.0007860435559848078\n",
            "batch 39 loss 0.0007943740265449487\n",
            "batch 40 loss 0.000973836978317599\n",
            "batch 41 loss 0.0006938185191358672\n",
            "batch 42 loss 0.0008634744865703424\n",
            "batch 43 loss 0.0005611207697309502\n",
            "batch 44 loss 0.0006383212363612521\n",
            "batch 45 loss 0.0007735530170746779\n",
            "batch 46 loss 0.0006472317223278206\n",
            "batch 47 loss 0.0007562396278648848\n",
            "batch 48 loss 0.0006856402728638495\n",
            "batch 49 loss 0.0007340805765954339\n",
            "\n",
            "res: 0.00029337180728986897\n",
            "bc0: 0.004316066655924034\n",
            "bc1: 0.010425020673499317\n",
            "bc2: 0.014436291593716977\n",
            "bc3: 0.010382817493434083\n",
            "\n",
            "\n",
            "finished unconstrained optimization\n",
            "res: 0.00029337180728986897\n",
            "bc0: 0.004316066655924034\n",
            "bc1: 0.010425020673499317\n",
            "bc2: 0.014436291593716977\n",
            "bc3: 0.010382817493434083\n",
            "\n",
            "\n",
            "[0.00863213 0.02085004 0.02887258 0.02076563]\n",
            "2.0\n",
            "Epoch 1\n",
            "batch 0 loss 0.002275372694250595\n",
            "batch 1 loss 0.002054962556688085\n",
            "batch 2 loss 0.0020142239700824092\n",
            "batch 3 loss 0.0020891150291945395\n",
            "batch 4 loss 0.0020784312333370645\n",
            "batch 5 loss 0.0021079263706998783\n",
            "batch 6 loss 0.0020457696991282273\n",
            "batch 7 loss 0.0020130700762739755\n",
            "batch 8 loss 0.0021726812841728702\n",
            "batch 9 loss 0.001964978375297939\n",
            "batch 10 loss 0.002042204930017932\n",
            "batch 11 loss 0.0019984394252308127\n",
            "batch 12 loss 0.001925974158252191\n",
            "batch 13 loss 0.0019014643592289635\n",
            "batch 14 loss 0.0019636885203603853\n",
            "batch 15 loss 0.0020008509441600455\n",
            "batch 16 loss 0.002141716005907116\n",
            "batch 17 loss 0.0019581904982850246\n",
            "batch 18 loss 0.0019158331051789693\n",
            "batch 19 loss 0.0019343925775861014\n",
            "batch 20 loss 0.0019909433828104017\n",
            "batch 21 loss 0.0019907566201848637\n",
            "batch 22 loss 0.0017820131999097164\n",
            "batch 23 loss 0.0019905443795214923\n",
            "batch 24 loss 0.0020185504017152254\n",
            "batch 25 loss 0.0017662281136208197\n",
            "batch 26 loss 0.002145007975486794\n",
            "batch 27 loss 0.002075739686900104\n",
            "batch 28 loss 0.0017384405916904195\n",
            "batch 29 loss 0.0019392738130914087\n",
            "batch 30 loss 0.001829490006786208\n",
            "batch 31 loss 0.0017587290938945225\n",
            "batch 32 loss 0.0018355632464815846\n",
            "batch 33 loss 0.0019459236494030472\n",
            "batch 34 loss 0.0020293036128220514\n",
            "batch 35 loss 0.0017552278529591198\n",
            "batch 36 loss 0.0018633062139936612\n",
            "batch 37 loss 0.0019130322782834716\n",
            "batch 38 loss 0.0018008822133173898\n",
            "batch 39 loss 0.001949465085083372\n",
            "batch 40 loss 0.001981517173979292\n",
            "batch 41 loss 0.00180477708470763\n",
            "batch 42 loss 0.0017556981853044838\n",
            "batch 43 loss 0.0017355278271922786\n",
            "batch 44 loss 0.0017961578732789358\n",
            "batch 45 loss 0.0017795855154265165\n",
            "batch 46 loss 0.002040529610594545\n",
            "batch 47 loss 0.001857040711296177\n",
            "batch 48 loss 0.0019006794570355342\n",
            "batch 49 loss 0.0019474303043512757\n",
            "\n",
            "res: 0.0003704579153907859\n",
            "bc0: 0.004512182654258015\n",
            "bc1: 0.010680731441262763\n",
            "bc2: 0.010285882829453091\n",
            "bc3: 0.010605796553395906\n",
            "\n",
            "\n",
            "Epoch 2\n",
            "batch 0 loss 0.0018698221915399178\n",
            "batch 1 loss 0.0019309224163924917\n",
            "batch 2 loss 0.0018314205437009114\n",
            "batch 3 loss 0.0017347587109175716\n",
            "batch 4 loss 0.0019352729049094816\n",
            "batch 5 loss 0.0019181074400756056\n",
            "batch 6 loss 0.001840550908460728\n",
            "batch 7 loss 0.001743996653375576\n",
            "batch 8 loss 0.0019393037392534049\n",
            "batch 9 loss 0.001907237579928712\n",
            "batch 10 loss 0.0018804497779453659\n",
            "batch 11 loss 0.0017667419805528905\n",
            "batch 12 loss 0.0017090410769481116\n",
            "batch 13 loss 0.001841664975586875\n",
            "batch 14 loss 0.0018513239583994648\n",
            "batch 15 loss 0.0018921902750079346\n",
            "batch 16 loss 0.0017814244022157954\n",
            "batch 17 loss 0.0018471272556982838\n",
            "batch 18 loss 0.0019275725356090352\n",
            "batch 19 loss 0.0018458757044791873\n",
            "batch 20 loss 0.001846738872488599\n",
            "batch 21 loss 0.0017782247682893786\n",
            "batch 22 loss 0.0018794443935267876\n",
            "batch 23 loss 0.0017931969859570358\n",
            "batch 24 loss 0.001855595375481848\n",
            "batch 25 loss 0.0017850984656140076\n",
            "batch 26 loss 0.0017231175639237269\n",
            "batch 27 loss 0.0017804732077610237\n",
            "batch 28 loss 0.0019587380748288573\n",
            "batch 29 loss 0.0017409549826165255\n",
            "batch 30 loss 0.0018235262381731954\n",
            "batch 31 loss 0.0018011292723111813\n",
            "batch 32 loss 0.0018319707316127927\n",
            "batch 33 loss 0.0018062975436649639\n",
            "batch 34 loss 0.001721223677607625\n",
            "batch 35 loss 0.0019103925377116412\n",
            "batch 36 loss 0.0017294257887757573\n",
            "batch 37 loss 0.0017401087788654153\n",
            "batch 38 loss 0.0017744455147223033\n",
            "batch 39 loss 0.0017900059851716256\n",
            "batch 40 loss 0.0016981010196016155\n",
            "batch 41 loss 0.0017499477533288392\n",
            "batch 42 loss 0.0017430127645066212\n",
            "batch 43 loss 0.0018311240652733069\n",
            "batch 44 loss 0.0016564567391395284\n",
            "batch 45 loss 0.0017901007596435663\n",
            "batch 46 loss 0.0015709016335945373\n",
            "batch 47 loss 0.0016232397204328794\n",
            "batch 48 loss 0.0016316756443962466\n",
            "batch 49 loss 0.0017335535288764549\n",
            "\n",
            "res: 0.00034581585235946374\n",
            "bc0: 0.004509000981832177\n",
            "bc1: 0.01053405006489113\n",
            "bc2: 0.009361231961835885\n",
            "bc3: 0.010468206815308453\n",
            "\n",
            "\n",
            "Epoch 3\n",
            "batch 0 loss 0.0017238504050465767\n",
            "batch 1 loss 0.001855270832927798\n",
            "batch 2 loss 0.001882375712581511\n",
            "batch 3 loss 0.0017509355064916229\n",
            "batch 4 loss 0.0018160176172721209\n",
            "batch 5 loss 0.0016397414516335519\n",
            "batch 6 loss 0.0017994608644437801\n",
            "batch 7 loss 0.0017032208247385794\n",
            "batch 8 loss 0.0019029403698761548\n",
            "batch 9 loss 0.0016308488538683027\n",
            "batch 10 loss 0.0016488442657659568\n",
            "batch 11 loss 0.0015839134797539364\n",
            "batch 12 loss 0.001878046633158283\n",
            "batch 13 loss 0.0016051103571396775\n",
            "batch 14 loss 0.0015427851004457187\n",
            "batch 15 loss 0.00181178071257652\n",
            "batch 16 loss 0.0017344327176622527\n",
            "batch 17 loss 0.0016516840293253423\n",
            "batch 18 loss 0.0016051805986560075\n",
            "batch 19 loss 0.0018601171026282669\n",
            "batch 20 loss 0.0017488869469595967\n",
            "batch 21 loss 0.0016050142943717536\n",
            "batch 22 loss 0.0019422520029649239\n",
            "batch 23 loss 0.001594714992316208\n",
            "batch 24 loss 0.0018171517224177037\n",
            "batch 25 loss 0.0015323356263092992\n",
            "batch 26 loss 0.0016721914685900706\n",
            "batch 27 loss 0.001657487799392013\n",
            "batch 28 loss 0.0017432430478845902\n",
            "batch 29 loss 0.0015846089603738837\n",
            "batch 30 loss 0.001554932058262083\n",
            "batch 31 loss 0.0017903553838798203\n",
            "batch 32 loss 0.0018027842713669764\n",
            "batch 33 loss 0.001679055593091436\n",
            "batch 34 loss 0.0016159291863879213\n",
            "batch 35 loss 0.0017121056699524087\n",
            "batch 36 loss 0.0016393846914701542\n",
            "batch 37 loss 0.0015952983657787692\n",
            "batch 38 loss 0.0018319375144984786\n",
            "batch 39 loss 0.0016492675988975017\n",
            "batch 40 loss 0.0017663575207187818\n",
            "batch 41 loss 0.0016123098712892067\n",
            "batch 42 loss 0.0016035844944358765\n",
            "batch 43 loss 0.001659080714103252\n",
            "batch 44 loss 0.0017006625723721973\n",
            "batch 45 loss 0.0016279393968566956\n",
            "batch 46 loss 0.0017309245458663886\n",
            "batch 47 loss 0.0016484027159161477\n",
            "batch 48 loss 0.0017365529045654392\n",
            "batch 49 loss 0.0017588643951477179\n",
            "\n",
            "res: 0.0003168988185145746\n",
            "bc0: 0.004507322356702041\n",
            "bc1: 0.010320465174584138\n",
            "bc2: 0.008839343844467521\n",
            "bc3: 0.010261513073637358\n",
            "\n",
            "\n",
            "Epoch 4\n",
            "batch 0 loss 0.0014968528669248903\n",
            "batch 1 loss 0.0017227784304872263\n",
            "batch 2 loss 0.0016786354134158221\n",
            "batch 3 loss 0.0016272876884070489\n",
            "batch 4 loss 0.0017787765783871252\n",
            "batch 5 loss 0.0014843461181523996\n",
            "batch 6 loss 0.0017254938955219472\n",
            "batch 7 loss 0.0016828009726461118\n",
            "batch 8 loss 0.001640099832008776\n",
            "batch 9 loss 0.001728782508875901\n",
            "batch 10 loss 0.001760262435581811\n",
            "batch 11 loss 0.0015381367418705807\n",
            "batch 12 loss 0.0015400777219754921\n",
            "batch 13 loss 0.0016799435863532302\n",
            "batch 14 loss 0.0016261471746779118\n",
            "batch 15 loss 0.0017955339514704896\n",
            "batch 16 loss 0.0018122624038736529\n",
            "batch 17 loss 0.0015659080323069314\n",
            "batch 18 loss 0.0017363035499277616\n",
            "batch 19 loss 0.001685591107356289\n",
            "batch 20 loss 0.0015582352210998078\n",
            "batch 21 loss 0.0015076780878876157\n",
            "batch 22 loss 0.0015685276119051712\n",
            "batch 23 loss 0.0016150394424835788\n",
            "batch 24 loss 0.001592548213457382\n",
            "batch 25 loss 0.0016477670115839463\n",
            "batch 26 loss 0.0015492080247032566\n",
            "batch 27 loss 0.0018698642778356357\n",
            "batch 28 loss 0.0015529412005975628\n",
            "batch 29 loss 0.0014578190545251302\n",
            "batch 30 loss 0.0016390930787771288\n",
            "batch 31 loss 0.001609545602914429\n",
            "batch 32 loss 0.0017077301212057912\n",
            "batch 33 loss 0.0015810370620752102\n",
            "batch 34 loss 0.0015446689181196494\n",
            "batch 35 loss 0.0017089177300994218\n",
            "batch 36 loss 0.0015211136328947925\n",
            "batch 37 loss 0.0015507952709502319\n",
            "batch 38 loss 0.00160329620765059\n",
            "batch 39 loss 0.0014741489539788486\n",
            "batch 40 loss 0.001572195431633224\n",
            "batch 41 loss 0.001477177370212911\n",
            "batch 42 loss 0.0016844445478012505\n",
            "batch 43 loss 0.001734231017035542\n",
            "batch 44 loss 0.0015576162879716686\n",
            "batch 45 loss 0.0016975469917581408\n",
            "batch 46 loss 0.001475715057477518\n",
            "batch 47 loss 0.0016292060773291555\n",
            "batch 48 loss 0.0015395811805700595\n",
            "batch 49 loss 0.0014836580936051051\n",
            "\n",
            "res: 0.00029308370586315577\n",
            "bc0: 0.004500797563304971\n",
            "bc1: 0.010138019137136807\n",
            "bc2: 0.008308265189792304\n",
            "bc3: 0.010084008654708654\n",
            "\n",
            "\n",
            "Epoch 5\n",
            "batch 0 loss 0.0014154757473279695\n",
            "batch 1 loss 0.0016194411999899524\n",
            "batch 2 loss 0.001600966736435318\n",
            "batch 3 loss 0.001439045519241813\n",
            "batch 4 loss 0.0018596472922346556\n",
            "batch 5 loss 0.0015002215968688058\n",
            "batch 6 loss 0.0016390650856718285\n",
            "batch 7 loss 0.0016386581412846921\n",
            "batch 8 loss 0.001518835566183384\n",
            "batch 9 loss 0.0016258398645315865\n",
            "batch 10 loss 0.001503862101781031\n",
            "batch 11 loss 0.0017964820429373963\n",
            "batch 12 loss 0.001601091278030234\n",
            "batch 13 loss 0.0015098072191415172\n",
            "batch 14 loss 0.001538594939783393\n",
            "batch 15 loss 0.0014868504756790344\n",
            "batch 16 loss 0.001797038125415359\n",
            "batch 17 loss 0.0015196153467215306\n",
            "batch 18 loss 0.0015248663606505948\n",
            "batch 19 loss 0.0016195113572681887\n",
            "batch 20 loss 0.001474772340018135\n",
            "batch 21 loss 0.0016368313652189172\n",
            "batch 22 loss 0.0014845038044624583\n",
            "batch 23 loss 0.001487805136305767\n",
            "batch 24 loss 0.0015508898039168757\n",
            "batch 25 loss 0.0014232826328256104\n",
            "batch 26 loss 0.0015342325272941252\n",
            "batch 27 loss 0.0015561621057921943\n",
            "batch 28 loss 0.0015291147799193043\n",
            "batch 29 loss 0.0015958206084633114\n",
            "batch 30 loss 0.0015028145205685523\n",
            "batch 31 loss 0.0015173633489941763\n",
            "batch 32 loss 0.0015385014161202624\n",
            "batch 33 loss 0.0014582349571473437\n",
            "batch 34 loss 0.001571823133018052\n",
            "batch 35 loss 0.0014193379493139633\n",
            "batch 36 loss 0.0014720264337403166\n",
            "batch 37 loss 0.0016369485242139022\n",
            "batch 38 loss 0.0015888557448028742\n",
            "batch 39 loss 0.001499910074617967\n",
            "batch 40 loss 0.0014991539123282982\n",
            "batch 41 loss 0.0015642462573971267\n",
            "batch 42 loss 0.001538046275004204\n",
            "batch 43 loss 0.001508968160062292\n",
            "batch 44 loss 0.0014829126416769658\n",
            "batch 45 loss 0.0014390916639964038\n",
            "batch 46 loss 0.0014950476343336477\n",
            "batch 47 loss 0.0014574685971171487\n",
            "batch 48 loss 0.0015629272513563376\n",
            "batch 49 loss 0.0014976350286150558\n",
            "\n",
            "res: 0.0002752477265150148\n",
            "bc0: 0.004498242111098581\n",
            "bc1: 0.009947190049139553\n",
            "bc2: 0.007826614612705785\n",
            "bc3: 0.009892542239297766\n",
            "\n",
            "\n",
            "Epoch 6\n",
            "batch 0 loss 0.0014306013563098693\n",
            "batch 1 loss 0.0015585796234611198\n",
            "batch 2 loss 0.0015730952480649413\n",
            "batch 3 loss 0.0015043473560814717\n",
            "batch 4 loss 0.0015277645874362657\n",
            "batch 5 loss 0.0015197813032119965\n",
            "batch 6 loss 0.0015846035301482176\n",
            "batch 7 loss 0.0015973325387424596\n",
            "batch 8 loss 0.0014804082803479145\n",
            "batch 9 loss 0.0015859641851814427\n",
            "batch 10 loss 0.0015923801261816255\n",
            "batch 11 loss 0.0015092256505740385\n",
            "batch 12 loss 0.0015128932337349044\n",
            "batch 13 loss 0.0015016746361105461\n",
            "batch 14 loss 0.0015113276959083403\n",
            "batch 15 loss 0.001538083771866018\n",
            "batch 16 loss 0.0014365608676990818\n",
            "batch 17 loss 0.0014198822746623223\n",
            "batch 18 loss 0.0014457388141848057\n",
            "batch 19 loss 0.0014670073649230941\n",
            "batch 20 loss 0.0014520422335915246\n",
            "batch 21 loss 0.0015517515014916918\n",
            "batch 22 loss 0.0015102971859900015\n",
            "batch 23 loss 0.0014529102351440522\n",
            "batch 24 loss 0.0015005323394396317\n",
            "batch 25 loss 0.0016133324638079346\n",
            "batch 26 loss 0.0013455183495042766\n",
            "batch 27 loss 0.0014417941927283946\n",
            "batch 28 loss 0.0014176853654432095\n",
            "batch 29 loss 0.0013977950088061143\n",
            "batch 30 loss 0.0016054232872462163\n",
            "batch 31 loss 0.0014823658381365235\n",
            "batch 32 loss 0.001455521441800896\n",
            "batch 33 loss 0.0013821910233787211\n",
            "batch 34 loss 0.0014744180060517625\n",
            "batch 35 loss 0.0013551972289545268\n",
            "batch 36 loss 0.001445787835432122\n",
            "batch 37 loss 0.0015457500292504777\n",
            "batch 38 loss 0.0015567189699665707\n",
            "batch 39 loss 0.0014576000999035514\n",
            "batch 40 loss 0.0014948535795430203\n",
            "batch 41 loss 0.001273063547414938\n",
            "batch 42 loss 0.0015056319379349397\n",
            "batch 43 loss 0.0014894272514388665\n",
            "batch 44 loss 0.0014140022204424982\n",
            "batch 45 loss 0.0013761621303188282\n",
            "batch 46 loss 0.001434275450427808\n",
            "batch 47 loss 0.0013800064595502395\n",
            "batch 48 loss 0.001423075735486019\n",
            "batch 49 loss 0.0013927670515280802\n",
            "\n",
            "res: 0.00025951628298961953\n",
            "bc0: 0.004493672454661125\n",
            "bc1: 0.009744075257794464\n",
            "bc2: 0.007421833823327049\n",
            "bc3: 0.009697786566291045\n",
            "\n",
            "\n",
            "Epoch 7\n",
            "batch 0 loss 0.0015713777274237036\n",
            "batch 1 loss 0.0014558852463581254\n",
            "batch 2 loss 0.0015467024943698134\n",
            "batch 3 loss 0.0014172374733761894\n",
            "batch 4 loss 0.0013868588105923698\n",
            "batch 5 loss 0.0014374512901053314\n",
            "batch 6 loss 0.001478792304636494\n",
            "batch 7 loss 0.001375964846020055\n",
            "batch 8 loss 0.0013537245423238954\n",
            "batch 9 loss 0.0014174453770785752\n",
            "batch 10 loss 0.0013613555319738865\n",
            "batch 11 loss 0.0014936319137968184\n",
            "batch 12 loss 0.0014620196724842844\n",
            "batch 13 loss 0.001360439893728306\n",
            "batch 14 loss 0.0014213403282519487\n",
            "batch 15 loss 0.001471593075299012\n",
            "batch 16 loss 0.001354261724781969\n",
            "batch 17 loss 0.0014385156322123506\n",
            "batch 18 loss 0.0015083623546773694\n",
            "batch 19 loss 0.0015429019437174663\n",
            "batch 20 loss 0.0013179532068914584\n",
            "batch 21 loss 0.0012875926501810925\n",
            "batch 22 loss 0.0015224616837383688\n",
            "batch 23 loss 0.0016457570593417443\n",
            "batch 24 loss 0.0013882394441281187\n",
            "batch 25 loss 0.0013787185031230316\n",
            "batch 26 loss 0.0014678840686520294\n",
            "batch 27 loss 0.0013819535814659787\n",
            "batch 28 loss 0.0014610702901689675\n",
            "batch 29 loss 0.00137351369647638\n",
            "batch 30 loss 0.001406898164992756\n",
            "batch 31 loss 0.001467699668504608\n",
            "batch 32 loss 0.001407361489589226\n",
            "batch 33 loss 0.0013913171962087719\n",
            "batch 34 loss 0.0013947742630205846\n",
            "batch 35 loss 0.0013228281299443388\n",
            "batch 36 loss 0.0013390846190430665\n",
            "batch 37 loss 0.00135160322735728\n",
            "batch 38 loss 0.0013725446381549836\n",
            "batch 39 loss 0.0013572273990016988\n",
            "batch 40 loss 0.0014719813557005827\n",
            "batch 41 loss 0.0014230697860898778\n",
            "batch 42 loss 0.001395447513830777\n",
            "batch 43 loss 0.0013705428449001364\n",
            "batch 44 loss 0.0014534867210828717\n",
            "batch 45 loss 0.0013219198604642432\n",
            "batch 46 loss 0.001390729061770882\n",
            "batch 47 loss 0.0012927066361015011\n",
            "batch 48 loss 0.00144627585159562\n",
            "batch 49 loss 0.0014140454338930463\n",
            "\n",
            "res: 0.00024664316465179714\n",
            "bc0: 0.004486771580403935\n",
            "bc1: 0.009555341014775122\n",
            "bc2: 0.007031502677218957\n",
            "bc3: 0.00951293401610825\n",
            "\n",
            "\n",
            "Epoch 8\n",
            "batch 0 loss 0.0013190144218359749\n",
            "batch 1 loss 0.001418829681507589\n",
            "batch 2 loss 0.0014044633232411083\n",
            "batch 3 loss 0.0013780624808863815\n",
            "batch 4 loss 0.0013211018153961797\n",
            "batch 5 loss 0.0013371482632025104\n",
            "batch 6 loss 0.0014085815074267182\n",
            "batch 7 loss 0.0013983430956604516\n",
            "batch 8 loss 0.0012854589270309994\n",
            "batch 9 loss 0.0013458077130529338\n",
            "batch 10 loss 0.0013839894167609898\n",
            "batch 11 loss 0.0014878396262956314\n",
            "batch 12 loss 0.0013476546523517108\n",
            "batch 13 loss 0.0013771712736652976\n",
            "batch 14 loss 0.0013187508073290858\n",
            "batch 15 loss 0.0014217720568609027\n",
            "batch 16 loss 0.0012680201041138906\n",
            "batch 17 loss 0.0013670554430640636\n",
            "batch 18 loss 0.0013476439375030942\n",
            "batch 19 loss 0.0013312700679019044\n",
            "batch 20 loss 0.0014057632206993303\n",
            "batch 21 loss 0.0014292549269223137\n",
            "batch 22 loss 0.001443202032797344\n",
            "batch 23 loss 0.0013699060639637312\n",
            "batch 24 loss 0.001288346828411905\n",
            "batch 25 loss 0.0013186112674805229\n",
            "batch 26 loss 0.0014664486002268172\n",
            "batch 27 loss 0.0013231805881533673\n",
            "batch 28 loss 0.0012759497778363639\n",
            "batch 29 loss 0.00130093894126762\n",
            "batch 30 loss 0.0013057074212326019\n",
            "batch 31 loss 0.0013935505763485894\n",
            "batch 32 loss 0.001343930050816977\n",
            "batch 33 loss 0.001318079214009753\n",
            "batch 34 loss 0.001414692677707324\n",
            "batch 35 loss 0.0013491176802579061\n",
            "batch 36 loss 0.0012757651176351988\n",
            "batch 37 loss 0.0012751939505636827\n",
            "batch 38 loss 0.0014185875474213908\n",
            "batch 39 loss 0.0012452234421581583\n",
            "batch 40 loss 0.001477240952976318\n",
            "batch 41 loss 0.0012653331488657426\n",
            "batch 42 loss 0.0013623369312349592\n",
            "batch 43 loss 0.001359324961881889\n",
            "batch 44 loss 0.001384726833677965\n",
            "batch 45 loss 0.0013591331192520774\n",
            "batch 46 loss 0.0013299287777498265\n",
            "batch 47 loss 0.0013610663949175153\n",
            "batch 48 loss 0.001523519910287495\n",
            "batch 49 loss 0.0014477309660850324\n",
            "\n",
            "res: 0.00023718689273516245\n",
            "bc0: 0.004477572453061953\n",
            "bc1: 0.009361756313921157\n",
            "bc2: 0.006663949490841561\n",
            "bc3: 0.009321710091029355\n",
            "\n",
            "\n",
            "Epoch 9\n",
            "batch 0 loss 0.0014140654244031374\n",
            "batch 1 loss 0.0013338460387893117\n",
            "batch 2 loss 0.001460563722303481\n",
            "batch 3 loss 0.001287941382231381\n",
            "batch 4 loss 0.0014950828285623526\n",
            "batch 5 loss 0.0012425511113388656\n",
            "batch 6 loss 0.0013397120629136281\n",
            "batch 7 loss 0.0012647845904711925\n",
            "batch 8 loss 0.0012779317273256482\n",
            "batch 9 loss 0.001249090023373749\n",
            "batch 10 loss 0.0013637798480064765\n",
            "batch 11 loss 0.0013477392835763477\n",
            "batch 12 loss 0.001347883188593562\n",
            "batch 13 loss 0.0014039932818378036\n",
            "batch 14 loss 0.001380006734063593\n",
            "batch 15 loss 0.001293416043123345\n",
            "batch 16 loss 0.0012950573130134464\n",
            "batch 17 loss 0.001314714930995261\n",
            "batch 18 loss 0.0012454877400200042\n",
            "batch 19 loss 0.0013144194666521159\n",
            "batch 20 loss 0.0014373254607771234\n",
            "batch 21 loss 0.0013792390171456538\n",
            "batch 22 loss 0.0012914938961401017\n",
            "batch 23 loss 0.0012827135371996814\n",
            "batch 24 loss 0.0012889902154238372\n",
            "batch 25 loss 0.0012314421060841427\n",
            "batch 26 loss 0.001224519957112101\n",
            "batch 27 loss 0.0012579900872669617\n",
            "batch 28 loss 0.0012971365458384425\n",
            "batch 29 loss 0.001278925925527811\n",
            "batch 30 loss 0.0012642682152851472\n",
            "batch 31 loss 0.0013692076385204916\n",
            "batch 32 loss 0.0012116343547195275\n",
            "batch 33 loss 0.0012420367019255326\n",
            "batch 34 loss 0.0013199782219593986\n",
            "batch 35 loss 0.0012817454995290817\n",
            "batch 36 loss 0.0013088989217225448\n",
            "batch 37 loss 0.0013406452496442756\n",
            "batch 38 loss 0.0013617337205850762\n",
            "batch 39 loss 0.0012928699002126424\n",
            "batch 40 loss 0.0012285837905017063\n",
            "batch 41 loss 0.0012483451981922815\n",
            "batch 42 loss 0.0012622652772499943\n",
            "batch 43 loss 0.001192578951343977\n",
            "batch 44 loss 0.0013560247618425408\n",
            "batch 45 loss 0.0014114651516453779\n",
            "batch 46 loss 0.001226761166939761\n",
            "batch 47 loss 0.0014385238850743286\n",
            "batch 48 loss 0.0012886381315959901\n",
            "batch 49 loss 0.0012702278212764245\n",
            "\n",
            "res: 0.00022775390225186727\n",
            "bc0: 0.004464347345802512\n",
            "bc1: 0.009158387175990542\n",
            "bc2: 0.006363120047368449\n",
            "bc3: 0.009125369367614752\n",
            "\n",
            "\n",
            "Epoch 10\n",
            "batch 0 loss 0.0013150950703016003\n",
            "batch 1 loss 0.001374747541078493\n",
            "batch 2 loss 0.0012749114250121974\n",
            "batch 3 loss 0.0011989664051889118\n",
            "batch 4 loss 0.0012148551973596164\n",
            "batch 5 loss 0.0013237095872725927\n",
            "batch 6 loss 0.0012699723709392074\n",
            "batch 7 loss 0.0013155620613119303\n",
            "batch 8 loss 0.001212003179972851\n",
            "batch 9 loss 0.001265563938177787\n",
            "batch 10 loss 0.0012757079988579931\n",
            "batch 11 loss 0.0012323701866698423\n",
            "batch 12 loss 0.0013176647608644984\n",
            "batch 13 loss 0.0012723375481567324\n",
            "batch 14 loss 0.0012079839660895212\n",
            "batch 15 loss 0.001196981706648562\n",
            "batch 16 loss 0.0011986445511105073\n",
            "batch 17 loss 0.0013053866324262973\n",
            "batch 18 loss 0.0013364378604667363\n",
            "batch 19 loss 0.0013073794390676562\n",
            "batch 20 loss 0.0012480433127552352\n",
            "batch 21 loss 0.0013046763559829975\n",
            "batch 22 loss 0.001242096283724937\n",
            "batch 23 loss 0.0012705110896917636\n",
            "batch 24 loss 0.001239817649683081\n",
            "batch 25 loss 0.0012360076030254478\n",
            "batch 26 loss 0.001182620906612233\n",
            "batch 27 loss 0.0012719651674622083\n",
            "batch 28 loss 0.0012746311507286254\n",
            "batch 29 loss 0.0011757683344817658\n",
            "batch 30 loss 0.0012764662433457944\n",
            "batch 31 loss 0.0012260136309388192\n",
            "batch 32 loss 0.0012354459760117356\n",
            "batch 33 loss 0.001360845136276361\n",
            "batch 34 loss 0.0013404435439780095\n",
            "batch 35 loss 0.0011916958388189943\n",
            "batch 36 loss 0.0012711652472962285\n",
            "batch 37 loss 0.0012938855648472274\n",
            "batch 38 loss 0.0012571781208469256\n",
            "batch 39 loss 0.001162290434152374\n",
            "batch 40 loss 0.0012913618289855202\n",
            "batch 41 loss 0.0012380771259883003\n",
            "batch 42 loss 0.0012230934470064036\n",
            "batch 43 loss 0.001238887289007515\n",
            "batch 44 loss 0.0013830032904752587\n",
            "batch 45 loss 0.0012522826023185839\n",
            "batch 46 loss 0.0012321601914991292\n",
            "batch 47 loss 0.0012642343375857223\n",
            "batch 48 loss 0.0012359352243253028\n",
            "batch 49 loss 0.0013260593248176887\n",
            "\n",
            "res: 0.00022174875919623118\n",
            "bc0: 0.004448302903243707\n",
            "bc1: 0.008953136236385257\n",
            "bc2: 0.006068499567736052\n",
            "bc3: 0.00891805288220607\n",
            "\n",
            "\n",
            "Epoch 11\n",
            "batch 0 loss 0.0012187271081624382\n",
            "batch 1 loss 0.0013015240499955729\n",
            "batch 2 loss 0.001302110306728102\n",
            "batch 3 loss 0.0011698241122166644\n",
            "batch 4 loss 0.0012432948254624807\n",
            "batch 5 loss 0.0013752668871869476\n",
            "batch 6 loss 0.0012993542946634752\n",
            "batch 7 loss 0.0012399717791510148\n",
            "batch 8 loss 0.001161021911744404\n",
            "batch 9 loss 0.001246484651344112\n",
            "batch 10 loss 0.0011827330819855339\n",
            "batch 11 loss 0.0012206019932558387\n",
            "batch 12 loss 0.0011338034091573663\n",
            "batch 13 loss 0.001233430067471471\n",
            "batch 14 loss 0.0011906214071282766\n",
            "batch 15 loss 0.0012476253150162065\n",
            "batch 16 loss 0.0012142036638072634\n",
            "batch 17 loss 0.0012216444770850115\n",
            "batch 18 loss 0.0012497213124749165\n",
            "batch 19 loss 0.0011989886871756194\n",
            "batch 20 loss 0.001370958543380302\n",
            "batch 21 loss 0.0011715805522838749\n",
            "batch 22 loss 0.0012860081756637877\n",
            "batch 23 loss 0.0012805812401121602\n",
            "batch 24 loss 0.00122216681946375\n",
            "batch 25 loss 0.0011496209247652594\n",
            "batch 26 loss 0.0012061209559021374\n",
            "batch 27 loss 0.001238298856498165\n",
            "batch 28 loss 0.0011814143108002427\n",
            "batch 29 loss 0.0011271309790892464\n",
            "batch 30 loss 0.0012579937162350988\n",
            "batch 31 loss 0.001148805659331541\n",
            "batch 32 loss 0.001215440918908954\n",
            "batch 33 loss 0.001167306516192552\n",
            "batch 34 loss 0.0012385188532815306\n",
            "batch 35 loss 0.0012417823613297704\n",
            "batch 36 loss 0.0012703998061553243\n",
            "batch 37 loss 0.0011689667256339505\n",
            "batch 38 loss 0.0012474949905483118\n",
            "batch 39 loss 0.0011520262181258503\n",
            "batch 40 loss 0.0011697938585366408\n",
            "batch 41 loss 0.001213593390868118\n",
            "batch 42 loss 0.0011373505524514755\n",
            "batch 43 loss 0.001242607758177012\n",
            "batch 44 loss 0.0011794812622587059\n",
            "batch 45 loss 0.0011469545814936058\n",
            "batch 46 loss 0.0012703721080875806\n",
            "batch 47 loss 0.0012245780357473402\n",
            "batch 48 loss 0.0011855333614467405\n",
            "batch 49 loss 0.0011845512715356757\n",
            "\n",
            "res: 0.00021554202840280167\n",
            "bc0: 0.0044339295803988085\n",
            "bc1: 0.008769330945179307\n",
            "bc2: 0.005761876845488243\n",
            "bc3: 0.008738736215051151\n",
            "\n",
            "\n",
            "Epoch 12\n",
            "batch 0 loss 0.001229811530690662\n",
            "batch 1 loss 0.0011306956144195985\n",
            "batch 2 loss 0.0011771432636848514\n",
            "batch 3 loss 0.0011663633378964373\n",
            "batch 4 loss 0.0011629927792866572\n",
            "batch 5 loss 0.0012140054239353991\n",
            "batch 6 loss 0.0011048624421613246\n",
            "batch 7 loss 0.0012123441253665604\n",
            "batch 8 loss 0.0012770499098994114\n",
            "batch 9 loss 0.0012086173472285124\n",
            "batch 10 loss 0.0012557342968433616\n",
            "batch 11 loss 0.0011825758077184155\n",
            "batch 12 loss 0.0011841611544692423\n",
            "batch 13 loss 0.001228169283290826\n",
            "batch 14 loss 0.0012363892051503482\n",
            "batch 15 loss 0.001112450252030526\n",
            "batch 16 loss 0.001153529472380068\n",
            "batch 17 loss 0.0011638880608752236\n",
            "batch 18 loss 0.0011433811927184507\n",
            "batch 19 loss 0.0011873237240922845\n",
            "batch 20 loss 0.0011491218085430174\n",
            "batch 21 loss 0.0011725154906012973\n",
            "batch 22 loss 0.001140147474960554\n",
            "batch 23 loss 0.0011544691695825318\n",
            "batch 24 loss 0.0011422355639131901\n",
            "batch 25 loss 0.0012751659936284672\n",
            "batch 26 loss 0.001174557319862585\n",
            "batch 27 loss 0.0011696400427670638\n",
            "batch 28 loss 0.0011576838465776227\n",
            "batch 29 loss 0.001187603617416714\n",
            "batch 30 loss 0.0012397040495272676\n",
            "batch 31 loss 0.0011803372577682768\n",
            "batch 32 loss 0.0012016796843945631\n",
            "batch 33 loss 0.0010950434244300052\n",
            "batch 34 loss 0.001172227560280956\n",
            "batch 35 loss 0.0011873850568585274\n",
            "batch 36 loss 0.0010766142770216398\n",
            "batch 37 loss 0.0011575513901643415\n",
            "batch 38 loss 0.0012360467821486655\n",
            "batch 39 loss 0.0011649324805279538\n",
            "batch 40 loss 0.0011006884768278674\n",
            "batch 41 loss 0.0011603054004635785\n",
            "batch 42 loss 0.0011940553640641962\n",
            "batch 43 loss 0.0012008998455576743\n",
            "batch 44 loss 0.0011409844265526158\n",
            "batch 45 loss 0.0012151168037740222\n",
            "batch 46 loss 0.0011343675421604637\n",
            "batch 47 loss 0.0011860648361865591\n",
            "batch 48 loss 0.0010937251773273933\n",
            "batch 49 loss 0.0012702323475867721\n",
            "\n",
            "res: 0.0002106898104258477\n",
            "bc0: 0.004419365749298924\n",
            "bc1: 0.008572348502703943\n",
            "bc2: 0.00550171898859195\n",
            "bc3: 0.0085447055301543\n",
            "\n",
            "\n",
            "Epoch 13\n",
            "batch 0 loss 0.0011544976754579916\n",
            "batch 1 loss 0.001168946339733057\n",
            "batch 2 loss 0.0011292166059282632\n",
            "batch 3 loss 0.0011608343192315246\n",
            "batch 4 loss 0.0012088840008183843\n",
            "batch 5 loss 0.0011232035566091667\n",
            "batch 6 loss 0.001132247283843645\n",
            "batch 7 loss 0.0011289005018139731\n",
            "batch 8 loss 0.0010539049081666708\n",
            "batch 9 loss 0.001134705947222279\n",
            "batch 10 loss 0.001129168581598993\n",
            "batch 11 loss 0.0011201877952117486\n",
            "batch 12 loss 0.0011815469618609166\n",
            "batch 13 loss 0.0012005090046155796\n",
            "batch 14 loss 0.0011939768085270455\n",
            "batch 15 loss 0.001099465379432057\n",
            "batch 16 loss 0.0011493844819887573\n",
            "batch 17 loss 0.0011269129067079363\n",
            "batch 18 loss 0.0011562726536229796\n",
            "batch 19 loss 0.0011163905538509257\n",
            "batch 20 loss 0.0011537790420199946\n",
            "batch 21 loss 0.001150262554995521\n",
            "batch 22 loss 0.0011327784744260675\n",
            "batch 23 loss 0.0011787893696335037\n",
            "batch 24 loss 0.0011533952082306213\n",
            "batch 25 loss 0.001119176979358448\n",
            "batch 26 loss 0.0011137065987919632\n",
            "batch 27 loss 0.0010826331622448414\n",
            "batch 28 loss 0.001139593780088442\n",
            "batch 29 loss 0.0011019936813167173\n",
            "batch 30 loss 0.0011257958167379765\n",
            "batch 31 loss 0.0011077272798212536\n",
            "batch 32 loss 0.0011253948053840026\n",
            "batch 33 loss 0.0010971565518134848\n",
            "batch 34 loss 0.0011076194764045835\n",
            "batch 35 loss 0.0011519146996526119\n",
            "batch 36 loss 0.0010992394377686962\n",
            "batch 37 loss 0.0011709110937334385\n",
            "batch 38 loss 0.0011535715940407258\n",
            "batch 39 loss 0.0010837364138428286\n",
            "batch 40 loss 0.0011004142865997072\n",
            "batch 41 loss 0.0011847901428512825\n",
            "batch 42 loss 0.0011811030821501586\n",
            "batch 43 loss 0.0011871126534622673\n",
            "batch 44 loss 0.0010985482495001339\n",
            "batch 45 loss 0.001105385258515729\n",
            "batch 46 loss 0.001116214107756796\n",
            "batch 47 loss 0.0010874622041249086\n",
            "batch 48 loss 0.0012725424800411873\n",
            "batch 49 loss 0.001140602954580105\n",
            "\n",
            "res: 0.00020716231144766466\n",
            "bc0: 0.004400321926531054\n",
            "bc1: 0.008369109224572315\n",
            "bc2: 0.005266291544473285\n",
            "bc3: 0.008340484380023187\n",
            "\n",
            "\n",
            "Epoch 14\n",
            "batch 0 loss 0.001161161732795342\n",
            "batch 1 loss 0.0011113962212336425\n",
            "batch 2 loss 0.0010689553974071368\n",
            "batch 3 loss 0.001046089449367781\n",
            "batch 4 loss 0.0010534472276120257\n",
            "batch 5 loss 0.0011617280165912805\n",
            "batch 6 loss 0.0011462137895719411\n",
            "batch 7 loss 0.0010391814897412832\n",
            "batch 8 loss 0.0011683417141328102\n",
            "batch 9 loss 0.001110362176835059\n",
            "batch 10 loss 0.0010972092971020307\n",
            "batch 11 loss 0.0011463209066446293\n",
            "batch 12 loss 0.0011335050530694893\n",
            "batch 13 loss 0.0011505328333316083\n",
            "batch 14 loss 0.001078291448894391\n",
            "batch 15 loss 0.0010716973020118677\n",
            "batch 16 loss 0.0010857336314997357\n",
            "batch 17 loss 0.0011065710449386591\n",
            "batch 18 loss 0.0011003852892812122\n",
            "batch 19 loss 0.0011063305711710434\n",
            "batch 20 loss 0.0010501209319691454\n",
            "batch 21 loss 0.0010405430374137113\n",
            "batch 22 loss 0.0010992859064077134\n",
            "batch 23 loss 0.0010866803115316386\n",
            "batch 24 loss 0.001084019180163362\n",
            "batch 25 loss 0.001115090501138285\n",
            "batch 26 loss 0.0010395033437535777\n",
            "batch 27 loss 0.0010844624083932654\n",
            "batch 28 loss 0.0011266292258450012\n",
            "batch 29 loss 0.0010480401564005885\n",
            "batch 30 loss 0.0010878981431690948\n",
            "batch 31 loss 0.001128300557196915\n",
            "batch 32 loss 0.0011613732310687972\n",
            "batch 33 loss 0.0012062677414877434\n",
            "batch 34 loss 0.001184206306049252\n",
            "batch 35 loss 0.001116952740334223\n",
            "batch 36 loss 0.0011202026500852227\n",
            "batch 37 loss 0.00105646681949267\n",
            "batch 38 loss 0.0010830224255378043\n",
            "batch 39 loss 0.0010758125990384185\n",
            "batch 40 loss 0.001160554439086432\n",
            "batch 41 loss 0.001052334204803361\n",
            "batch 42 loss 0.0011000465675118161\n",
            "batch 43 loss 0.0011073119531321656\n",
            "batch 44 loss 0.001041655678157267\n",
            "batch 45 loss 0.0010552294165350282\n",
            "batch 46 loss 0.0010760636127667726\n",
            "batch 47 loss 0.0011432780440560856\n",
            "batch 48 loss 0.0010777114249838002\n",
            "batch 49 loss 0.0010834073900489322\n",
            "\n",
            "res: 0.0002029900740512958\n",
            "bc0: 0.004380533471504367\n",
            "bc1: 0.00817347405228514\n",
            "bc2: 0.005043727370917689\n",
            "bc3: 0.00815213419793416\n",
            "\n",
            "\n",
            "Epoch 15\n",
            "batch 0 loss 0.0010767815821578655\n",
            "batch 1 loss 0.0011154611879610418\n",
            "batch 2 loss 0.0011081606446730614\n",
            "batch 3 loss 0.0010518500628839828\n",
            "batch 4 loss 0.0011382215135766834\n",
            "batch 5 loss 0.0011193956264352826\n",
            "batch 6 loss 0.0009951743458480506\n",
            "batch 7 loss 0.0011055845573960225\n",
            "batch 8 loss 0.0010372396883293897\n",
            "batch 9 loss 0.0010537652427822163\n",
            "batch 10 loss 0.0011342263670036706\n",
            "batch 11 loss 0.0010066270892965044\n",
            "batch 12 loss 0.0011270392346853263\n",
            "batch 13 loss 0.0011031159468433281\n",
            "batch 14 loss 0.0010933462342486268\n",
            "batch 15 loss 0.0010520988682600906\n",
            "batch 16 loss 0.001048706887275657\n",
            "batch 17 loss 0.0011003494607815271\n",
            "batch 18 loss 0.0010192133843742006\n",
            "batch 19 loss 0.0011043614847531928\n",
            "batch 20 loss 0.0010705598744121245\n",
            "batch 21 loss 0.0010689084354991365\n",
            "batch 22 loss 0.0010583058955346224\n",
            "batch 23 loss 0.0010127182502250283\n",
            "batch 24 loss 0.0010413651074326717\n",
            "batch 25 loss 0.0010496764718111863\n",
            "batch 26 loss 0.0010871910652932775\n",
            "batch 27 loss 0.0010284515126220183\n",
            "batch 28 loss 0.0011030048633089633\n",
            "batch 29 loss 0.0011334290231405763\n",
            "batch 30 loss 0.0010495315360542783\n",
            "batch 31 loss 0.001034835257652559\n",
            "batch 32 loss 0.0009876489013292333\n",
            "batch 33 loss 0.0010367320591746034\n",
            "batch 34 loss 0.0010423333806426171\n",
            "batch 35 loss 0.001170204349912483\n",
            "batch 36 loss 0.0010537160510814955\n",
            "batch 37 loss 0.0010612134273272006\n",
            "batch 38 loss 0.001040657071457356\n",
            "batch 39 loss 0.0010441174781199401\n",
            "batch 40 loss 0.0010497720189783428\n",
            "batch 41 loss 0.0010005760644381755\n",
            "batch 42 loss 0.0010016485746731068\n",
            "batch 43 loss 0.0011225052604056283\n",
            "batch 44 loss 0.0011218264314886075\n",
            "batch 45 loss 0.001058010095962686\n",
            "batch 46 loss 0.0010342316919719405\n",
            "batch 47 loss 0.0010295681925596688\n",
            "batch 48 loss 0.0010086995123277718\n",
            "batch 49 loss 0.0010729925920277231\n",
            "\n",
            "res: 0.00020016434486060239\n",
            "bc0: 0.004361597708185289\n",
            "bc1: 0.007988892651951775\n",
            "bc2: 0.004800018421969626\n",
            "bc3: 0.007967341095664864\n",
            "\n",
            "\n",
            "Epoch 16\n",
            "batch 0 loss 0.001017731098988362\n",
            "batch 1 loss 0.0010601778458486496\n",
            "batch 2 loss 0.0011351397816197436\n",
            "batch 3 loss 0.00101916200032281\n",
            "batch 4 loss 0.0010030679290179275\n",
            "batch 5 loss 0.0010437288630573954\n",
            "batch 6 loss 0.0010594327727148662\n",
            "batch 7 loss 0.0009914134085246868\n",
            "batch 8 loss 0.0009767680100891455\n",
            "batch 9 loss 0.0010353627455651178\n",
            "batch 10 loss 0.001044391936722563\n",
            "batch 11 loss 0.001050734657080449\n",
            "batch 12 loss 0.0010549968282019405\n",
            "batch 13 loss 0.0010300600479288846\n",
            "batch 14 loss 0.0010414999233261221\n",
            "batch 15 loss 0.001048480413404442\n",
            "batch 16 loss 0.0009847383462435\n",
            "batch 17 loss 0.0011275972551077005\n",
            "batch 18 loss 0.0010208265002437141\n",
            "batch 19 loss 0.0010827313743202549\n",
            "batch 20 loss 0.0010398183089831967\n",
            "batch 21 loss 0.0010763949550962205\n",
            "batch 22 loss 0.0010575552788020465\n",
            "batch 23 loss 0.0010001539042234346\n",
            "batch 24 loss 0.0009767497445869406\n",
            "batch 25 loss 0.0010430828894177938\n",
            "batch 26 loss 0.00101287614650103\n",
            "batch 27 loss 0.0010214297048231613\n",
            "batch 28 loss 0.0010001681662112974\n",
            "batch 29 loss 0.0010563208882679183\n",
            "batch 30 loss 0.0010294650432164547\n",
            "batch 31 loss 0.0010501509756999656\n",
            "batch 32 loss 0.0009792253416336463\n",
            "batch 33 loss 0.0011012666704601055\n",
            "batch 34 loss 0.0010415731836341219\n",
            "batch 35 loss 0.0010161775553006235\n",
            "batch 36 loss 0.0009595524667631963\n",
            "batch 37 loss 0.0010201226295803842\n",
            "batch 38 loss 0.0011348595143597542\n",
            "batch 39 loss 0.0009787579428240049\n",
            "batch 40 loss 0.001019274414857314\n",
            "batch 41 loss 0.0010056466949338978\n",
            "batch 42 loss 0.000995029789806141\n",
            "batch 43 loss 0.0009746640412869689\n",
            "batch 44 loss 0.0010716859887643935\n",
            "batch 45 loss 0.0009832223369394088\n",
            "batch 46 loss 0.001000208002366916\n",
            "batch 47 loss 0.0010740280335566137\n",
            "batch 48 loss 0.0010043317909999329\n",
            "batch 49 loss 0.001010593063715159\n",
            "\n",
            "res: 0.00019774080861005935\n",
            "bc0: 0.004342916618431872\n",
            "bc1: 0.007804993302277721\n",
            "bc2: 0.004571154668873929\n",
            "bc3: 0.007782999545866732\n",
            "\n",
            "\n",
            "Epoch 17\n",
            "batch 0 loss 0.0010752175699397098\n",
            "batch 1 loss 0.0009754946025651382\n",
            "batch 2 loss 0.0010195989974060241\n",
            "batch 3 loss 0.0009336806720237584\n",
            "batch 4 loss 0.0009826218468133193\n",
            "batch 5 loss 0.0010445012096973675\n",
            "batch 6 loss 0.0009485022856582265\n",
            "batch 7 loss 0.0009763983764812373\n",
            "batch 8 loss 0.0010479205151720642\n",
            "batch 9 loss 0.0011186343605055606\n",
            "batch 10 loss 0.0010196322692070086\n",
            "batch 11 loss 0.0010259635548090324\n",
            "batch 12 loss 0.0010116784064495427\n",
            "batch 13 loss 0.0010686519746863172\n",
            "batch 14 loss 0.001074571762861859\n",
            "batch 15 loss 0.0009381527042428148\n",
            "batch 16 loss 0.0010451091234941095\n",
            "batch 17 loss 0.0010302374784291574\n",
            "batch 18 loss 0.001025779703616855\n",
            "batch 19 loss 0.0009865873731770862\n",
            "batch 20 loss 0.000922750413349271\n",
            "batch 21 loss 0.000946404649894788\n",
            "batch 22 loss 0.0011006904768390566\n",
            "batch 23 loss 0.0010024148334626184\n",
            "batch 24 loss 0.0010490156618399083\n",
            "batch 25 loss 0.0009715974994490787\n",
            "batch 26 loss 0.0009544235067390128\n",
            "batch 27 loss 0.000963711062030144\n",
            "batch 28 loss 0.000964539059665067\n",
            "batch 29 loss 0.0009563970878736523\n",
            "batch 30 loss 0.0009991655915499355\n",
            "batch 31 loss 0.0010185817211093575\n",
            "batch 32 loss 0.000981362839824248\n",
            "batch 33 loss 0.0009199782424336156\n",
            "batch 34 loss 0.001005186395781114\n",
            "batch 35 loss 0.0010224219898767113\n",
            "batch 36 loss 0.000991124962811878\n",
            "batch 37 loss 0.0010040418396832869\n",
            "batch 38 loss 0.0009464689475593122\n",
            "batch 39 loss 0.000936521483844025\n",
            "batch 40 loss 0.0009923170193708144\n",
            "batch 41 loss 0.000989749033054413\n",
            "batch 42 loss 0.0010804142114046041\n",
            "batch 43 loss 0.0009594470447489466\n",
            "batch 44 loss 0.0010187793955498837\n",
            "batch 45 loss 0.0009620328658275373\n",
            "batch 46 loss 0.0009638220382412299\n",
            "batch 47 loss 0.0009389843260100407\n",
            "batch 48 loss 0.00106605476946976\n",
            "batch 49 loss 0.0009700705596228562\n",
            "\n",
            "res: 0.0001952234578418686\n",
            "bc0: 0.0043224089993468195\n",
            "bc1: 0.007613845164346949\n",
            "bc2: 0.004376179103813984\n",
            "bc3: 0.0075941113017894656\n",
            "\n",
            "\n",
            "Epoch 18\n",
            "batch 0 loss 0.000944516988864422\n",
            "batch 1 loss 0.0009712839505359012\n",
            "batch 2 loss 0.0009540962385616741\n",
            "batch 3 loss 0.0010299793698926834\n",
            "batch 4 loss 0.0009588000832013915\n",
            "batch 5 loss 0.0009617097095175436\n",
            "batch 6 loss 0.0009070326148109925\n",
            "batch 7 loss 0.0009684906033645874\n",
            "batch 8 loss 0.001035143993948049\n",
            "batch 9 loss 0.0009704977621110646\n",
            "batch 10 loss 0.0009404122916699527\n",
            "batch 11 loss 0.000904759385407333\n",
            "batch 12 loss 0.0009163319251936511\n",
            "batch 13 loss 0.0009448892578494617\n",
            "batch 14 loss 0.0009585108927707955\n",
            "batch 15 loss 0.0009847416453191858\n",
            "batch 16 loss 0.0009376571598815544\n",
            "batch 17 loss 0.0009202217945316221\n",
            "batch 18 loss 0.0009622001848892962\n",
            "batch 19 loss 0.0010062603563338077\n",
            "batch 20 loss 0.0009880066256105664\n",
            "batch 21 loss 0.0010164280031519652\n",
            "batch 22 loss 0.0009290989015603581\n",
            "batch 23 loss 0.0009915480938348163\n",
            "batch 24 loss 0.0010048238733624377\n",
            "batch 25 loss 0.0009848291387146596\n",
            "batch 26 loss 0.0009365928765902102\n",
            "batch 27 loss 0.000955530127794547\n",
            "batch 28 loss 0.0009241294495957159\n",
            "batch 29 loss 0.0010562536081120267\n",
            "batch 30 loss 0.0010114428209956952\n",
            "batch 31 loss 0.0009642535738783514\n",
            "batch 32 loss 0.0010529236935363604\n",
            "batch 33 loss 0.000934588690417141\n",
            "batch 34 loss 0.0009745717873652816\n",
            "batch 35 loss 0.001004991599561945\n",
            "batch 36 loss 0.000996279598491863\n",
            "batch 37 loss 0.0008945609256798864\n",
            "batch 38 loss 0.0009447805902599216\n",
            "batch 39 loss 0.0009142512795891122\n",
            "batch 40 loss 0.000957313669433774\n",
            "batch 41 loss 0.0010392467528045988\n",
            "batch 42 loss 0.0008768532887964194\n",
            "batch 43 loss 0.0009685099217315816\n",
            "batch 44 loss 0.0009762539571313957\n",
            "batch 45 loss 0.0009331781578859961\n",
            "batch 46 loss 0.0009502893512769898\n",
            "batch 47 loss 0.0009965364717310988\n",
            "batch 48 loss 0.0010420998345286165\n",
            "batch 49 loss 0.0009935034866278612\n",
            "\n",
            "res: 0.00019375454663995203\n",
            "bc0: 0.004297602248224166\n",
            "bc1: 0.007434710484469007\n",
            "bc2: 0.0041642781995937725\n",
            "bc3: 0.0074110145403498045\n",
            "\n",
            "\n",
            "Epoch 19\n",
            "batch 0 loss 0.0009483154679185658\n",
            "batch 1 loss 0.0009138453324399043\n",
            "batch 2 loss 0.0009495804407395568\n",
            "batch 3 loss 0.0010095360430344017\n",
            "batch 4 loss 0.000960278406315351\n",
            "batch 5 loss 0.001002163551103353\n",
            "batch 6 loss 0.0008937944625212408\n",
            "batch 7 loss 0.0009292320271483011\n",
            "batch 8 loss 0.0009529799921347346\n",
            "batch 9 loss 0.000974036175712631\n",
            "batch 10 loss 0.0009269752180644733\n",
            "batch 11 loss 0.001013299509019876\n",
            "batch 12 loss 0.0009741413335303245\n",
            "batch 13 loss 0.0009396445841441799\n",
            "batch 14 loss 0.0009258695412670941\n",
            "batch 15 loss 0.0009325930359590319\n",
            "batch 16 loss 0.0009405093750701427\n",
            "batch 17 loss 0.000978495215311323\n",
            "batch 18 loss 0.0009141716649505246\n",
            "batch 19 loss 0.0009513562139673346\n",
            "batch 20 loss 0.000978018048682085\n",
            "batch 21 loss 0.0008866614976313249\n",
            "batch 22 loss 0.0010219592451358446\n",
            "batch 23 loss 0.0009255243518902968\n",
            "batch 24 loss 0.0009444076549476722\n",
            "batch 25 loss 0.0008521915162811698\n",
            "batch 26 loss 0.0009717623797729804\n",
            "batch 27 loss 0.0008817372345979083\n",
            "batch 28 loss 0.0009195403758842735\n",
            "batch 29 loss 0.0008956536446205291\n",
            "batch 30 loss 0.0009291776054680975\n",
            "batch 31 loss 0.0008626452790871776\n",
            "batch 32 loss 0.0009294148235624752\n",
            "batch 33 loss 0.0009259463387110656\n",
            "batch 34 loss 0.0008804853002085123\n",
            "batch 35 loss 0.0009496050777831776\n",
            "batch 36 loss 0.0010607245544263218\n",
            "batch 37 loss 0.0009463873887794805\n",
            "batch 38 loss 0.0009368234514548505\n",
            "batch 39 loss 0.0009889390913333663\n",
            "batch 40 loss 0.000918783073278005\n",
            "batch 41 loss 0.0009139204306150981\n",
            "batch 42 loss 0.0008697559700568422\n",
            "batch 43 loss 0.0009496154033566456\n",
            "batch 44 loss 0.0010021709113421028\n",
            "batch 45 loss 0.0009257689416679562\n",
            "batch 46 loss 0.0009108132896345808\n",
            "batch 47 loss 0.0008932473671565191\n",
            "batch 48 loss 0.0009055070762450541\n",
            "batch 49 loss 0.0008799573898117405\n",
            "\n",
            "res: 0.00019094868392720483\n",
            "bc0: 0.004274687418943561\n",
            "bc1: 0.007223222615228848\n",
            "bc2: 0.004044476175491977\n",
            "bc3: 0.007205521221370992\n",
            "\n",
            "\n",
            "Epoch 20\n",
            "batch 0 loss 0.0009401805813719789\n",
            "batch 1 loss 0.0008836960913284254\n",
            "batch 2 loss 0.0009007654064242281\n",
            "batch 3 loss 0.0009088437411026517\n",
            "batch 4 loss 0.000912817921115622\n",
            "batch 5 loss 0.0009137738097343871\n",
            "batch 6 loss 0.0008936998961918525\n",
            "batch 7 loss 0.0009571588893509152\n",
            "batch 8 loss 0.000910044351687853\n",
            "batch 9 loss 0.0008944732992932307\n",
            "batch 10 loss 0.0009094133902447482\n",
            "batch 11 loss 0.0008815708946154232\n",
            "batch 12 loss 0.0009190283654674446\n",
            "batch 13 loss 0.0008735153434488658\n",
            "batch 14 loss 0.0008904336387895142\n",
            "batch 15 loss 0.0008620120482858455\n",
            "batch 16 loss 0.0009068521898214152\n",
            "batch 17 loss 0.0009118469083573505\n",
            "batch 18 loss 0.0008560108359603858\n",
            "batch 19 loss 0.0009147820103392835\n",
            "batch 20 loss 0.0009499094516308135\n",
            "batch 21 loss 0.0009069733583041356\n",
            "batch 22 loss 0.0009284860134982923\n",
            "batch 23 loss 0.0008533686492632523\n",
            "batch 24 loss 0.0008884591877023107\n",
            "batch 25 loss 0.0008638465354524552\n",
            "batch 26 loss 0.0009304331557120842\n",
            "batch 27 loss 0.0009253413932475467\n",
            "batch 28 loss 0.0009592869312154953\n",
            "batch 29 loss 0.0008463835006491092\n",
            "batch 30 loss 0.0009533529545862274\n",
            "batch 31 loss 0.0009916068478341854\n",
            "batch 32 loss 0.0009298760095432776\n",
            "batch 33 loss 0.0009419710963562577\n",
            "batch 34 loss 0.0009897185559409446\n",
            "batch 35 loss 0.0008843092738700165\n",
            "batch 36 loss 0.0009057896790019157\n",
            "batch 37 loss 0.0009136243064865855\n",
            "batch 38 loss 0.0009317629563591913\n",
            "batch 39 loss 0.00103361668444112\n",
            "batch 40 loss 0.0009146687037448471\n",
            "batch 41 loss 0.0008331033143286269\n",
            "batch 42 loss 0.0009285080694057072\n",
            "batch 43 loss 0.0008592741616023803\n",
            "batch 44 loss 0.0008511031243660339\n",
            "batch 45 loss 0.0008728074769425031\n",
            "batch 46 loss 0.000892474298443684\n",
            "batch 47 loss 0.0009462007481812142\n",
            "batch 48 loss 0.000868005923290762\n",
            "batch 49 loss 0.0009095108078986707\n",
            "\n",
            "res: 0.00018852876443898204\n",
            "bc0: 0.004250843705159525\n",
            "bc1: 0.007048868609385136\n",
            "bc2: 0.0038601783201510594\n",
            "bc3: 0.007032420092294498\n",
            "\n",
            "\n",
            "Epoch 21\n",
            "batch 0 loss 0.0010409805671087033\n",
            "batch 1 loss 0.0008472222928149922\n",
            "batch 2 loss 0.000829028024038122\n",
            "batch 3 loss 0.0008465224490958089\n",
            "batch 4 loss 0.0009614650534431321\n",
            "batch 5 loss 0.0008803962102702639\n",
            "batch 6 loss 0.0008944867142436334\n",
            "batch 7 loss 0.0008831382876813177\n",
            "batch 8 loss 0.0008627298342328986\n",
            "batch 9 loss 0.0008892995903410435\n",
            "batch 10 loss 0.0009579775027356393\n",
            "batch 11 loss 0.0009089601281957962\n",
            "batch 12 loss 0.0008788224311019197\n",
            "batch 13 loss 0.0008406812259678965\n",
            "batch 14 loss 0.0008886878209697552\n",
            "batch 15 loss 0.000885280075833573\n",
            "batch 16 loss 0.000820526480555763\n",
            "batch 17 loss 0.0009423595841009536\n",
            "batch 18 loss 0.0008251387163018763\n",
            "batch 19 loss 0.0008646440445130173\n",
            "batch 20 loss 0.0009129473225006622\n",
            "batch 21 loss 0.0008614094687855782\n",
            "batch 22 loss 0.000873289453357545\n",
            "batch 23 loss 0.0008245374571675599\n",
            "batch 24 loss 0.0009560208914741343\n",
            "batch 25 loss 0.0009062389633910815\n",
            "batch 26 loss 0.0008463933145735597\n",
            "batch 27 loss 0.0008317611446605867\n",
            "batch 28 loss 0.0008541012924835975\n",
            "batch 29 loss 0.0009398022624670997\n",
            "batch 30 loss 0.0008874503806544879\n",
            "batch 31 loss 0.0008854775820011582\n",
            "batch 32 loss 0.0008579529810051705\n",
            "batch 33 loss 0.000885042992513899\n",
            "batch 34 loss 0.0009356436618383846\n",
            "batch 35 loss 0.0008581135025105147\n",
            "batch 36 loss 0.0008647763180315423\n",
            "batch 37 loss 0.0008433945036069511\n",
            "batch 38 loss 0.0008670383032403549\n",
            "batch 39 loss 0.0009257335019635857\n",
            "batch 40 loss 0.0008922452357157574\n",
            "batch 41 loss 0.0009574872611625863\n",
            "batch 42 loss 0.0008308810582994379\n",
            "batch 43 loss 0.000866993733001091\n",
            "batch 44 loss 0.0008358338526219402\n",
            "batch 45 loss 0.0008663467934789625\n",
            "batch 46 loss 0.0008565578248122142\n",
            "batch 47 loss 0.000838670132509604\n",
            "batch 48 loss 0.0008742563057987386\n",
            "batch 49 loss 0.0008622859370493937\n",
            "\n",
            "res: 0.00018684826169509944\n",
            "bc0: 0.004226681169713345\n",
            "bc1: 0.006856002447132696\n",
            "bc2: 0.00371366282029876\n",
            "bc3: 0.006841236325305369\n",
            "\n",
            "\n",
            "Epoch 22\n",
            "batch 0 loss 0.0008813368194559862\n",
            "batch 1 loss 0.0008735528509116637\n",
            "batch 2 loss 0.0008531575358063884\n",
            "batch 3 loss 0.0008366954198660294\n",
            "batch 4 loss 0.0008675738529362555\n",
            "batch 5 loss 0.0008575401181321892\n",
            "batch 6 loss 0.000850604235080236\n",
            "batch 7 loss 0.0008348028409515368\n",
            "batch 8 loss 0.0009551048034059426\n",
            "batch 9 loss 0.0009245903920093609\n",
            "batch 10 loss 0.0008475988209330315\n",
            "batch 11 loss 0.0009184053762818716\n",
            "batch 12 loss 0.0008617206768972562\n",
            "batch 13 loss 0.0009039843727627838\n",
            "batch 14 loss 0.0008854204801063624\n",
            "batch 15 loss 0.0008481130511484238\n",
            "batch 16 loss 0.0008254331114529608\n",
            "batch 17 loss 0.0008500688352221433\n",
            "batch 18 loss 0.0008654252148508211\n",
            "batch 19 loss 0.0008579647007193257\n",
            "batch 20 loss 0.0008230470370480167\n",
            "batch 21 loss 0.0008361282835243636\n",
            "batch 22 loss 0.0007966160528696072\n",
            "batch 23 loss 0.0008795106328965543\n",
            "batch 24 loss 0.0008348948570377467\n",
            "batch 25 loss 0.0008284870665837058\n",
            "batch 26 loss 0.0008081166407665982\n",
            "batch 27 loss 0.0008759863488093032\n",
            "batch 28 loss 0.0008119768521488731\n",
            "batch 29 loss 0.000855233857375722\n",
            "batch 30 loss 0.0008455480626757408\n",
            "batch 31 loss 0.0008538304899220182\n",
            "batch 32 loss 0.0007932583620333658\n",
            "batch 33 loss 0.0009290266562760926\n",
            "batch 34 loss 0.0008522133235721376\n",
            "batch 35 loss 0.0008307157844913122\n",
            "batch 36 loss 0.0008100264384716121\n",
            "batch 37 loss 0.0008053118689462906\n",
            "batch 38 loss 0.0008934439130860593\n",
            "batch 39 loss 0.0008404784342197648\n",
            "batch 40 loss 0.0008158034150421736\n",
            "batch 41 loss 0.0008384017087735209\n",
            "batch 42 loss 0.0008268670043603256\n",
            "batch 43 loss 0.0008913103027207766\n",
            "batch 44 loss 0.0008514713407940418\n",
            "batch 45 loss 0.0008087454302909296\n",
            "batch 46 loss 0.0008409528322938057\n",
            "batch 47 loss 0.000881957344522826\n",
            "batch 48 loss 0.0008347845613085222\n",
            "batch 49 loss 0.0009126418189846967\n",
            "\n",
            "res: 0.00018515458098055857\n",
            "bc0: 0.004206223881915355\n",
            "bc1: 0.006690299531468756\n",
            "bc2: 0.0035243309073700892\n",
            "bc3: 0.0066720299241270755\n",
            "\n",
            "\n",
            "Epoch 23\n",
            "batch 0 loss 0.0008861235174580989\n",
            "batch 1 loss 0.0009180169340239782\n",
            "batch 2 loss 0.0008395924437513138\n",
            "batch 3 loss 0.0008474117172649527\n",
            "batch 4 loss 0.0009245031240744601\n",
            "batch 5 loss 0.000776802463570096\n",
            "batch 6 loss 0.000803289795920236\n",
            "batch 7 loss 0.0008052568182587961\n",
            "batch 8 loss 0.0008172313625590289\n",
            "batch 9 loss 0.0008245767756174163\n",
            "batch 10 loss 0.000929140926333049\n",
            "batch 11 loss 0.000791651970555829\n",
            "batch 12 loss 0.0008052889561780591\n",
            "batch 13 loss 0.0008556113521939421\n",
            "batch 14 loss 0.0008500540345387742\n",
            "batch 15 loss 0.0008457081730014375\n",
            "batch 16 loss 0.0008191737244346749\n",
            "batch 17 loss 0.0008103764340541501\n",
            "batch 18 loss 0.0008230917680841007\n",
            "batch 19 loss 0.0008144961699770227\n",
            "batch 20 loss 0.0008137197390338072\n",
            "batch 21 loss 0.0008184826236758455\n",
            "batch 22 loss 0.0008555487167284765\n",
            "batch 23 loss 0.000773671991509895\n",
            "batch 24 loss 0.0008260775829294198\n",
            "batch 25 loss 0.0008271243030471808\n",
            "batch 26 loss 0.0008567818422261016\n",
            "batch 27 loss 0.0008164747801194093\n",
            "batch 28 loss 0.0008175111220890248\n",
            "batch 29 loss 0.0007832698995139966\n",
            "batch 30 loss 0.0008103026101685964\n",
            "batch 31 loss 0.0007879576354174963\n",
            "batch 32 loss 0.0008150227014798543\n",
            "batch 33 loss 0.0007833800698649582\n",
            "batch 34 loss 0.000811051030048705\n",
            "batch 35 loss 0.0008240907003201313\n",
            "batch 36 loss 0.0008115109624405241\n",
            "batch 37 loss 0.0007652244136660844\n",
            "batch 38 loss 0.0008531460762648605\n",
            "batch 39 loss 0.0008159101832121066\n",
            "batch 40 loss 0.0008748288960138328\n",
            "batch 41 loss 0.0008481688125284448\n",
            "batch 42 loss 0.0008393362277707483\n",
            "batch 43 loss 0.0008230013754384067\n",
            "batch 44 loss 0.0008116785998674705\n",
            "batch 45 loss 0.0008833611061231806\n",
            "batch 46 loss 0.0008871800768837012\n",
            "batch 47 loss 0.0008208908537254304\n",
            "batch 48 loss 0.0007921975451445248\n",
            "batch 49 loss 0.0007584455047738606\n",
            "\n",
            "res: 0.00018323312878128036\n",
            "bc0: 0.004178627761651374\n",
            "bc1: 0.006501885820785813\n",
            "bc2: 0.0033970541555800255\n",
            "bc3: 0.006487623969262667\n",
            "\n",
            "\n",
            "Epoch 24\n",
            "batch 0 loss 0.0007696125062320225\n",
            "batch 1 loss 0.0008113386697772396\n",
            "batch 2 loss 0.0007907783444767089\n",
            "batch 3 loss 0.0007563767182149796\n",
            "batch 4 loss 0.0008551238064102614\n",
            "batch 5 loss 0.0007920912991042801\n",
            "batch 6 loss 0.0008205735696079294\n",
            "batch 7 loss 0.0008212023133581113\n",
            "batch 8 loss 0.0008457803504999516\n",
            "batch 9 loss 0.0007803597768383405\n",
            "batch 10 loss 0.0007832301169465174\n",
            "batch 11 loss 0.0007547915298898616\n",
            "batch 12 loss 0.0008052657864547134\n",
            "batch 13 loss 0.0007997576415679533\n",
            "batch 14 loss 0.0008414519087991802\n",
            "batch 15 loss 0.0009270706842739971\n",
            "batch 16 loss 0.0008396333322719079\n",
            "batch 17 loss 0.0007785160645083619\n",
            "batch 18 loss 0.0008597308763584996\n",
            "batch 19 loss 0.0007786506007401931\n",
            "batch 20 loss 0.0007966657763437332\n",
            "batch 21 loss 0.0007730965387191775\n",
            "batch 22 loss 0.0007625608163439679\n",
            "batch 23 loss 0.0007976683963475013\n",
            "batch 24 loss 0.0008126390915090879\n",
            "batch 25 loss 0.0008433079913427167\n",
            "batch 26 loss 0.0008497439932169527\n",
            "batch 27 loss 0.0007514212045646549\n",
            "batch 28 loss 0.0008114029355467391\n",
            "batch 29 loss 0.000780175922328483\n",
            "batch 30 loss 0.0007887799610757775\n",
            "batch 31 loss 0.0008362393431347567\n",
            "batch 32 loss 0.0007310143417197108\n",
            "batch 33 loss 0.0008002217639160644\n",
            "batch 34 loss 0.0008455669291029416\n",
            "batch 35 loss 0.0007865655235116502\n",
            "batch 36 loss 0.0007859959524029371\n",
            "batch 37 loss 0.0007939258857102511\n",
            "batch 38 loss 0.000754707560338768\n",
            "batch 39 loss 0.0008079107623200953\n",
            "batch 40 loss 0.0007533745041650752\n",
            "batch 41 loss 0.0008090575847854364\n",
            "batch 42 loss 0.0007683768608936161\n",
            "batch 43 loss 0.0008261749954553299\n",
            "batch 44 loss 0.0007826923169503302\n",
            "batch 45 loss 0.0008272172595993963\n",
            "batch 46 loss 0.0008317554632900294\n",
            "batch 47 loss 0.0007856123992786135\n",
            "batch 48 loss 0.0007533326569934009\n",
            "batch 49 loss 0.0008753218014838412\n",
            "\n",
            "res: 0.0001811919021872903\n",
            "bc0: 0.004153168855730207\n",
            "bc1: 0.0063220596613420416\n",
            "bc2: 0.003266777832694188\n",
            "bc3: 0.006309744450718033\n",
            "\n",
            "\n",
            "Epoch 25\n",
            "batch 0 loss 0.0007689964256282707\n",
            "batch 1 loss 0.0008261314739749392\n",
            "batch 2 loss 0.0007578169036178021\n",
            "batch 3 loss 0.0008161920860089598\n",
            "batch 4 loss 0.0007527908269802571\n",
            "batch 5 loss 0.0008336430154375913\n",
            "batch 6 loss 0.0007333425815805943\n",
            "batch 7 loss 0.0008148733686455538\n",
            "batch 8 loss 0.0007913343640048538\n",
            "batch 9 loss 0.0007864629419098347\n",
            "batch 10 loss 0.000762087519032594\n",
            "batch 11 loss 0.0007781275940717304\n",
            "batch 12 loss 0.000803930052208834\n",
            "batch 13 loss 0.0007967908478266942\n",
            "batch 14 loss 0.000823404397847689\n",
            "batch 15 loss 0.0008092737434052138\n",
            "batch 16 loss 0.0008111710074832983\n",
            "batch 17 loss 0.0007678813474042268\n",
            "batch 18 loss 0.0007486469713700507\n",
            "batch 19 loss 0.0008295464502650914\n",
            "batch 20 loss 0.0007868579337543077\n",
            "batch 21 loss 0.0007859207102621094\n",
            "batch 22 loss 0.0007757894776016754\n",
            "batch 23 loss 0.0007835443743730132\n",
            "batch 24 loss 0.0007514985048712996\n",
            "batch 25 loss 0.0007715328295553008\n",
            "batch 26 loss 0.0007579144623401148\n",
            "batch 27 loss 0.0007518993605037334\n",
            "batch 28 loss 0.000724043849845964\n",
            "batch 29 loss 0.0007683375579853489\n",
            "batch 30 loss 0.0007298047035143156\n",
            "batch 31 loss 0.0008463993331370546\n",
            "batch 32 loss 0.0007975059901808292\n",
            "batch 33 loss 0.0007958095396543637\n",
            "batch 34 loss 0.000737186835178216\n",
            "batch 35 loss 0.0007537235281849465\n",
            "batch 36 loss 0.0007334959465541914\n",
            "batch 37 loss 0.0007872327108186416\n",
            "batch 38 loss 0.000788309576753309\n",
            "batch 39 loss 0.0007744531760391477\n",
            "batch 40 loss 0.0007865821189122547\n",
            "batch 41 loss 0.0007652395957140497\n",
            "batch 42 loss 0.0007749854925435462\n",
            "batch 43 loss 0.00073330143974249\n",
            "batch 44 loss 0.0007688691702900459\n",
            "batch 45 loss 0.0007685722647776002\n",
            "batch 46 loss 0.00081214757793862\n",
            "batch 47 loss 0.0007679539839415078\n",
            "batch 48 loss 0.000719950527287213\n",
            "batch 49 loss 0.0007940729678853867\n",
            "\n",
            "res: 0.00017961787207034787\n",
            "bc0: 0.004126086579420591\n",
            "bc1: 0.006137488780729286\n",
            "bc2: 0.0031507646426390586\n",
            "bc3: 0.006124148498407949\n",
            "\n",
            "\n",
            "Epoch 26\n",
            "batch 0 loss 0.0007268656544675931\n",
            "batch 1 loss 0.0007231724660243128\n",
            "batch 2 loss 0.0007055677469490322\n",
            "batch 3 loss 0.0008256669023853554\n",
            "batch 4 loss 0.0007544052860741705\n",
            "batch 5 loss 0.0008000780568158672\n",
            "batch 6 loss 0.0007787369787912362\n",
            "batch 7 loss 0.0007323165537018689\n",
            "batch 8 loss 0.0007479216888001711\n",
            "batch 9 loss 0.0007387622389053176\n",
            "batch 10 loss 0.0007514752871835334\n",
            "batch 11 loss 0.0007490229617018701\n",
            "batch 12 loss 0.0007864903604380316\n",
            "batch 13 loss 0.000763154345561003\n",
            "batch 14 loss 0.0008116967973809486\n",
            "batch 15 loss 0.0007261818780434515\n",
            "batch 16 loss 0.0007451987420088299\n",
            "batch 17 loss 0.0007747630408230235\n",
            "batch 18 loss 0.0006924047179931588\n",
            "batch 19 loss 0.0007734499065791194\n",
            "batch 20 loss 0.0007710079763260388\n",
            "batch 21 loss 0.0007741962297818551\n",
            "batch 22 loss 0.0006914706435554295\n",
            "batch 23 loss 0.0007952787012950361\n",
            "batch 24 loss 0.0007184156958703978\n",
            "batch 25 loss 0.0007870901340070803\n",
            "batch 26 loss 0.0006957219571415882\n",
            "batch 27 loss 0.0007351022390421757\n",
            "batch 28 loss 0.0007494314631273048\n",
            "batch 29 loss 0.000781345960834301\n",
            "batch 30 loss 0.0007565992979048403\n",
            "batch 31 loss 0.0007501164539926226\n",
            "batch 32 loss 0.000736072581456\n",
            "batch 33 loss 0.0008041349209351616\n",
            "batch 34 loss 0.0007534870313047296\n",
            "batch 35 loss 0.0007611163317481267\n",
            "batch 36 loss 0.0008136402143113637\n",
            "batch 37 loss 0.0008235230318674782\n",
            "batch 38 loss 0.0007513166096560862\n",
            "batch 39 loss 0.0007343400612084141\n",
            "batch 40 loss 0.0007340824580273474\n",
            "batch 41 loss 0.0007665554271586316\n",
            "batch 42 loss 0.0007117919379322987\n",
            "batch 43 loss 0.0006910861942898054\n",
            "batch 44 loss 0.0007879409927675883\n",
            "batch 45 loss 0.0007163413014598228\n",
            "batch 46 loss 0.0007466462950688398\n",
            "batch 47 loss 0.0007735414954017944\n",
            "batch 48 loss 0.000735870821252905\n",
            "batch 49 loss 0.0007662356186810008\n",
            "\n",
            "res: 0.0001780134559405136\n",
            "bc0: 0.004098201556526385\n",
            "bc1: 0.005967042295873046\n",
            "bc2: 0.003015765559745488\n",
            "bc3: 0.00595320434954048\n",
            "\n",
            "\n",
            "Epoch 27\n",
            "batch 0 loss 0.0007328318814756284\n",
            "batch 1 loss 0.0007129315295457022\n",
            "batch 2 loss 0.0007310904692761541\n",
            "batch 3 loss 0.0007246739388320744\n",
            "batch 4 loss 0.000778712233952201\n",
            "batch 5 loss 0.0007895150442889097\n",
            "batch 6 loss 0.000745320293217611\n",
            "batch 7 loss 0.0007077273987763177\n",
            "batch 8 loss 0.000731102429820295\n",
            "batch 9 loss 0.000747406727570672\n",
            "batch 10 loss 0.0007590004020228959\n",
            "batch 11 loss 0.0007606467776062131\n",
            "batch 12 loss 0.0007051283286067582\n",
            "batch 13 loss 0.0007115474213674832\n",
            "batch 14 loss 0.0007464756289354709\n",
            "batch 15 loss 0.0007628956429330357\n",
            "batch 16 loss 0.0007035516346681339\n",
            "batch 17 loss 0.0007137789138843848\n",
            "batch 18 loss 0.0007190731448643873\n",
            "batch 19 loss 0.0006948499665199997\n",
            "batch 20 loss 0.0007841973655902895\n",
            "batch 21 loss 0.0007030314569122454\n",
            "batch 22 loss 0.0006897001422276672\n",
            "batch 23 loss 0.0007224157384816887\n",
            "batch 24 loss 0.0006848157868985122\n",
            "batch 25 loss 0.0007332590095521612\n",
            "batch 26 loss 0.0007594125008956389\n",
            "batch 27 loss 0.0007405702204601999\n",
            "batch 28 loss 0.0007232315002095419\n",
            "batch 29 loss 0.0006975003469072722\n",
            "batch 30 loss 0.000734831165225661\n",
            "batch 31 loss 0.0007434655653838032\n",
            "batch 32 loss 0.000731793794171038\n",
            "batch 33 loss 0.0007008249579329523\n",
            "batch 34 loss 0.0007356039391237782\n",
            "batch 35 loss 0.0007400459730429635\n",
            "batch 36 loss 0.0007347795044416947\n",
            "batch 37 loss 0.0007373213134425722\n",
            "batch 38 loss 0.0006997227373140554\n",
            "batch 39 loss 0.0007442986625481949\n",
            "batch 40 loss 0.0007464766897828258\n",
            "batch 41 loss 0.0008198057453116631\n",
            "batch 42 loss 0.0007370535134839812\n",
            "batch 43 loss 0.0007312238203130921\n",
            "batch 44 loss 0.0007154960832284347\n",
            "batch 45 loss 0.0007190815379388376\n",
            "batch 46 loss 0.000746106172963838\n",
            "batch 47 loss 0.0007317661193966517\n",
            "batch 48 loss 0.0007401145041052575\n",
            "batch 49 loss 0.0006731097735265366\n",
            "\n",
            "res: 0.00017567701564233775\n",
            "bc0: 0.004077054508265704\n",
            "bc1: 0.005807322304619143\n",
            "bc2: 0.0028789186595656463\n",
            "bc3: 0.0057966575567529625\n",
            "\n",
            "\n",
            "Epoch 28\n",
            "batch 0 loss 0.0007344118350505025\n",
            "batch 1 loss 0.0006838028240461725\n",
            "batch 2 loss 0.00079438174393752\n",
            "batch 3 loss 0.0007254219273352765\n",
            "batch 4 loss 0.000814403756466387\n",
            "batch 5 loss 0.0007057038302679925\n",
            "batch 6 loss 0.000745197864665826\n",
            "batch 7 loss 0.000680443650000244\n",
            "batch 8 loss 0.0006771624811741841\n",
            "batch 9 loss 0.0008036519504392985\n",
            "batch 10 loss 0.0006740944450093166\n",
            "batch 11 loss 0.0006894865029999778\n",
            "batch 12 loss 0.000814097196001344\n",
            "batch 13 loss 0.0006735679125184883\n",
            "batch 14 loss 0.0007328716352834208\n",
            "batch 15 loss 0.0007193704529700376\n",
            "batch 16 loss 0.0007669189541987308\n",
            "batch 17 loss 0.0006996459805825742\n",
            "batch 18 loss 0.0007042880121835087\n",
            "batch 19 loss 0.0007135201921165254\n",
            "batch 20 loss 0.0006871581752035066\n",
            "batch 21 loss 0.0006816280823467374\n",
            "batch 22 loss 0.0007108820934179389\n",
            "batch 23 loss 0.000712273106447344\n",
            "batch 24 loss 0.0006893734598601657\n",
            "batch 25 loss 0.0006553938902355005\n",
            "batch 26 loss 0.0006928713316500367\n",
            "batch 27 loss 0.0007399211755090994\n",
            "batch 28 loss 0.0007264007492789133\n",
            "batch 29 loss 0.0006710743431377542\n",
            "batch 30 loss 0.0007075438501510852\n",
            "batch 31 loss 0.0007414952427124532\n",
            "batch 32 loss 0.0006958322328289793\n",
            "batch 33 loss 0.0007158831994225215\n",
            "batch 34 loss 0.0007065793166982099\n",
            "batch 35 loss 0.0007316765648132742\n",
            "batch 36 loss 0.0006986479579971691\n",
            "batch 37 loss 0.0007252911771367039\n",
            "batch 38 loss 0.0006795587546985298\n",
            "batch 39 loss 0.0007257266351502847\n",
            "batch 40 loss 0.0007167203510906717\n",
            "batch 41 loss 0.0006531351011757914\n",
            "batch 42 loss 0.0007150437565660637\n",
            "batch 43 loss 0.0006956587187583042\n",
            "batch 44 loss 0.0007430966932643382\n",
            "batch 45 loss 0.0006674821339998591\n",
            "batch 46 loss 0.0006765284253958434\n",
            "batch 47 loss 0.0006540210672200815\n",
            "batch 48 loss 0.0006518314486417635\n",
            "batch 49 loss 0.0006544200597531332\n",
            "\n",
            "res: 0.00017405790413940896\n",
            "bc0: 0.0040505901979606155\n",
            "bc1: 0.005634235801004277\n",
            "bc2: 0.0027652390483579054\n",
            "bc3: 0.005621840344513965\n",
            "\n",
            "\n",
            "Epoch 29\n",
            "batch 0 loss 0.0006771370328533908\n",
            "batch 1 loss 0.0007681262721948453\n",
            "batch 2 loss 0.0007140354120982539\n",
            "batch 3 loss 0.0007232457034613716\n",
            "batch 4 loss 0.0006754058865834014\n",
            "batch 5 loss 0.0006938439082285187\n",
            "batch 6 loss 0.0007416487860045889\n",
            "batch 7 loss 0.0006358408365248222\n",
            "batch 8 loss 0.0006875147737323141\n",
            "batch 9 loss 0.0006731566526013648\n",
            "batch 10 loss 0.0006314611105513724\n",
            "batch 11 loss 0.0007086484070918638\n",
            "batch 12 loss 0.000735059384320732\n",
            "batch 13 loss 0.0006504159757068143\n",
            "batch 14 loss 0.0006355445935930292\n",
            "batch 15 loss 0.0006966487406075293\n",
            "batch 16 loss 0.000660592121372248\n",
            "batch 17 loss 0.0006883281899371568\n",
            "batch 18 loss 0.0006569434977240021\n",
            "batch 19 loss 0.0007085157499976108\n",
            "batch 20 loss 0.0007066721887918778\n",
            "batch 21 loss 0.0007424495616774206\n",
            "batch 22 loss 0.0006799948868389986\n",
            "batch 23 loss 0.0007020843534765642\n",
            "batch 24 loss 0.0006552607271476389\n",
            "batch 25 loss 0.0007034612010559166\n",
            "batch 26 loss 0.0006725004615531245\n",
            "batch 27 loss 0.000694489636106601\n",
            "batch 28 loss 0.0007408660347247146\n",
            "batch 29 loss 0.00063511108065249\n",
            "batch 30 loss 0.0006253217933099091\n",
            "batch 31 loss 0.0007522275426553092\n",
            "batch 32 loss 0.0007169034825384914\n",
            "batch 33 loss 0.000674315024582055\n",
            "batch 34 loss 0.0006815130839905602\n",
            "batch 35 loss 0.0006832708740011341\n",
            "batch 36 loss 0.0006642574384879441\n",
            "batch 37 loss 0.0007635859454092487\n",
            "batch 38 loss 0.0006799120195908024\n",
            "batch 39 loss 0.0007039506853172634\n",
            "batch 40 loss 0.0006949765952217466\n",
            "batch 41 loss 0.0007093374791731967\n",
            "batch 42 loss 0.0006411852424409386\n",
            "batch 43 loss 0.0006462725817244993\n",
            "batch 44 loss 0.0006731817098028796\n",
            "batch 45 loss 0.0006792290249159187\n",
            "batch 46 loss 0.0006825035846224686\n",
            "batch 47 loss 0.0006364730584256031\n",
            "batch 48 loss 0.0006974902022759168\n",
            "batch 49 loss 0.0006951454177168434\n",
            "\n",
            "res: 0.00017191283427596502\n",
            "bc0: 0.004023795697396119\n",
            "bc1: 0.005477868566448031\n",
            "bc2: 0.0026417089950443273\n",
            "bc3: 0.005468922594735994\n",
            "\n",
            "\n",
            "Epoch 30\n",
            "batch 0 loss 0.000712110395479597\n",
            "batch 1 loss 0.0006377042800057797\n",
            "batch 2 loss 0.0006490292479123613\n",
            "batch 3 loss 0.0007376488894399276\n",
            "batch 4 loss 0.0006814007569887141\n",
            "batch 5 loss 0.0007049749232823767\n",
            "batch 6 loss 0.0006594066629239434\n",
            "batch 7 loss 0.0007165321138689882\n",
            "batch 8 loss 0.0006934450334312448\n",
            "batch 9 loss 0.0006972447826588898\n",
            "batch 10 loss 0.0006775905898343162\n",
            "batch 11 loss 0.0006630439949508594\n",
            "batch 12 loss 0.0006790778886719962\n",
            "batch 13 loss 0.0006603831100394301\n",
            "batch 14 loss 0.0006913916406364686\n",
            "batch 15 loss 0.0006239281088846512\n",
            "batch 16 loss 0.0006462116549767833\n",
            "batch 17 loss 0.0006779760546542142\n",
            "batch 18 loss 0.0006439228690342866\n",
            "batch 19 loss 0.0006399154824572685\n",
            "batch 20 loss 0.0006420718501454796\n",
            "batch 21 loss 0.0006411664898452495\n",
            "batch 22 loss 0.0006510301476641058\n",
            "batch 23 loss 0.0006647246270710662\n",
            "batch 24 loss 0.0006620554710972236\n",
            "batch 25 loss 0.0006659555147427197\n",
            "batch 26 loss 0.0006736706324487092\n",
            "batch 27 loss 0.0006731542017698561\n",
            "batch 28 loss 0.0006505038154140533\n",
            "batch 29 loss 0.0006275034017683367\n",
            "batch 30 loss 0.0006402561878336063\n",
            "batch 31 loss 0.0007175834833648246\n",
            "batch 32 loss 0.0006762844643073987\n",
            "batch 33 loss 0.0006620736408713371\n",
            "batch 34 loss 0.0006799293902570443\n",
            "batch 35 loss 0.0006256615847367547\n",
            "batch 36 loss 0.0006837042004192963\n",
            "batch 37 loss 0.0006724716018514784\n",
            "batch 38 loss 0.0006645214698818192\n",
            "batch 39 loss 0.0006850429996731993\n",
            "batch 40 loss 0.0006490929658280993\n",
            "batch 41 loss 0.000641017326212197\n",
            "batch 42 loss 0.0006844598934936091\n",
            "batch 43 loss 0.000639250202093813\n",
            "batch 44 loss 0.0006243957572618433\n",
            "batch 45 loss 0.000708706361898148\n",
            "batch 46 loss 0.0006855301445198967\n",
            "batch 47 loss 0.0006260416439260499\n",
            "batch 48 loss 0.0007188426198654382\n",
            "batch 49 loss 0.0006266430044260561\n",
            "\n",
            "res: 0.0001700648458483571\n",
            "bc0: 0.003997156476700207\n",
            "bc1: 0.0053082864238442615\n",
            "bc2: 0.002545165262271636\n",
            "bc3: 0.005299460018781586\n",
            "\n",
            "\n",
            "finished unconstrained optimization\n",
            "res: 0.0001700648458483571\n",
            "bc0: 0.003997156476700207\n",
            "bc1: 0.0053082864238442615\n",
            "bc2: 0.002545165262271636\n",
            "bc3: 0.005299460018781586\n",
            "\n",
            "\n",
            "[0.02462076 0.04208319 0.03905324 0.04196348]\n",
            "4.0\n",
            "Epoch 1\n",
            "batch 0 loss 0.0011328344959812187\n",
            "batch 1 loss 0.0011057049236672407\n",
            "batch 2 loss 0.0011985017261027047\n",
            "batch 3 loss 0.001162769090840606\n",
            "batch 4 loss 0.0011371113876673543\n",
            "batch 5 loss 0.0011094754600549448\n",
            "batch 6 loss 0.0011611753473039087\n",
            "batch 7 loss 0.0010765225609637918\n",
            "batch 8 loss 0.0010674605763592887\n",
            "batch 9 loss 0.0011047268533246047\n",
            "batch 10 loss 0.0011526557644902812\n",
            "batch 11 loss 0.001076756075904073\n",
            "batch 12 loss 0.0011278318707385717\n",
            "batch 13 loss 0.0011151072107330692\n",
            "batch 14 loss 0.0010620498137586417\n",
            "batch 15 loss 0.0011030400805132955\n",
            "batch 16 loss 0.0010959418145559193\n",
            "batch 17 loss 0.0010910536051108497\n",
            "batch 18 loss 0.0011213912107007026\n",
            "batch 19 loss 0.0010584603533829076\n",
            "batch 20 loss 0.0011030185445985864\n",
            "batch 21 loss 0.001113426221616442\n",
            "batch 22 loss 0.0010518251789773127\n",
            "batch 23 loss 0.0011510142199442346\n",
            "batch 24 loss 0.0010881606236349982\n",
            "batch 25 loss 0.0010665796016353986\n",
            "batch 26 loss 0.0010970859798699975\n",
            "batch 27 loss 0.0011323997579946864\n",
            "batch 28 loss 0.001088018421403387\n",
            "batch 29 loss 0.0010787774170219746\n",
            "batch 30 loss 0.0010293523048585992\n",
            "batch 31 loss 0.0010937663912855952\n",
            "batch 32 loss 0.0011240141951931518\n",
            "batch 33 loss 0.0011084400886938325\n",
            "batch 34 loss 0.0011742862155530573\n",
            "batch 35 loss 0.001104230984481524\n",
            "batch 36 loss 0.0010582937290311981\n",
            "batch 37 loss 0.0010679696045750925\n",
            "batch 38 loss 0.0011113551908572073\n",
            "batch 39 loss 0.0010710881233350657\n",
            "batch 40 loss 0.0010952148639252939\n",
            "batch 41 loss 0.0010659378911625095\n",
            "batch 42 loss 0.0010592388633402369\n",
            "batch 43 loss 0.0010839151037247761\n",
            "batch 44 loss 0.0010934018270940488\n",
            "batch 45 loss 0.0010866312145894866\n",
            "batch 46 loss 0.0010546844630791546\n",
            "batch 47 loss 0.0010805156398666334\n",
            "batch 48 loss 0.0010415111957577648\n",
            "batch 49 loss 0.001078753940459017\n",
            "\n",
            "res: 0.00017280185890244476\n",
            "bc0: 0.003912570864964781\n",
            "bc1: 0.004935030554433762\n",
            "bc2: 0.002544967445584538\n",
            "bc3: 0.004925247982223176\n",
            "\n",
            "\n",
            "Epoch 2\n",
            "batch 0 loss 0.0010467027107494856\n",
            "batch 1 loss 0.0010726665210269329\n",
            "batch 2 loss 0.0010552042751558648\n",
            "batch 3 loss 0.001093509475863792\n",
            "batch 4 loss 0.0010927681227753047\n",
            "batch 5 loss 0.0010803769845977528\n",
            "batch 6 loss 0.0010711990004748052\n",
            "batch 7 loss 0.001041698494652689\n",
            "batch 8 loss 0.0010988689235000922\n",
            "batch 9 loss 0.0009905695838975305\n",
            "batch 10 loss 0.0010694191832128013\n",
            "batch 11 loss 0.0010290203195356062\n",
            "batch 12 loss 0.0010624200777303945\n",
            "batch 13 loss 0.0010366100259243437\n",
            "batch 14 loss 0.0009732253083443064\n",
            "batch 15 loss 0.001106480788854293\n",
            "batch 16 loss 0.0010455886626015796\n",
            "batch 17 loss 0.0010660396848292115\n",
            "batch 18 loss 0.0011030197387614507\n",
            "batch 19 loss 0.001027796680138371\n",
            "batch 20 loss 0.001085926394025894\n",
            "batch 21 loss 0.0010312828566344748\n",
            "batch 22 loss 0.0010877218681518315\n",
            "batch 23 loss 0.001112432665606147\n",
            "batch 24 loss 0.0010672555560626486\n",
            "batch 25 loss 0.001010071753519391\n",
            "batch 26 loss 0.0010327135725995311\n",
            "batch 27 loss 0.0009915544819536358\n",
            "batch 28 loss 0.0010153275604475685\n",
            "batch 29 loss 0.0010060446024440708\n",
            "batch 30 loss 0.0010157177160721946\n",
            "batch 31 loss 0.0010198743771077004\n",
            "batch 32 loss 0.0010259572464177524\n",
            "batch 33 loss 0.0010048118267497706\n",
            "batch 34 loss 0.0010086874567876456\n",
            "batch 35 loss 0.001013341372096023\n",
            "batch 36 loss 0.0009760968227611806\n",
            "batch 37 loss 0.0010594205033327244\n",
            "batch 38 loss 0.0010015694392692786\n",
            "batch 39 loss 0.0009760936979461454\n",
            "batch 40 loss 0.0010000296803735185\n",
            "batch 41 loss 0.0009863289831517465\n",
            "batch 42 loss 0.0010060978910136841\n",
            "batch 43 loss 0.0009660800677958367\n",
            "batch 44 loss 0.0010148980779626693\n",
            "batch 45 loss 0.0010060501566470802\n",
            "batch 46 loss 0.0009566469462987208\n",
            "batch 47 loss 0.0010345771377107784\n",
            "batch 48 loss 0.0010549563616343005\n",
            "batch 49 loss 0.0010234411764838756\n",
            "\n",
            "res: 0.00016998759172488472\n",
            "bc0: 0.0038675432180164016\n",
            "bc1: 0.004658502922156499\n",
            "bc2: 0.0023652137307607173\n",
            "bc3: 0.004646994325007301\n",
            "\n",
            "\n",
            "Epoch 3\n",
            "batch 0 loss 0.0010947435049054116\n",
            "batch 1 loss 0.0009715500310144931\n",
            "batch 2 loss 0.000997524075148132\n",
            "batch 3 loss 0.0009766741461601767\n",
            "batch 4 loss 0.0010115874027866556\n",
            "batch 5 loss 0.0009826473971910245\n",
            "batch 6 loss 0.0009861871640891686\n",
            "batch 7 loss 0.0009667215168344306\n",
            "batch 8 loss 0.000986776858799127\n",
            "batch 9 loss 0.0010024034806717164\n",
            "batch 10 loss 0.0010117035087844088\n",
            "batch 11 loss 0.0010187508790988672\n",
            "batch 12 loss 0.0009813308690565879\n",
            "batch 13 loss 0.001024571121327107\n",
            "batch 14 loss 0.0009941656205002835\n",
            "batch 15 loss 0.000991111034881767\n",
            "batch 16 loss 0.001040381480126441\n",
            "batch 17 loss 0.0010002536173013283\n",
            "batch 18 loss 0.0010027383207812856\n",
            "batch 19 loss 0.0009914120307575743\n",
            "batch 20 loss 0.0009847650485391936\n",
            "batch 21 loss 0.0009728601745156083\n",
            "batch 22 loss 0.0010404898217688603\n",
            "batch 23 loss 0.0009407080772753447\n",
            "batch 24 loss 0.0009885476536646556\n",
            "batch 25 loss 0.0010549803989754018\n",
            "batch 26 loss 0.0009498168009510224\n",
            "batch 27 loss 0.00098917970349893\n",
            "batch 28 loss 0.0009400859537966319\n",
            "batch 29 loss 0.0009964038178491407\n",
            "batch 30 loss 0.00097118004613006\n",
            "batch 31 loss 0.000928186839624634\n",
            "batch 32 loss 0.0009559147935450983\n",
            "batch 33 loss 0.0009458863272229507\n",
            "batch 34 loss 0.000931166055020819\n",
            "batch 35 loss 0.0009429178429322615\n",
            "batch 36 loss 0.0009962933115123009\n",
            "batch 37 loss 0.0009381579250664655\n",
            "batch 38 loss 0.0009712350184429802\n",
            "batch 39 loss 0.0009189585870509491\n",
            "batch 40 loss 0.0009679551245714087\n",
            "batch 41 loss 0.0009030300588504865\n",
            "batch 42 loss 0.000991400888772062\n",
            "batch 43 loss 0.000906236428093692\n",
            "batch 44 loss 0.000983815882820284\n",
            "batch 45 loss 0.0009031673693009974\n",
            "batch 46 loss 0.0009530391558511558\n",
            "batch 47 loss 0.0009192173772912936\n",
            "batch 48 loss 0.0009600138238772887\n",
            "batch 49 loss 0.0009719429119799391\n",
            "\n",
            "res: 0.0001671888798166584\n",
            "bc0: 0.0038202107465044933\n",
            "bc1: 0.004395099297316986\n",
            "bc2: 0.002199105659626569\n",
            "bc3: 0.004387278376104429\n",
            "\n",
            "\n",
            "Epoch 4\n",
            "batch 0 loss 0.0009710451255678362\n",
            "batch 1 loss 0.0009201085187639422\n",
            "batch 2 loss 0.0009877732772320973\n",
            "batch 3 loss 0.0009665437133334712\n",
            "batch 4 loss 0.0009738267682659051\n",
            "batch 5 loss 0.0009394289857482335\n",
            "batch 6 loss 0.0009746023483730307\n",
            "batch 7 loss 0.0008900239169155125\n",
            "batch 8 loss 0.0008544206088431657\n",
            "batch 9 loss 0.0009453887247764807\n",
            "batch 10 loss 0.0009018867189336952\n",
            "batch 11 loss 0.0009069431663415582\n",
            "batch 12 loss 0.0009459165418360557\n",
            "batch 13 loss 0.0009431478649731573\n",
            "batch 14 loss 0.0009742345449537051\n",
            "batch 15 loss 0.0008800086696908984\n",
            "batch 16 loss 0.0009044952822196703\n",
            "batch 17 loss 0.0009030625179089022\n",
            "batch 18 loss 0.0009143069610142756\n",
            "batch 19 loss 0.0009698724026360976\n",
            "batch 20 loss 0.0008760286592332995\n",
            "batch 21 loss 0.0009615642697191213\n",
            "batch 22 loss 0.0009556926381506553\n",
            "batch 23 loss 0.0009045798863475974\n",
            "batch 24 loss 0.0009221882723896036\n",
            "batch 25 loss 0.0009168753583664807\n",
            "batch 26 loss 0.0010127720356026837\n",
            "batch 27 loss 0.0008862722141783352\n",
            "batch 28 loss 0.0008943353294996824\n",
            "batch 29 loss 0.0009162675836615895\n",
            "batch 30 loss 0.0008979984959026797\n",
            "batch 31 loss 0.000937964134854673\n",
            "batch 32 loss 0.0009312497629892255\n",
            "batch 33 loss 0.0008982890583372264\n",
            "batch 34 loss 0.0009451716533902308\n",
            "batch 35 loss 0.000909073253407414\n",
            "batch 36 loss 0.0008855326177794133\n",
            "batch 37 loss 0.0009513261754071197\n",
            "batch 38 loss 0.0008816030860872824\n",
            "batch 39 loss 0.0008722489025711679\n",
            "batch 40 loss 0.0009240302632599823\n",
            "batch 41 loss 0.0009488508062469321\n",
            "batch 42 loss 0.000942143071563521\n",
            "batch 43 loss 0.0009485606727306825\n",
            "batch 44 loss 0.0009070703320621028\n",
            "batch 45 loss 0.000899168485276937\n",
            "batch 46 loss 0.0009403025134896333\n",
            "batch 47 loss 0.0008500539225530827\n",
            "batch 48 loss 0.0009207719757588097\n",
            "batch 49 loss 0.0008672005362597244\n",
            "\n",
            "res: 0.0001657019671702834\n",
            "bc0: 0.0037689891428914117\n",
            "bc1: 0.00414066143233929\n",
            "bc2: 0.0020553200976766114\n",
            "bc3: 0.004128977930339442\n",
            "\n",
            "\n",
            "Epoch 5\n",
            "batch 0 loss 0.0009227167544300589\n",
            "batch 1 loss 0.0008524086175427374\n",
            "batch 2 loss 0.0009377754401024775\n",
            "batch 3 loss 0.0008975601099379621\n",
            "batch 4 loss 0.0009167328534794609\n",
            "batch 5 loss 0.0009035712690243243\n",
            "batch 6 loss 0.000877191095405917\n",
            "batch 7 loss 0.000918100651589965\n",
            "batch 8 loss 0.000915205417980698\n",
            "batch 9 loss 0.0008621974320302992\n",
            "batch 10 loss 0.0009144424869472855\n",
            "batch 11 loss 0.0008976182862714622\n",
            "batch 12 loss 0.0008540302405559406\n",
            "batch 13 loss 0.0008985785821199444\n",
            "batch 14 loss 0.0008864451613502156\n",
            "batch 15 loss 0.0008622665891031652\n",
            "batch 16 loss 0.0009330280311045583\n",
            "batch 17 loss 0.0008701835323464042\n",
            "batch 18 loss 0.0008980051027181501\n",
            "batch 19 loss 0.0008877635903712124\n",
            "batch 20 loss 0.0009486512639101672\n",
            "batch 21 loss 0.0008540204770473669\n",
            "batch 22 loss 0.0009198111509644436\n",
            "batch 23 loss 0.0008233596615157073\n",
            "batch 24 loss 0.0008556582802583181\n",
            "batch 25 loss 0.0008686381533608655\n",
            "batch 26 loss 0.0009095246934050147\n",
            "batch 27 loss 0.0008276154266611866\n",
            "batch 28 loss 0.0008178725661371447\n",
            "batch 29 loss 0.0008757041023601327\n",
            "batch 30 loss 0.0008260284388135839\n",
            "batch 31 loss 0.0009282980856495616\n",
            "batch 32 loss 0.0008201963651506844\n",
            "batch 33 loss 0.0009184026042605703\n",
            "batch 34 loss 0.0008225344025421827\n",
            "batch 35 loss 0.0008657423059180392\n",
            "batch 36 loss 0.0008374086557905962\n",
            "batch 37 loss 0.0008610910645284098\n",
            "batch 38 loss 0.0008819539777396115\n",
            "batch 39 loss 0.0008498111780855713\n",
            "batch 40 loss 0.000844650541705669\n",
            "batch 41 loss 0.0008432087646386634\n",
            "batch 42 loss 0.0008246878048850297\n",
            "batch 43 loss 0.0008936654926944277\n",
            "batch 44 loss 0.0008436276350777806\n",
            "batch 45 loss 0.0008550447082667594\n",
            "batch 46 loss 0.0008412489708433631\n",
            "batch 47 loss 0.0008413076389793461\n",
            "batch 48 loss 0.0008036362449570258\n",
            "batch 49 loss 0.0008647912257214454\n",
            "\n",
            "res: 0.00016233631476743107\n",
            "bc0: 0.0037263493050549426\n",
            "bc1: 0.003907072463366165\n",
            "bc2: 0.0019132755608264275\n",
            "bc3: 0.0038984026266536617\n",
            "\n",
            "\n",
            "Epoch 6\n",
            "batch 0 loss 0.0008437180857759449\n",
            "batch 1 loss 0.0008825099823511054\n",
            "batch 2 loss 0.0008067362274791739\n",
            "batch 3 loss 0.0009166480366628033\n",
            "batch 4 loss 0.0008620434715971279\n",
            "batch 5 loss 0.0008516634884402453\n",
            "batch 6 loss 0.0008254084054616635\n",
            "batch 7 loss 0.0008215213613888493\n",
            "batch 8 loss 0.0008322693091758329\n",
            "batch 9 loss 0.0008006386441423961\n",
            "batch 10 loss 0.0008483972233572729\n",
            "batch 11 loss 0.0008196544151091963\n",
            "batch 12 loss 0.0008760075283777222\n",
            "batch 13 loss 0.0008136098874715022\n",
            "batch 14 loss 0.000827568210419588\n",
            "batch 15 loss 0.000802232032524785\n",
            "batch 16 loss 0.0008841272291911816\n",
            "batch 17 loss 0.0008163190465453338\n",
            "batch 18 loss 0.0007900071827735669\n",
            "batch 19 loss 0.0008428799100449963\n",
            "batch 20 loss 0.0008466883745623074\n",
            "batch 21 loss 0.0008554774416327604\n",
            "batch 22 loss 0.0007734065430194204\n",
            "batch 23 loss 0.0008970419120283821\n",
            "batch 24 loss 0.0008178616833943819\n",
            "batch 25 loss 0.0008385556509763624\n",
            "batch 26 loss 0.0008702443757520242\n",
            "batch 27 loss 0.0007966605436282152\n",
            "batch 28 loss 0.0008274240705915673\n",
            "batch 29 loss 0.0007976237988035365\n",
            "batch 30 loss 0.0008345083646196752\n",
            "batch 31 loss 0.0008074349665162543\n",
            "batch 32 loss 0.0008508000145523003\n",
            "batch 33 loss 0.0008507716911221912\n",
            "batch 34 loss 0.0008776227593858327\n",
            "batch 35 loss 0.0007659957713195578\n",
            "batch 36 loss 0.0008420715549710433\n",
            "batch 37 loss 0.0007794225903481536\n",
            "batch 38 loss 0.0008387749132058855\n",
            "batch 39 loss 0.0007840702880816684\n",
            "batch 40 loss 0.0008793747271563369\n",
            "batch 41 loss 0.0007848185131064442\n",
            "batch 42 loss 0.0007773376647895106\n",
            "batch 43 loss 0.0008509992870814512\n",
            "batch 44 loss 0.000766920791846925\n",
            "batch 45 loss 0.0007996553912520163\n",
            "batch 46 loss 0.0008165403774098687\n",
            "batch 47 loss 0.0008166965836037368\n",
            "batch 48 loss 0.0007980948536788295\n",
            "batch 49 loss 0.0007742271159855901\n",
            "\n",
            "res: 0.00016005098933496499\n",
            "bc0: 0.003680160046923291\n",
            "bc1: 0.0036901592746215496\n",
            "bc2: 0.0017709373181708993\n",
            "bc3: 0.003680697195886342\n",
            "\n",
            "\n",
            "Epoch 7\n",
            "batch 0 loss 0.0007683132118849168\n",
            "batch 1 loss 0.0008023724668178568\n",
            "batch 2 loss 0.0007629349215379094\n",
            "batch 3 loss 0.0008236221813486382\n",
            "batch 4 loss 0.0008085331165209791\n",
            "batch 5 loss 0.0008131399116499582\n",
            "batch 6 loss 0.0008065845231263123\n",
            "batch 7 loss 0.0007918212027816151\n",
            "batch 8 loss 0.000849937712373681\n",
            "batch 9 loss 0.0007508830233766394\n",
            "batch 10 loss 0.0007884910777497344\n",
            "batch 11 loss 0.0007901505615927553\n",
            "batch 12 loss 0.0008080914820623419\n",
            "batch 13 loss 0.0008352966647054465\n",
            "batch 14 loss 0.0008471870348309495\n",
            "batch 15 loss 0.0008127504328316156\n",
            "batch 16 loss 0.0007576074438784345\n",
            "batch 17 loss 0.0007760545659229661\n",
            "batch 18 loss 0.0008197855010805333\n",
            "batch 19 loss 0.000773289487948462\n",
            "batch 20 loss 0.0007691905373307766\n",
            "batch 21 loss 0.0008102820710500922\n",
            "batch 22 loss 0.0007695119093157493\n",
            "batch 23 loss 0.0008045788168323301\n",
            "batch 24 loss 0.0007462201894480311\n",
            "batch 25 loss 0.000784868895785797\n",
            "batch 26 loss 0.0008050116734568727\n",
            "batch 27 loss 0.0007333644387189721\n",
            "batch 28 loss 0.000782655171940336\n",
            "batch 29 loss 0.0007842361369387829\n",
            "batch 30 loss 0.0007930111218134141\n",
            "batch 31 loss 0.0007477579386426134\n",
            "batch 32 loss 0.0008176559074175849\n",
            "batch 33 loss 0.0007283952171626164\n",
            "batch 34 loss 0.0007677211648231169\n",
            "batch 35 loss 0.0007510452453010205\n",
            "batch 36 loss 0.00075713761901041\n",
            "batch 37 loss 0.0007721954584209201\n",
            "batch 38 loss 0.0008046553505615115\n",
            "batch 39 loss 0.0008266954787973132\n",
            "batch 40 loss 0.0008555750164915637\n",
            "batch 41 loss 0.0007385218444094895\n",
            "batch 42 loss 0.0008424827869614773\n",
            "batch 43 loss 0.0007230343104653189\n",
            "batch 44 loss 0.000734439119232745\n",
            "batch 45 loss 0.0007522949742662384\n",
            "batch 46 loss 0.0007757410877051146\n",
            "batch 47 loss 0.0008074425205839458\n",
            "batch 48 loss 0.0007739572946333774\n",
            "batch 49 loss 0.0007174573134676692\n",
            "\n",
            "res: 0.00015718744954248617\n",
            "bc0: 0.0036399344764088556\n",
            "bc1: 0.0034747851260358326\n",
            "bc2: 0.0016628563700460284\n",
            "bc3: 0.003468466462953078\n",
            "\n",
            "\n",
            "Epoch 8\n",
            "batch 0 loss 0.0007261571398394881\n",
            "batch 1 loss 0.0007747221850729054\n",
            "batch 2 loss 0.0007267438126989117\n",
            "batch 3 loss 0.0007454803893027775\n",
            "batch 4 loss 0.0007595759265866124\n",
            "batch 5 loss 0.0007111334519242907\n",
            "batch 6 loss 0.0007304949503248947\n",
            "batch 7 loss 0.000799155611901225\n",
            "batch 8 loss 0.0007743975044177957\n",
            "batch 9 loss 0.0007626920967190859\n",
            "batch 10 loss 0.0007472092149391109\n",
            "batch 11 loss 0.0007698193709062738\n",
            "batch 12 loss 0.00078084425377274\n",
            "batch 13 loss 0.0007157613914655219\n",
            "batch 14 loss 0.0007736534939278982\n",
            "batch 15 loss 0.0007737338568126192\n",
            "batch 16 loss 0.0006908755663341606\n",
            "batch 17 loss 0.0008002411111572199\n",
            "batch 18 loss 0.0007173855503108167\n",
            "batch 19 loss 0.0007739205066003943\n",
            "batch 20 loss 0.0008038338066679607\n",
            "batch 21 loss 0.0007251501990550047\n",
            "batch 22 loss 0.0007326004889534312\n",
            "batch 23 loss 0.0007720655550303327\n",
            "batch 24 loss 0.0007469605444383817\n",
            "batch 25 loss 0.0007868054345154215\n",
            "batch 26 loss 0.0007376799636094422\n",
            "batch 27 loss 0.0007569102133566901\n",
            "batch 28 loss 0.0006892777226440474\n",
            "batch 29 loss 0.0007814915042559828\n",
            "batch 30 loss 0.000774343496500599\n",
            "batch 31 loss 0.0006906963656727092\n",
            "batch 32 loss 0.0007515163702755505\n",
            "batch 33 loss 0.0007538436141757903\n",
            "batch 34 loss 0.0007610637495138502\n",
            "batch 35 loss 0.0007126130265437323\n",
            "batch 36 loss 0.0007735884302907678\n",
            "batch 37 loss 0.0007020303201316516\n",
            "batch 38 loss 0.0007063528433925139\n",
            "batch 39 loss 0.0007550641825051985\n",
            "batch 40 loss 0.0007035402244623586\n",
            "batch 41 loss 0.0007590318045912566\n",
            "batch 42 loss 0.000679609572463037\n",
            "batch 43 loss 0.0007200704698202074\n",
            "batch 44 loss 0.0007721828820364738\n",
            "batch 45 loss 0.0007357442235064353\n",
            "batch 46 loss 0.0006973322764300428\n",
            "batch 47 loss 0.0007165126714666436\n",
            "batch 48 loss 0.0008085791650450437\n",
            "batch 49 loss 0.0007497484717431547\n",
            "\n",
            "res: 0.00015466917185619824\n",
            "bc0: 0.003598987525039432\n",
            "bc1: 0.003277243239632011\n",
            "bc2: 0.0015448884458689417\n",
            "bc3: 0.00327233533854511\n",
            "\n",
            "\n",
            "Epoch 9\n",
            "batch 0 loss 0.0007728534810857025\n",
            "batch 1 loss 0.0007004331721696614\n",
            "batch 2 loss 0.0006823201401834185\n",
            "batch 3 loss 0.0007918309019662296\n",
            "batch 4 loss 0.0007092486012207378\n",
            "batch 5 loss 0.0007599664383148272\n",
            "batch 6 loss 0.0007118930173933522\n",
            "batch 7 loss 0.0007254869684307308\n",
            "batch 8 loss 0.0007294469921156977\n",
            "batch 9 loss 0.0007826791038707351\n",
            "batch 10 loss 0.0007332764165981074\n",
            "batch 11 loss 0.0007366224104518994\n",
            "batch 12 loss 0.0007248299186939188\n",
            "batch 13 loss 0.0007092728840779354\n",
            "batch 14 loss 0.0007156895611774897\n",
            "batch 15 loss 0.0006672598473122066\n",
            "batch 16 loss 0.0007023636149119121\n",
            "batch 17 loss 0.0007126965111547012\n",
            "batch 18 loss 0.0006880894619712079\n",
            "batch 19 loss 0.000688249212055257\n",
            "batch 20 loss 0.0007333798137637312\n",
            "batch 21 loss 0.0007326799116125531\n",
            "batch 22 loss 0.0007253723026559626\n",
            "batch 23 loss 0.0006638972584922887\n",
            "batch 24 loss 0.0006977344204558496\n",
            "batch 25 loss 0.0006889218719394253\n",
            "batch 26 loss 0.0007259778096529298\n",
            "batch 27 loss 0.0007163775507715517\n",
            "batch 28 loss 0.0007172973052859077\n",
            "batch 29 loss 0.0006748172009739037\n",
            "batch 30 loss 0.0006784361032423293\n",
            "batch 31 loss 0.0007761087831224368\n",
            "batch 32 loss 0.0006953749776214001\n",
            "batch 33 loss 0.0006926256060899033\n",
            "batch 34 loss 0.0007402849407695298\n",
            "batch 35 loss 0.0007081098816087763\n",
            "batch 36 loss 0.0006767693956506323\n",
            "batch 37 loss 0.0006731610225685723\n",
            "batch 38 loss 0.0007205392401309003\n",
            "batch 39 loss 0.0006674468017580032\n",
            "batch 40 loss 0.0006921918778054912\n",
            "batch 41 loss 0.0007013737642386007\n",
            "batch 42 loss 0.0006602269353722175\n",
            "batch 43 loss 0.0006753708883805126\n",
            "batch 44 loss 0.0007064178483281616\n",
            "batch 45 loss 0.0007280422774858107\n",
            "batch 46 loss 0.0006810462688811565\n",
            "batch 47 loss 0.0006855418910597524\n",
            "batch 48 loss 0.0006591809089130522\n",
            "batch 49 loss 0.0007189817252567092\n",
            "\n",
            "res: 0.000153203971378196\n",
            "bc0: 0.0035597002122859022\n",
            "bc1: 0.003090508555008854\n",
            "bc2: 0.001431216492176484\n",
            "bc3: 0.0030846004923488165\n",
            "\n",
            "\n",
            "Epoch 10\n",
            "batch 0 loss 0.0007056332904872774\n",
            "batch 1 loss 0.0006942033540054429\n",
            "batch 2 loss 0.0007269696380046195\n",
            "batch 3 loss 0.000687980648990387\n",
            "batch 4 loss 0.0007043020403422849\n",
            "batch 5 loss 0.0006947309415647096\n",
            "batch 6 loss 0.0006557532568368303\n",
            "batch 7 loss 0.0007514186349292702\n",
            "batch 8 loss 0.0006445772353887758\n",
            "batch 9 loss 0.0006450372561607945\n",
            "batch 10 loss 0.0006349557761027234\n",
            "batch 11 loss 0.0007034751176727183\n",
            "batch 12 loss 0.0007168247796378403\n",
            "batch 13 loss 0.0006486406988484347\n",
            "batch 14 loss 0.0006921998751175695\n",
            "batch 15 loss 0.0006404741984553104\n",
            "batch 16 loss 0.0006399274402806021\n",
            "batch 17 loss 0.0006861405376737654\n",
            "batch 18 loss 0.0006579649826549843\n",
            "batch 19 loss 0.0006345504620012586\n",
            "batch 20 loss 0.0007335025718838158\n",
            "batch 21 loss 0.0006372791147728098\n",
            "batch 22 loss 0.0006452004666938969\n",
            "batch 23 loss 0.0006725711384288937\n",
            "batch 24 loss 0.0006582703801000331\n",
            "batch 25 loss 0.0006862491982980521\n",
            "batch 26 loss 0.000692443395082847\n",
            "batch 27 loss 0.0007334212570381918\n",
            "batch 28 loss 0.0007152539178668565\n",
            "batch 29 loss 0.0006171626919377739\n",
            "batch 30 loss 0.0007133431277737433\n",
            "batch 31 loss 0.0006359228885589628\n",
            "batch 32 loss 0.0006397069159818458\n",
            "batch 33 loss 0.0006938502343162847\n",
            "batch 34 loss 0.0006564361389760448\n",
            "batch 35 loss 0.0006450468092503591\n",
            "batch 36 loss 0.0006712124470600654\n",
            "batch 37 loss 0.0006729333093252925\n",
            "batch 38 loss 0.0006553355285461773\n",
            "batch 39 loss 0.0006611100766775732\n",
            "batch 40 loss 0.0006959470891168938\n",
            "batch 41 loss 0.0006139546449604425\n",
            "batch 42 loss 0.0006483627812409727\n",
            "batch 43 loss 0.0006473545446347261\n",
            "batch 44 loss 0.0007333295367642023\n",
            "batch 45 loss 0.0006659154937441098\n",
            "batch 46 loss 0.0006801925023065117\n",
            "batch 47 loss 0.0006672632902844334\n",
            "batch 48 loss 0.000691491042812507\n",
            "batch 49 loss 0.0007084418599200245\n",
            "\n",
            "res: 0.0001498864324422276\n",
            "bc0: 0.003520852369968299\n",
            "bc1: 0.0029023160307873144\n",
            "bc2: 0.0013609111585105693\n",
            "bc3: 0.0028982020340143472\n",
            "\n",
            "\n",
            "Epoch 11\n",
            "batch 0 loss 0.000661859258901119\n",
            "batch 1 loss 0.0006693885938354277\n",
            "batch 2 loss 0.0006368448379489134\n",
            "batch 3 loss 0.0006238192862815829\n",
            "batch 4 loss 0.0006560438986653075\n",
            "batch 5 loss 0.0006292987412334584\n",
            "batch 6 loss 0.0006161068568460075\n",
            "batch 7 loss 0.0006611900379587904\n",
            "batch 8 loss 0.0006286653063509568\n",
            "batch 9 loss 0.0006810674522734175\n",
            "batch 10 loss 0.0006988376396506599\n",
            "batch 11 loss 0.0006326869875913187\n",
            "batch 12 loss 0.0006659507914009671\n",
            "batch 13 loss 0.000636412668354441\n",
            "batch 14 loss 0.0006210942908840414\n",
            "batch 15 loss 0.0006162453671427936\n",
            "batch 16 loss 0.0005984867099051513\n",
            "batch 17 loss 0.0006894867151479229\n",
            "batch 18 loss 0.0006221159066890095\n",
            "batch 19 loss 0.0006733691786072227\n",
            "batch 20 loss 0.0006411844226359731\n",
            "batch 21 loss 0.0006671931447518907\n",
            "batch 22 loss 0.0006973920684406338\n",
            "batch 23 loss 0.0006810251155152004\n",
            "batch 24 loss 0.0006729829574961549\n",
            "batch 25 loss 0.0006398516821095266\n",
            "batch 26 loss 0.0006725567344448591\n",
            "batch 27 loss 0.0006112505820432978\n",
            "batch 28 loss 0.0006195841334123144\n",
            "batch 29 loss 0.0006406736861016724\n",
            "batch 30 loss 0.0006495030065824536\n",
            "batch 31 loss 0.0006238723526959675\n",
            "batch 32 loss 0.0006745698342403104\n",
            "batch 33 loss 0.0006802546837379862\n",
            "batch 34 loss 0.0006402529470849428\n",
            "batch 35 loss 0.0006457914440995841\n",
            "batch 36 loss 0.000641736688965753\n",
            "batch 37 loss 0.0006191287473307463\n",
            "batch 38 loss 0.0005770578871228589\n",
            "batch 39 loss 0.0006629384704600538\n",
            "batch 40 loss 0.0006197293991088935\n",
            "batch 41 loss 0.0006358416989355807\n",
            "batch 42 loss 0.0006797780839194839\n",
            "batch 43 loss 0.0006241138818486248\n",
            "batch 44 loss 0.0006542384612974501\n",
            "batch 45 loss 0.0005623114576504101\n",
            "batch 46 loss 0.0007018447446826946\n",
            "batch 47 loss 0.0005985293874617928\n",
            "batch 48 loss 0.0005842133992574167\n",
            "batch 49 loss 0.0006419516480353651\n",
            "\n",
            "res: 0.00014823046049358517\n",
            "bc0: 0.0034788835662365354\n",
            "bc1: 0.002739828490039581\n",
            "bc2: 0.0012458592526512749\n",
            "bc3: 0.0027342273816569934\n",
            "\n",
            "\n",
            "Epoch 12\n",
            "batch 0 loss 0.000599385753704369\n",
            "batch 1 loss 0.0006669371685217388\n",
            "batch 2 loss 0.0006079035051641577\n",
            "batch 3 loss 0.0006943587187906476\n",
            "batch 4 loss 0.0006625916231883669\n",
            "batch 5 loss 0.0005942959411997382\n",
            "batch 6 loss 0.000622566695640482\n",
            "batch 7 loss 0.0006710674969901908\n",
            "batch 8 loss 0.0006126697814832818\n",
            "batch 9 loss 0.0006392274706616764\n",
            "batch 10 loss 0.0005909416098461817\n",
            "batch 11 loss 0.0006113965848597065\n",
            "batch 12 loss 0.0005750013939593534\n",
            "batch 13 loss 0.0006379618935870359\n",
            "batch 14 loss 0.0005933939281495808\n",
            "batch 15 loss 0.0006083242435005056\n",
            "batch 16 loss 0.0007011785636092256\n",
            "batch 17 loss 0.0005937551071168359\n",
            "batch 18 loss 0.0006620131602936459\n",
            "batch 19 loss 0.0006143193013858803\n",
            "batch 20 loss 0.0006108418618127308\n",
            "batch 21 loss 0.0006513786283590681\n",
            "batch 22 loss 0.0005638823138871034\n",
            "batch 23 loss 0.0006211722251188872\n",
            "batch 24 loss 0.0006104250181188987\n",
            "batch 25 loss 0.0006305650513782179\n",
            "batch 26 loss 0.0006273445793497974\n",
            "batch 27 loss 0.0005665297365563986\n",
            "batch 28 loss 0.000565443389082042\n",
            "batch 29 loss 0.0005916501648020492\n",
            "batch 30 loss 0.0005824952502111817\n",
            "batch 31 loss 0.0006113188959836982\n",
            "batch 32 loss 0.0005780122693489106\n",
            "batch 33 loss 0.0006107831948036912\n",
            "batch 34 loss 0.0006178397326214389\n",
            "batch 35 loss 0.0006373132962941739\n",
            "batch 36 loss 0.0006162088877328384\n",
            "batch 37 loss 0.0006232986731987953\n",
            "batch 38 loss 0.0005871005165498217\n",
            "batch 39 loss 0.0006211548603997236\n",
            "batch 40 loss 0.0006173914386057992\n",
            "batch 41 loss 0.0006241833089862322\n",
            "batch 42 loss 0.0006127677469703692\n",
            "batch 43 loss 0.0005953078069623299\n",
            "batch 44 loss 0.0006037224586089965\n",
            "batch 45 loss 0.0005835947814204195\n",
            "batch 46 loss 0.0005669899974309405\n",
            "batch 47 loss 0.0006059989518388869\n",
            "batch 48 loss 0.0005538382143315518\n",
            "batch 49 loss 0.0006262823660975121\n",
            "\n",
            "res: 0.00014630869015813788\n",
            "bc0: 0.0034418785764054895\n",
            "bc1: 0.0025673013616508552\n",
            "bc2: 0.001180224078095186\n",
            "bc3: 0.0025631716045420895\n",
            "\n",
            "\n",
            "Epoch 13\n",
            "batch 0 loss 0.0005738423554748958\n",
            "batch 1 loss 0.0006276305081355566\n",
            "batch 2 loss 0.0005975106933578576\n",
            "batch 3 loss 0.0005774909174026586\n",
            "batch 4 loss 0.0006227821552038053\n",
            "batch 5 loss 0.0005402337385255508\n",
            "batch 6 loss 0.0005698527046026116\n",
            "batch 7 loss 0.0005937134583239205\n",
            "batch 8 loss 0.0005967188094246212\n",
            "batch 9 loss 0.0005654116499385404\n",
            "batch 10 loss 0.0005918912368000193\n",
            "batch 11 loss 0.0005861162010718908\n",
            "batch 12 loss 0.0005957860126731939\n",
            "batch 13 loss 0.0005750271761112879\n",
            "batch 14 loss 0.000613733606812799\n",
            "batch 15 loss 0.0005734868231003777\n",
            "batch 16 loss 0.0005859919564517068\n",
            "batch 17 loss 0.0005697383009973053\n",
            "batch 18 loss 0.0006024627330847507\n",
            "batch 19 loss 0.000525780500553612\n",
            "batch 20 loss 0.0005657883510005834\n",
            "batch 21 loss 0.0006023496898565398\n",
            "batch 22 loss 0.0005786247888804698\n",
            "batch 23 loss 0.0005720329849928682\n",
            "batch 24 loss 0.0006298140079756414\n",
            "batch 25 loss 0.0006326827382447008\n",
            "batch 26 loss 0.0006775064482701236\n",
            "batch 27 loss 0.0005658202647995498\n",
            "batch 28 loss 0.0005328353257539006\n",
            "batch 29 loss 0.0005993927220026113\n",
            "batch 30 loss 0.0005806881355356264\n",
            "batch 31 loss 0.0005585282940616492\n",
            "batch 32 loss 0.0005522700446738636\n",
            "batch 33 loss 0.0006207386504426412\n",
            "batch 34 loss 0.0006045403516699606\n",
            "batch 35 loss 0.0005685295317600743\n",
            "batch 36 loss 0.000583807456208972\n",
            "batch 37 loss 0.0005516732754645689\n",
            "batch 38 loss 0.0006081229011859093\n",
            "batch 39 loss 0.0006138378836080125\n",
            "batch 40 loss 0.0005625788778595158\n",
            "batch 41 loss 0.0005678806883882405\n",
            "batch 42 loss 0.0005997610373303544\n",
            "batch 43 loss 0.0005463737863147811\n",
            "batch 44 loss 0.0005791222259974645\n",
            "batch 45 loss 0.0005907636248607841\n",
            "batch 46 loss 0.0005707444340334046\n",
            "batch 47 loss 0.0006160262717183785\n",
            "batch 48 loss 0.0005902551170275437\n",
            "batch 49 loss 0.0005783063039302967\n",
            "\n",
            "res: 0.00014367369592562337\n",
            "bc0: 0.003404987378355727\n",
            "bc1: 0.002416822016056597\n",
            "bc2: 0.0011013627351250377\n",
            "bc3: 0.0024132386129507334\n",
            "\n",
            "\n",
            "Epoch 14\n",
            "batch 0 loss 0.0005414873936713232\n",
            "batch 1 loss 0.0006031372131076596\n",
            "batch 2 loss 0.0005573017166626499\n",
            "batch 3 loss 0.00055313423538719\n",
            "batch 4 loss 0.0005157642955409898\n",
            "batch 5 loss 0.0006270772577562423\n",
            "batch 6 loss 0.0005422102079780237\n",
            "batch 7 loss 0.0005812416643924388\n",
            "batch 8 loss 0.0006548203103302836\n",
            "batch 9 loss 0.0005368791740273029\n",
            "batch 10 loss 0.0005781101491697282\n",
            "batch 11 loss 0.0005711301398734023\n",
            "batch 12 loss 0.000568174370623667\n",
            "batch 13 loss 0.0005870299048609738\n",
            "batch 14 loss 0.0005256825774802934\n",
            "batch 15 loss 0.0005856267689116345\n",
            "batch 16 loss 0.0006014345674871814\n",
            "batch 17 loss 0.0005046313403202303\n",
            "batch 18 loss 0.000597201874761783\n",
            "batch 19 loss 0.0005504599175040794\n",
            "batch 20 loss 0.0006269243664778406\n",
            "batch 21 loss 0.0005446224827328674\n",
            "batch 22 loss 0.0005581994427660149\n",
            "batch 23 loss 0.0005802499063864046\n",
            "batch 24 loss 0.0005451613600369313\n",
            "batch 25 loss 0.0005523062241784194\n",
            "batch 26 loss 0.0005989911100307353\n",
            "batch 27 loss 0.0005526896769416742\n",
            "batch 28 loss 0.0005508069267495992\n",
            "batch 29 loss 0.0005829422949099279\n",
            "batch 30 loss 0.0005565138030357746\n",
            "batch 31 loss 0.0005363152781108555\n",
            "batch 32 loss 0.0005534886779743248\n",
            "batch 33 loss 0.0005736877424618831\n",
            "batch 34 loss 0.0005545465206219967\n",
            "batch 35 loss 0.0005305645031457926\n",
            "batch 36 loss 0.0005065879735733924\n",
            "batch 37 loss 0.0005901584286983296\n",
            "batch 38 loss 0.0005576639121820938\n",
            "batch 39 loss 0.0005298451412839705\n",
            "batch 40 loss 0.0005679136254193247\n",
            "batch 41 loss 0.0005332917234547237\n",
            "batch 42 loss 0.0005306281736160207\n",
            "batch 43 loss 0.0005750515026584126\n",
            "batch 44 loss 0.000517464739465531\n",
            "batch 45 loss 0.0005410076592595103\n",
            "batch 46 loss 0.0005360407415563725\n",
            "batch 47 loss 0.0005251433193322007\n",
            "batch 48 loss 0.0005414990362104853\n",
            "batch 49 loss 0.0005473843437766021\n",
            "\n",
            "res: 0.00014188316976063912\n",
            "bc0: 0.003371771495454666\n",
            "bc1: 0.002272765184166521\n",
            "bc2: 0.001019543466485959\n",
            "bc3: 0.0022678079866591473\n",
            "\n",
            "\n",
            "Epoch 15\n",
            "batch 0 loss 0.0006041424567318992\n",
            "batch 1 loss 0.0005472687370586599\n",
            "batch 2 loss 0.0005401947067650584\n",
            "batch 3 loss 0.0005154331948586854\n",
            "batch 4 loss 0.0005690768433227341\n",
            "batch 5 loss 0.0005367408286735045\n",
            "batch 6 loss 0.0005288270551523543\n",
            "batch 7 loss 0.0005546846923352888\n",
            "batch 8 loss 0.000526888175736424\n",
            "batch 9 loss 0.0006472556082454089\n",
            "batch 10 loss 0.0005366003682969065\n",
            "batch 11 loss 0.000523054303835952\n",
            "batch 12 loss 0.000512117182520644\n",
            "batch 13 loss 0.0005690700437187921\n",
            "batch 14 loss 0.0005593071834725957\n",
            "batch 15 loss 0.0005279897149377101\n",
            "batch 16 loss 0.000542248836414846\n",
            "batch 17 loss 0.0005452450316349316\n",
            "batch 18 loss 0.0005297540486505762\n",
            "batch 19 loss 0.0005249623632927083\n",
            "batch 20 loss 0.0005587823722425189\n",
            "batch 21 loss 0.0004644535187726725\n",
            "batch 22 loss 0.000545428398077952\n",
            "batch 23 loss 0.0005512256194306252\n",
            "batch 24 loss 0.0005279999442605989\n",
            "batch 25 loss 0.0005553773412166917\n",
            "batch 26 loss 0.0005412649531463965\n",
            "batch 27 loss 0.0005700220630067784\n",
            "batch 28 loss 0.0005163964847794175\n",
            "batch 29 loss 0.0005343361347869244\n",
            "batch 30 loss 0.0005160307575431467\n",
            "batch 31 loss 0.0005323174247836146\n",
            "batch 32 loss 0.0005095366510712025\n",
            "batch 33 loss 0.0005551032962822006\n",
            "batch 34 loss 0.0004840675087490192\n",
            "batch 35 loss 0.0005295793544614197\n",
            "batch 36 loss 0.000501265395116777\n",
            "batch 37 loss 0.0005837161064001085\n",
            "batch 38 loss 0.0005524753172875626\n",
            "batch 39 loss 0.0005045300894138978\n",
            "batch 40 loss 0.0005266592487496672\n",
            "batch 41 loss 0.0005321065874338771\n",
            "batch 42 loss 0.0004953868693639382\n",
            "batch 43 loss 0.0005210088457580251\n",
            "batch 44 loss 0.0005364131412778218\n",
            "batch 45 loss 0.0005033268581427405\n",
            "batch 46 loss 0.0005440165609115682\n",
            "batch 47 loss 0.00048314014308541316\n",
            "batch 48 loss 0.0005117775735388244\n",
            "batch 49 loss 0.0005345511321321766\n",
            "\n",
            "res: 0.00013916370533209138\n",
            "bc0: 0.0033388741115361266\n",
            "bc1: 0.002135867713421736\n",
            "bc2: 0.0009551717007284344\n",
            "bc3: 0.0021336707008860816\n",
            "\n",
            "\n",
            "Epoch 16\n",
            "batch 0 loss 0.0004964642829927952\n",
            "batch 1 loss 0.0004945460403958476\n",
            "batch 2 loss 0.0005031448560313969\n",
            "batch 3 loss 0.0005258543731305006\n",
            "batch 4 loss 0.0005897600429154311\n",
            "batch 5 loss 0.0005547475450401579\n",
            "batch 6 loss 0.0005136198226997423\n",
            "batch 7 loss 0.000473738937746619\n",
            "batch 8 loss 0.0005380527320730274\n",
            "batch 9 loss 0.0005222655020339632\n",
            "batch 10 loss 0.0005063674609160114\n",
            "batch 11 loss 0.0005057147206388014\n",
            "batch 12 loss 0.0005707341207241356\n",
            "batch 13 loss 0.0005584173870783775\n",
            "batch 14 loss 0.000499181744386765\n",
            "batch 15 loss 0.0004936220003033848\n",
            "batch 16 loss 0.0005133754768510159\n",
            "batch 17 loss 0.000575374440187698\n",
            "batch 18 loss 0.0004794928391590447\n",
            "batch 19 loss 0.0005684843410901894\n",
            "batch 20 loss 0.0005140874520595342\n",
            "batch 21 loss 0.00048285009710833824\n",
            "batch 22 loss 0.0005219929390977984\n",
            "batch 23 loss 0.0005129156856041361\n",
            "batch 24 loss 0.0005362492628909923\n",
            "batch 25 loss 0.0005104228386188292\n",
            "batch 26 loss 0.0004900249049369894\n",
            "batch 27 loss 0.0005175109872609277\n",
            "batch 28 loss 0.00048764764216512914\n",
            "batch 29 loss 0.0005409262500624761\n",
            "batch 30 loss 0.0005744411454476996\n",
            "batch 31 loss 0.0005084111531264537\n",
            "batch 32 loss 0.000553463156967791\n",
            "batch 33 loss 0.0005240220958164555\n",
            "batch 34 loss 0.00044590920686656324\n",
            "batch 35 loss 0.000544168454638859\n",
            "batch 36 loss 0.0004926317965312365\n",
            "batch 37 loss 0.00046436386552197026\n",
            "batch 38 loss 0.00047671027842212654\n",
            "batch 39 loss 0.0004855725579965591\n",
            "batch 40 loss 0.0005323044210906008\n",
            "batch 41 loss 0.00043736112395161876\n",
            "batch 42 loss 0.0004924217395331126\n",
            "batch 43 loss 0.0005030088231275979\n",
            "batch 44 loss 0.0005256022851963856\n",
            "batch 45 loss 0.00047499741624450144\n",
            "batch 46 loss 0.00048744479613817146\n",
            "batch 47 loss 0.0005145863810085298\n",
            "batch 48 loss 0.0004889867173779261\n",
            "batch 49 loss 0.0004963055227044565\n",
            "\n",
            "res: 0.00013794564420588877\n",
            "bc0: 0.003306882178052828\n",
            "bc1: 0.002020039748769798\n",
            "bc2: 0.0008494805191275453\n",
            "bc3: 0.0020163472528438274\n",
            "\n",
            "\n",
            "Epoch 17\n",
            "batch 0 loss 0.00048718302734799954\n",
            "batch 1 loss 0.0004640932919649317\n",
            "batch 2 loss 0.0005178458260182093\n",
            "batch 3 loss 0.0005386386602749495\n",
            "batch 4 loss 0.0005033242838732757\n",
            "batch 5 loss 0.00048234312537441515\n",
            "batch 6 loss 0.0004762003679374316\n",
            "batch 7 loss 0.0004579491788572678\n",
            "batch 8 loss 0.0005180563756239458\n",
            "batch 9 loss 0.0005342266584943113\n",
            "batch 10 loss 0.0004858731504234556\n",
            "batch 11 loss 0.0005022106262093318\n",
            "batch 12 loss 0.0005347739432107685\n",
            "batch 13 loss 0.00045282005184877255\n",
            "batch 14 loss 0.0005153955096292171\n",
            "batch 15 loss 0.00047965091128591975\n",
            "batch 16 loss 0.0005184833793485593\n",
            "batch 17 loss 0.0004637035925883971\n",
            "batch 18 loss 0.0004949906577499993\n",
            "batch 19 loss 0.000473914424935924\n",
            "batch 20 loss 0.0004730300320630066\n",
            "batch 21 loss 0.00044754907739947864\n",
            "batch 22 loss 0.0005240579832240792\n",
            "batch 23 loss 0.00047559608162239906\n",
            "batch 24 loss 0.00048786450999403926\n",
            "batch 25 loss 0.0004699006009454342\n",
            "batch 26 loss 0.000490432412543506\n",
            "batch 27 loss 0.0004742509137081457\n",
            "batch 28 loss 0.0005138329770457964\n",
            "batch 29 loss 0.0004502986110084105\n",
            "batch 30 loss 0.0004962991724917246\n",
            "batch 31 loss 0.0004223786359112824\n",
            "batch 32 loss 0.0005165263479264123\n",
            "batch 33 loss 0.00047808581489747226\n",
            "batch 34 loss 0.0004911148049821013\n",
            "batch 35 loss 0.0005288037269917792\n",
            "batch 36 loss 0.0004370177222959281\n",
            "batch 37 loss 0.0004861356692604879\n",
            "batch 38 loss 0.0005035341142088594\n",
            "batch 39 loss 0.0004973395375184681\n",
            "batch 40 loss 0.0004560812156405234\n",
            "batch 41 loss 0.000514725519549969\n",
            "batch 42 loss 0.0005057122837377442\n",
            "batch 43 loss 0.000466232999426148\n",
            "batch 44 loss 0.0005091907769117477\n",
            "batch 45 loss 0.0005797214003145162\n",
            "batch 46 loss 0.0004499286413083544\n",
            "batch 47 loss 0.0005168456674524135\n",
            "batch 48 loss 0.00053398712537699\n",
            "batch 49 loss 0.00045731758465985265\n",
            "\n",
            "res: 0.00013586586521851314\n",
            "bc0: 0.0032674046656905945\n",
            "bc1: 0.0018803968921548416\n",
            "bc2: 0.0008260443706242146\n",
            "bc3: 0.0018777040461412536\n",
            "\n",
            "\n",
            "Epoch 18\n",
            "batch 0 loss 0.000514800847405351\n",
            "batch 1 loss 0.0004788207001750846\n",
            "batch 2 loss 0.00048487515294901295\n",
            "batch 3 loss 0.00047871329709882576\n",
            "batch 4 loss 0.0004452681946804613\n",
            "batch 5 loss 0.0004885615767706215\n",
            "batch 6 loss 0.0004359789696783794\n",
            "batch 7 loss 0.0005010551136299322\n",
            "batch 8 loss 0.0004593306551324043\n",
            "batch 9 loss 0.000464638488742924\n",
            "batch 10 loss 0.0004581743071091976\n",
            "batch 11 loss 0.0004674281960623613\n",
            "batch 12 loss 0.0004729648448372396\n",
            "batch 13 loss 0.0004815307927390994\n",
            "batch 14 loss 0.0004777246169807674\n",
            "batch 15 loss 0.00045470590902695843\n",
            "batch 16 loss 0.0004306529842742484\n",
            "batch 17 loss 0.0005048152626282655\n",
            "batch 18 loss 0.00046171789201303695\n",
            "batch 19 loss 0.000476151292851845\n",
            "batch 20 loss 0.0004682385391253706\n",
            "batch 21 loss 0.0004935979681309178\n",
            "batch 22 loss 0.00046129260279048937\n",
            "batch 23 loss 0.000476068029613925\n",
            "batch 24 loss 0.00044844910616976673\n",
            "batch 25 loss 0.00048235243564112606\n",
            "batch 26 loss 0.00048090318846788477\n",
            "batch 27 loss 0.0004756621971190972\n",
            "batch 28 loss 0.0004615604995199865\n",
            "batch 29 loss 0.0004835189451509792\n",
            "batch 30 loss 0.0005042417400750372\n",
            "batch 31 loss 0.00046570559427344346\n",
            "batch 32 loss 0.00043512805137863167\n",
            "batch 33 loss 0.00045844568615344876\n",
            "batch 34 loss 0.0005238215839084811\n",
            "batch 35 loss 0.0004725311098150615\n",
            "batch 36 loss 0.0004885842129431726\n",
            "batch 37 loss 0.00045325121200548234\n",
            "batch 38 loss 0.0005170671088461298\n",
            "batch 39 loss 0.0003932127282851134\n",
            "batch 40 loss 0.0004332321379186467\n",
            "batch 41 loss 0.0004741432755855679\n",
            "batch 42 loss 0.0004938199735996715\n",
            "batch 43 loss 0.0004614277258396342\n",
            "batch 44 loss 0.0004482448486913019\n",
            "batch 45 loss 0.0004349517191984505\n",
            "batch 46 loss 0.0004735447556196379\n",
            "batch 47 loss 0.0004797149407754769\n",
            "batch 48 loss 0.00048555267081544143\n",
            "batch 49 loss 0.0004891365107926975\n",
            "\n",
            "res: 0.00013359146770417938\n",
            "bc0: 0.003237012185606962\n",
            "bc1: 0.0017635658529869293\n",
            "bc2: 0.0007682072797949138\n",
            "bc3: 0.001759972836764071\n",
            "\n",
            "\n",
            "Epoch 19\n",
            "batch 0 loss 0.00043483156782936285\n",
            "batch 1 loss 0.0004486872026454258\n",
            "batch 2 loss 0.0003964505257154384\n",
            "batch 3 loss 0.000529777933224868\n",
            "batch 4 loss 0.0004581049235601032\n",
            "batch 5 loss 0.00043989901123716826\n",
            "batch 6 loss 0.00042540606339980827\n",
            "batch 7 loss 0.00045614488516512025\n",
            "batch 8 loss 0.000471005906728139\n",
            "batch 9 loss 0.0004190499891082791\n",
            "batch 10 loss 0.000450773646623188\n",
            "batch 11 loss 0.00047181707382621725\n",
            "batch 12 loss 0.0004976371318928232\n",
            "batch 13 loss 0.00044311526759114445\n",
            "batch 14 loss 0.00043433898246055555\n",
            "batch 15 loss 0.0004406718412629663\n",
            "batch 16 loss 0.00045969168470671036\n",
            "batch 17 loss 0.0004261256657363941\n",
            "batch 18 loss 0.00045835225304675285\n",
            "batch 19 loss 0.0004116326113819957\n",
            "batch 20 loss 0.0004709483623574514\n",
            "batch 21 loss 0.000482945503843578\n",
            "batch 22 loss 0.00044704484982967013\n",
            "batch 23 loss 0.0004376089226469309\n",
            "batch 24 loss 0.00047384465222770523\n",
            "batch 25 loss 0.00042009531460201235\n",
            "batch 26 loss 0.0004636920550085439\n",
            "batch 27 loss 0.0004585132571265487\n",
            "batch 28 loss 0.0004909725255542141\n",
            "batch 29 loss 0.0004917283483344851\n",
            "batch 30 loss 0.0004327911694334605\n",
            "batch 31 loss 0.0004691905658422464\n",
            "batch 32 loss 0.00042728362713131144\n",
            "batch 33 loss 0.0004886728553412463\n",
            "batch 34 loss 0.0004042911683164642\n",
            "batch 35 loss 0.00047405512509596926\n",
            "batch 36 loss 0.0004841387179108541\n",
            "batch 37 loss 0.0004179858805040514\n",
            "batch 38 loss 0.00047925667987345664\n",
            "batch 39 loss 0.00042866269013883574\n",
            "batch 40 loss 0.0004588946169517092\n",
            "batch 41 loss 0.00042815268025679653\n",
            "batch 42 loss 0.0004504538304509388\n",
            "batch 43 loss 0.0004587454898694932\n",
            "batch 44 loss 0.00045591074147589936\n",
            "batch 45 loss 0.000485188231742866\n",
            "batch 46 loss 0.00041902660033441897\n",
            "batch 47 loss 0.0004173119007855559\n",
            "batch 48 loss 0.00047718249705969325\n",
            "batch 49 loss 0.0004395146988977811\n",
            "\n",
            "res: 0.00013195578634105695\n",
            "bc0: 0.0032013779688278843\n",
            "bc1: 0.0016486015250785923\n",
            "bc2: 0.0007182109559133628\n",
            "bc3: 0.001645721188578139\n",
            "\n",
            "\n",
            "Epoch 20\n",
            "batch 0 loss 0.00037736851979961166\n",
            "batch 1 loss 0.00045584946604093607\n",
            "batch 2 loss 0.00046417180612210715\n",
            "batch 3 loss 0.0004538054961645622\n",
            "batch 4 loss 0.000434679043516221\n",
            "batch 5 loss 0.00043528488144636623\n",
            "batch 6 loss 0.0004493023779869587\n",
            "batch 7 loss 0.00042123877547955215\n",
            "batch 8 loss 0.0004553450896674139\n",
            "batch 9 loss 0.0003926316941874609\n",
            "batch 10 loss 0.00047109514675275094\n",
            "batch 11 loss 0.0004668559460622642\n",
            "batch 12 loss 0.00045454454088198544\n",
            "batch 13 loss 0.00046354493906916106\n",
            "batch 14 loss 0.00043766104109378684\n",
            "batch 15 loss 0.00042896963496909344\n",
            "batch 16 loss 0.0005106365385265108\n",
            "batch 17 loss 0.0004233585599648586\n",
            "batch 18 loss 0.0004266212503473199\n",
            "batch 19 loss 0.00045063495680846763\n",
            "batch 20 loss 0.00045345647917961496\n",
            "batch 21 loss 0.0004337587072818735\n",
            "batch 22 loss 0.00041229645785221907\n",
            "batch 23 loss 0.00043533980625827106\n",
            "batch 24 loss 0.00040220809162330577\n",
            "batch 25 loss 0.00045855827908121503\n",
            "batch 26 loss 0.000392325161875027\n",
            "batch 27 loss 0.00046745704021245187\n",
            "batch 28 loss 0.00039814108475974113\n",
            "batch 29 loss 0.00047225177212197235\n",
            "batch 30 loss 0.00046384764051141954\n",
            "batch 31 loss 0.00039499932481987146\n",
            "batch 32 loss 0.00044011386526943163\n",
            "batch 33 loss 0.0004213087712313547\n",
            "batch 34 loss 0.00045815805213987685\n",
            "batch 35 loss 0.0004078133262114899\n",
            "batch 36 loss 0.00041852367163953594\n",
            "batch 37 loss 0.00040114942626129693\n",
            "batch 38 loss 0.000427555349260042\n",
            "batch 39 loss 0.0003866225490501533\n",
            "batch 40 loss 0.00041752414894557366\n",
            "batch 41 loss 0.0004376815711158364\n",
            "batch 42 loss 0.00044183714872876963\n",
            "batch 43 loss 0.0003954840856795857\n",
            "batch 44 loss 0.0004140944542795538\n",
            "batch 45 loss 0.0004527403711784897\n",
            "batch 46 loss 0.0004195120135034464\n",
            "batch 47 loss 0.00044719733902595014\n",
            "batch 48 loss 0.00038112240644294713\n",
            "batch 49 loss 0.00048028915762321066\n",
            "\n",
            "res: 0.00012947917674447254\n",
            "bc0: 0.0031794693242134437\n",
            "bc1: 0.001548794424576982\n",
            "bc2: 0.0006545811799036073\n",
            "bc3: 0.0015470969552424546\n",
            "\n",
            "\n",
            "Epoch 21\n",
            "batch 0 loss 0.0004225280401623262\n",
            "batch 1 loss 0.0004131458712864101\n",
            "batch 2 loss 0.0004022413928737391\n",
            "batch 3 loss 0.00044180543800413283\n",
            "batch 4 loss 0.00043817497975230337\n",
            "batch 5 loss 0.00041748890984208187\n",
            "batch 6 loss 0.00042383325981244725\n",
            "batch 7 loss 0.00041056089457157315\n",
            "batch 8 loss 0.0004454903502058339\n",
            "batch 9 loss 0.00047654509151588925\n",
            "batch 10 loss 0.00039961614190953663\n",
            "batch 11 loss 0.0004501510719061722\n",
            "batch 12 loss 0.00039559858608004885\n",
            "batch 13 loss 0.0004136099294644487\n",
            "batch 14 loss 0.0004023337437875991\n",
            "batch 15 loss 0.0004220151360342481\n",
            "batch 16 loss 0.00041282673870067174\n",
            "batch 17 loss 0.0004567583386116529\n",
            "batch 18 loss 0.000435429878102355\n",
            "batch 19 loss 0.00041524926970975623\n",
            "batch 20 loss 0.00041810674006250077\n",
            "batch 21 loss 0.0004225155909701556\n",
            "batch 22 loss 0.0004252720179902025\n",
            "batch 23 loss 0.0004184130060369527\n",
            "batch 24 loss 0.0003904386320458641\n",
            "batch 25 loss 0.00041302573723508225\n",
            "batch 26 loss 0.0004443370934578392\n",
            "batch 27 loss 0.0004086194873642823\n",
            "batch 28 loss 0.0004160321573895395\n",
            "batch 29 loss 0.0004348818557120623\n",
            "batch 30 loss 0.00039690752327092274\n",
            "batch 31 loss 0.00038953660144313705\n",
            "batch 32 loss 0.0003939008591398053\n",
            "batch 33 loss 0.0004242934939567006\n",
            "batch 34 loss 0.00037798347952940455\n",
            "batch 35 loss 0.00039877774601752684\n",
            "batch 36 loss 0.0004210423149551857\n",
            "batch 37 loss 0.0004390060057503706\n",
            "batch 38 loss 0.00039713723846347834\n",
            "batch 39 loss 0.0003687930393328643\n",
            "batch 40 loss 0.00044024683992025285\n",
            "batch 41 loss 0.0004350323187770527\n",
            "batch 42 loss 0.0004119986522105994\n",
            "batch 43 loss 0.0004016865351546206\n",
            "batch 44 loss 0.00039084726028019317\n",
            "batch 45 loss 0.00043343038553154387\n",
            "batch 46 loss 0.0003917068998081405\n",
            "batch 47 loss 0.0004305227296029565\n",
            "batch 48 loss 0.000396192714531517\n",
            "batch 49 loss 0.0004233493668752556\n",
            "\n",
            "res: 0.0001278519314438692\n",
            "bc0: 0.0031463580009887028\n",
            "bc1: 0.001451080630660021\n",
            "bc2: 0.0005987981861318299\n",
            "bc3: 0.0014501192485071362\n",
            "\n",
            "\n",
            "Epoch 22\n",
            "batch 0 loss 0.000400150925039784\n",
            "batch 1 loss 0.0003973238943600773\n",
            "batch 2 loss 0.0003838867140278708\n",
            "batch 3 loss 0.00042503958541393726\n",
            "batch 4 loss 0.0003767642091196941\n",
            "batch 5 loss 0.00041421578454037814\n",
            "batch 6 loss 0.00040845400093856324\n",
            "batch 7 loss 0.0004338738699409411\n",
            "batch 8 loss 0.00038532331446163444\n",
            "batch 9 loss 0.00043582766262763805\n",
            "batch 10 loss 0.0003713444962701149\n",
            "batch 11 loss 0.000441717006211195\n",
            "batch 12 loss 0.0004494876041970898\n",
            "batch 13 loss 0.00040752388278972323\n",
            "batch 14 loss 0.00040582752441080524\n",
            "batch 15 loss 0.0003798035820703451\n",
            "batch 16 loss 0.000425090712551814\n",
            "batch 17 loss 0.00041881348797159\n",
            "batch 18 loss 0.00041906403651559\n",
            "batch 19 loss 0.0003773573291319568\n",
            "batch 20 loss 0.00038136693997996464\n",
            "batch 21 loss 0.0003855600475329216\n",
            "batch 22 loss 0.0003868662258438577\n",
            "batch 23 loss 0.00037310820616656833\n",
            "batch 24 loss 0.00038278700529486946\n",
            "batch 25 loss 0.0004063479956682173\n",
            "batch 26 loss 0.000442846780874957\n",
            "batch 27 loss 0.0003782302511977618\n",
            "batch 28 loss 0.00036517536914730693\n",
            "batch 29 loss 0.0003760169634778153\n",
            "batch 30 loss 0.0003887271674202227\n",
            "batch 31 loss 0.0004499797924733954\n",
            "batch 32 loss 0.0004078754857133249\n",
            "batch 33 loss 0.00038271782190019957\n",
            "batch 34 loss 0.00042397880461505583\n",
            "batch 35 loss 0.0003776992768258743\n",
            "batch 36 loss 0.00037489821740544497\n",
            "batch 37 loss 0.0004600151921815093\n",
            "batch 38 loss 0.0004004644478835253\n",
            "batch 39 loss 0.0003757992367903786\n",
            "batch 40 loss 0.00040623201532637816\n",
            "batch 41 loss 0.0004050444681315331\n",
            "batch 42 loss 0.0003640015886476811\n",
            "batch 43 loss 0.00042420693778586193\n",
            "batch 44 loss 0.00038632758298179625\n",
            "batch 45 loss 0.0004547041759067318\n",
            "batch 46 loss 0.0003489909323984995\n",
            "batch 47 loss 0.0003751816533910821\n",
            "batch 48 loss 0.00040179300126741153\n",
            "batch 49 loss 0.0004252610245815913\n",
            "\n",
            "res: 0.00012603787787441825\n",
            "bc0: 0.0031147320204070314\n",
            "bc1: 0.0013499142703025534\n",
            "bc2: 0.0005697382918926564\n",
            "bc3: 0.0013490573693818262\n",
            "\n",
            "\n",
            "Epoch 23\n",
            "batch 0 loss 0.00040906678360620415\n",
            "batch 1 loss 0.0003580247339372086\n",
            "batch 2 loss 0.0003788195009181709\n",
            "batch 3 loss 0.0004213640530060432\n",
            "batch 4 loss 0.0003731878104770594\n",
            "batch 5 loss 0.00037052648646743305\n",
            "batch 6 loss 0.00042436424004668917\n",
            "batch 7 loss 0.00033132595357265946\n",
            "batch 8 loss 0.00037796484481847494\n",
            "batch 9 loss 0.00036604637926144037\n",
            "batch 10 loss 0.0003743403506052761\n",
            "batch 11 loss 0.0003937842500044278\n",
            "batch 12 loss 0.00046452593364069885\n",
            "batch 13 loss 0.0003751128925579639\n",
            "batch 14 loss 0.0003801379938127294\n",
            "batch 15 loss 0.000404190336723949\n",
            "batch 16 loss 0.0004186539521339579\n",
            "batch 17 loss 0.00038642166428309974\n",
            "batch 18 loss 0.00038621650495755\n",
            "batch 19 loss 0.00035204534328559287\n",
            "batch 20 loss 0.00042364770068696616\n",
            "batch 21 loss 0.00040352949333507696\n",
            "batch 22 loss 0.00035238340300808296\n",
            "batch 23 loss 0.00038524575625699744\n",
            "batch 24 loss 0.000386987249025439\n",
            "batch 25 loss 0.0003631397397922109\n",
            "batch 26 loss 0.0004005437192890866\n",
            "batch 27 loss 0.00036885672592724294\n",
            "batch 28 loss 0.00043543500695077293\n",
            "batch 29 loss 0.00035276729931482654\n",
            "batch 30 loss 0.00039581205634304665\n",
            "batch 31 loss 0.0004144677497937463\n",
            "batch 32 loss 0.0003772231737991988\n",
            "batch 33 loss 0.0003724544370053298\n",
            "batch 34 loss 0.0004007231846471313\n",
            "batch 35 loss 0.0003671289323585666\n",
            "batch 36 loss 0.00038552965600754696\n",
            "batch 37 loss 0.00035982830735633984\n",
            "batch 38 loss 0.00035785564847235483\n",
            "batch 39 loss 0.0004318504094664587\n",
            "batch 40 loss 0.0004318626192565693\n",
            "batch 41 loss 0.0003296561533613534\n",
            "batch 42 loss 0.00039633891945504285\n",
            "batch 43 loss 0.0003932937930042458\n",
            "batch 44 loss 0.0003992967658352833\n",
            "batch 45 loss 0.00038469938452886313\n",
            "batch 46 loss 0.0003805041345538262\n",
            "batch 47 loss 0.0003791237346835637\n",
            "batch 48 loss 0.000373334907753889\n",
            "batch 49 loss 0.00038758397922582704\n",
            "\n",
            "res: 0.0001241751205077476\n",
            "bc0: 0.003086409905958173\n",
            "bc1: 0.0012625796718634614\n",
            "bc2: 0.0005232384128353436\n",
            "bc3: 0.0012614005383284666\n",
            "\n",
            "\n",
            "Epoch 24\n",
            "batch 0 loss 0.00036654261943471405\n",
            "batch 1 loss 0.00040928258524173644\n",
            "batch 2 loss 0.000372212745279877\n",
            "batch 3 loss 0.0003502909202101865\n",
            "batch 4 loss 0.00033821344991886494\n",
            "batch 5 loss 0.0003921443044818705\n",
            "batch 6 loss 0.0003573369671485037\n",
            "batch 7 loss 0.00031261303369741795\n",
            "batch 8 loss 0.0003847303396593789\n",
            "batch 9 loss 0.0003565728756324687\n",
            "batch 10 loss 0.00042625589330188906\n",
            "batch 11 loss 0.0003875747412752188\n",
            "batch 12 loss 0.00039834410974141763\n",
            "batch 13 loss 0.0003714948626143338\n",
            "batch 14 loss 0.00037181183145219626\n",
            "batch 15 loss 0.0003883916901860173\n",
            "batch 16 loss 0.00037335208443300266\n",
            "batch 17 loss 0.00038270579202312905\n",
            "batch 18 loss 0.00038284871618343756\n",
            "batch 19 loss 0.00038532568783623193\n",
            "batch 20 loss 0.00037036523455549904\n",
            "batch 21 loss 0.0003484949512339728\n",
            "batch 22 loss 0.00036625650120179727\n",
            "batch 23 loss 0.0003558904117153243\n",
            "batch 24 loss 0.00035347782067688663\n",
            "batch 25 loss 0.00038986382885074314\n",
            "batch 26 loss 0.00035892524477752633\n",
            "batch 27 loss 0.00040881564686262014\n",
            "batch 28 loss 0.0003889552123364386\n",
            "batch 29 loss 0.00041025715028443217\n",
            "batch 30 loss 0.00037377961202604126\n",
            "batch 31 loss 0.0003637690293234932\n",
            "batch 32 loss 0.00033917084628614237\n",
            "batch 33 loss 0.0003894529703050068\n",
            "batch 34 loss 0.00037259962397526923\n",
            "batch 35 loss 0.00036497491063102415\n",
            "batch 36 loss 0.00035245450706185223\n",
            "batch 37 loss 0.00038628794738494954\n",
            "batch 38 loss 0.00036612546402940466\n",
            "batch 39 loss 0.000407331777158257\n",
            "batch 40 loss 0.0004049190569884796\n",
            "batch 41 loss 0.0003263544780947862\n",
            "batch 42 loss 0.0003509213272959898\n",
            "batch 43 loss 0.0004066008431569105\n",
            "batch 44 loss 0.0003745258361545372\n",
            "batch 45 loss 0.00035548551050545546\n",
            "batch 46 loss 0.0003505920663343928\n",
            "batch 47 loss 0.0003551372477249551\n",
            "batch 48 loss 0.0003349762337706882\n",
            "batch 49 loss 0.00039246879421027473\n",
            "\n",
            "res: 0.00012324595827990724\n",
            "bc0: 0.0030533761291236874\n",
            "bc1: 0.0011679482324993766\n",
            "bc2: 0.0005022955238838007\n",
            "bc3: 0.0011660814352205603\n",
            "\n",
            "\n",
            "Epoch 25\n",
            "batch 0 loss 0.00036664858505340776\n",
            "batch 1 loss 0.00037998489496032355\n",
            "batch 2 loss 0.0003558233117832335\n",
            "batch 3 loss 0.00038564355111236226\n",
            "batch 4 loss 0.00037324438611064395\n",
            "batch 5 loss 0.0003838628421570102\n",
            "batch 6 loss 0.00034944080671986464\n",
            "batch 7 loss 0.0003624443615091852\n",
            "batch 8 loss 0.00039911983251241634\n",
            "batch 9 loss 0.0003929789758270358\n",
            "batch 10 loss 0.0003776880507901261\n",
            "batch 11 loss 0.00033217562696029416\n",
            "batch 12 loss 0.0003909808868192618\n",
            "batch 13 loss 0.0003452209411089787\n",
            "batch 14 loss 0.00033720890093039265\n",
            "batch 15 loss 0.00038392904036242274\n",
            "batch 16 loss 0.0003405769104606435\n",
            "batch 17 loss 0.00037962535422938077\n",
            "batch 18 loss 0.0003526805194736878\n",
            "batch 19 loss 0.00036680077444147674\n",
            "batch 20 loss 0.00038054923218756414\n",
            "batch 21 loss 0.00036565648615540696\n",
            "batch 22 loss 0.0003855286645745369\n",
            "batch 23 loss 0.00033092521308081903\n",
            "batch 24 loss 0.00034734516959337275\n",
            "batch 25 loss 0.00033834077582762406\n",
            "batch 26 loss 0.00032594313427048734\n",
            "batch 27 loss 0.0003478690737073433\n",
            "batch 28 loss 0.0003508135843432966\n",
            "batch 29 loss 0.0003846069444503601\n",
            "batch 30 loss 0.0003625568734052548\n",
            "batch 31 loss 0.00033395168753181927\n",
            "batch 32 loss 0.0003506238768824729\n",
            "batch 33 loss 0.00031457729945917406\n",
            "batch 34 loss 0.0004102427108109266\n",
            "batch 35 loss 0.0003399258663046633\n",
            "batch 36 loss 0.00037993640595566247\n",
            "batch 37 loss 0.000409181643446429\n",
            "batch 38 loss 0.0003932185807443802\n",
            "batch 39 loss 0.0003447046506188438\n",
            "batch 40 loss 0.00033029931710820313\n",
            "batch 41 loss 0.00034657744169793705\n",
            "batch 42 loss 0.00036349263486058346\n",
            "batch 43 loss 0.0003187162071776925\n",
            "batch 44 loss 0.00034639466935162185\n",
            "batch 45 loss 0.0003168834586360996\n",
            "batch 46 loss 0.0003491677302811702\n",
            "batch 47 loss 0.00033838021265601226\n",
            "batch 48 loss 0.0003520347612329759\n",
            "batch 49 loss 0.0003519792600452216\n",
            "\n",
            "res: 0.0001212381893329327\n",
            "bc0: 0.0030330269499627472\n",
            "bc1: 0.0011013604773738976\n",
            "bc2: 0.00043262575342854943\n",
            "bc3: 0.001100960410143641\n",
            "\n",
            "\n",
            "Epoch 26\n",
            "batch 0 loss 0.000357668114202712\n",
            "batch 1 loss 0.00041299872691923144\n",
            "batch 2 loss 0.0004090526449968895\n",
            "batch 3 loss 0.0003269254399205659\n",
            "batch 4 loss 0.0003202257895506959\n",
            "batch 5 loss 0.00034259365103064745\n",
            "batch 6 loss 0.00032071164666958046\n",
            "batch 7 loss 0.00038372370040379593\n",
            "batch 8 loss 0.0003720665774039007\n",
            "batch 9 loss 0.00032577766798007964\n",
            "batch 10 loss 0.00031008097702298044\n",
            "batch 11 loss 0.00035110495852798716\n",
            "batch 12 loss 0.00035967674017765833\n",
            "batch 13 loss 0.0003392198359901996\n",
            "batch 14 loss 0.0003048869007737688\n",
            "batch 15 loss 0.0003763895973339813\n",
            "batch 16 loss 0.00033344043950578247\n",
            "batch 17 loss 0.0003453711798332008\n",
            "batch 18 loss 0.0003534858177092742\n",
            "batch 19 loss 0.00034005063511748773\n",
            "batch 20 loss 0.0003282671847722235\n",
            "batch 21 loss 0.00031317411047469113\n",
            "batch 22 loss 0.0002996648208970431\n",
            "batch 23 loss 0.00035034150342186024\n",
            "batch 24 loss 0.00035419595280865513\n",
            "batch 25 loss 0.0003511350128730059\n",
            "batch 26 loss 0.00037319315656495557\n",
            "batch 27 loss 0.0003491817060782832\n",
            "batch 28 loss 0.00033405514963599824\n",
            "batch 29 loss 0.00034946367039457666\n",
            "batch 30 loss 0.0003804782363772494\n",
            "batch 31 loss 0.00030385144761677486\n",
            "batch 32 loss 0.000328873268753928\n",
            "batch 33 loss 0.00037138915549462387\n",
            "batch 34 loss 0.00033011642316445344\n",
            "batch 35 loss 0.0003049362681166096\n",
            "batch 36 loss 0.0003508099452233482\n",
            "batch 37 loss 0.0003754829090566906\n",
            "batch 38 loss 0.00034231757649981243\n",
            "batch 39 loss 0.00037461140106960185\n",
            "batch 40 loss 0.000272106558171948\n",
            "batch 41 loss 0.0003336476187281912\n",
            "batch 42 loss 0.0003550178962322408\n",
            "batch 43 loss 0.00037149494599684633\n",
            "batch 44 loss 0.00032344576739601506\n",
            "batch 45 loss 0.00040563323619419395\n",
            "batch 46 loss 0.0003248054129879148\n",
            "batch 47 loss 0.0003571395357644427\n",
            "batch 48 loss 0.000362353428978101\n",
            "batch 49 loss 0.00037966746428048746\n",
            "\n",
            "res: 0.00012019831176814436\n",
            "bc0: 0.0029981230289459564\n",
            "bc1: 0.0010064484715411304\n",
            "bc2: 0.0004376748397094304\n",
            "bc3: 0.001004760611684356\n",
            "\n",
            "\n",
            "Epoch 27\n",
            "batch 0 loss 0.0003277814372896392\n",
            "batch 1 loss 0.0003414973149549108\n",
            "batch 2 loss 0.0003495987875535871\n",
            "batch 3 loss 0.00032059611312164435\n",
            "batch 4 loss 0.0003350267834107393\n",
            "batch 5 loss 0.00030564972985979095\n",
            "batch 6 loss 0.00035583451913865505\n",
            "batch 7 loss 0.0003186210813815607\n",
            "batch 8 loss 0.0002781402125449379\n",
            "batch 9 loss 0.0003855466140817559\n",
            "batch 10 loss 0.00035427817549759326\n",
            "batch 11 loss 0.0003076847519115436\n",
            "batch 12 loss 0.0003166152702338045\n",
            "batch 13 loss 0.00036380364180308876\n",
            "batch 14 loss 0.0003232004847232745\n",
            "batch 15 loss 0.0003314085451424993\n",
            "batch 16 loss 0.0003365486592411412\n",
            "batch 17 loss 0.0003474971240708494\n",
            "batch 18 loss 0.0003583564026998879\n",
            "batch 19 loss 0.0003392052421085878\n",
            "batch 20 loss 0.00033934747174761486\n",
            "batch 21 loss 0.000310868883513005\n",
            "batch 22 loss 0.0003560068601004879\n",
            "batch 23 loss 0.0003135452396614028\n",
            "batch 24 loss 0.00034390081517504786\n",
            "batch 25 loss 0.0003554365274415047\n",
            "batch 26 loss 0.0003527063703505709\n",
            "batch 27 loss 0.0003687654230320557\n",
            "batch 28 loss 0.00031686169318778364\n",
            "batch 29 loss 0.00030308884170345153\n",
            "batch 30 loss 0.00030425078716503134\n",
            "batch 31 loss 0.00034500390217656536\n",
            "batch 32 loss 0.00030156912441736336\n",
            "batch 33 loss 0.00032058665238256397\n",
            "batch 34 loss 0.0003632134297222216\n",
            "batch 35 loss 0.0003273593101322377\n",
            "batch 36 loss 0.0003607405152895928\n",
            "batch 37 loss 0.0003324936205641552\n",
            "batch 38 loss 0.0003764013790737287\n",
            "batch 39 loss 0.000320588106876423\n",
            "batch 40 loss 0.0003693265685436992\n",
            "batch 41 loss 0.00033269252557396986\n",
            "batch 42 loss 0.0003513484788706307\n",
            "batch 43 loss 0.00033673889027785464\n",
            "batch 44 loss 0.00033488355695538845\n",
            "batch 45 loss 0.00033621559101409227\n",
            "batch 46 loss 0.00035956641510618247\n",
            "batch 47 loss 0.0002913321557324205\n",
            "batch 48 loss 0.00031379477042354543\n",
            "batch 49 loss 0.00031919726310666933\n",
            "\n",
            "res: 0.00011753946460439158\n",
            "bc0: 0.002972663578501261\n",
            "bc1: 0.0009398622821658779\n",
            "bc2: 0.00039571441125130376\n",
            "bc3: 0.0009398629766623306\n",
            "\n",
            "\n",
            "Epoch 28\n",
            "batch 0 loss 0.00032139832180721697\n",
            "batch 1 loss 0.00032901118221727175\n",
            "batch 2 loss 0.00031396767682355164\n",
            "batch 3 loss 0.0003318661290323825\n",
            "batch 4 loss 0.00033867733178048255\n",
            "batch 5 loss 0.00028635062101354435\n",
            "batch 6 loss 0.0002720202535929106\n",
            "batch 7 loss 0.00034006592263332116\n",
            "batch 8 loss 0.0003437223603424315\n",
            "batch 9 loss 0.0003181611276132029\n",
            "batch 10 loss 0.0003465532665052549\n",
            "batch 11 loss 0.00035634470823833124\n",
            "batch 12 loss 0.00034304396876189147\n",
            "batch 13 loss 0.0003112601911968439\n",
            "batch 14 loss 0.0003230463197645002\n",
            "batch 15 loss 0.0003842883822781454\n",
            "batch 16 loss 0.0003559235612389343\n",
            "batch 17 loss 0.0002756092671828297\n",
            "batch 18 loss 0.0003203977864609622\n",
            "batch 19 loss 0.00031503104011496556\n",
            "batch 20 loss 0.00035398951636202695\n",
            "batch 21 loss 0.00034268322957742173\n",
            "batch 22 loss 0.00029257108567508464\n",
            "batch 23 loss 0.00035711119612133376\n",
            "batch 24 loss 0.00035468450340745523\n",
            "batch 25 loss 0.00028852715973307834\n",
            "batch 26 loss 0.00028616622126383627\n",
            "batch 27 loss 0.00028402449271741474\n",
            "batch 28 loss 0.0002867581036717275\n",
            "batch 29 loss 0.0003498383332992127\n",
            "batch 30 loss 0.0003272941654898253\n",
            "batch 31 loss 0.00030814167872432593\n",
            "batch 32 loss 0.0003204108286518971\n",
            "batch 33 loss 0.0003660002658288245\n",
            "batch 34 loss 0.00031395154535655243\n",
            "batch 35 loss 0.0003543734055787353\n",
            "batch 36 loss 0.00031174102891380775\n",
            "batch 37 loss 0.0003158569910139422\n",
            "batch 38 loss 0.00029682949333680703\n",
            "batch 39 loss 0.00031756437149761674\n",
            "batch 40 loss 0.0003518105555301684\n",
            "batch 41 loss 0.0003241359220680192\n",
            "batch 42 loss 0.0003767912899782488\n",
            "batch 43 loss 0.000282701843177958\n",
            "batch 44 loss 0.000312075127910958\n",
            "batch 45 loss 0.0003022169791730047\n",
            "batch 46 loss 0.00029509773033560826\n",
            "batch 47 loss 0.0003355964065954148\n",
            "batch 48 loss 0.00029415689507064435\n",
            "batch 49 loss 0.00033847118269430014\n",
            "\n",
            "res: 0.0001158657115976157\n",
            "bc0: 0.0029498027178970084\n",
            "bc1: 0.0008702253095519571\n",
            "bc2: 0.00036516747077637704\n",
            "bc3: 0.0008695468655261634\n",
            "\n",
            "\n",
            "Epoch 29\n",
            "batch 0 loss 0.0003110302887715644\n",
            "batch 1 loss 0.0003079580263134852\n",
            "batch 2 loss 0.00028790453201835175\n",
            "batch 3 loss 0.0003102789646555789\n",
            "batch 4 loss 0.00032185594936501066\n",
            "batch 5 loss 0.0002824225039858417\n",
            "batch 6 loss 0.000295097885274789\n",
            "batch 7 loss 0.0003368727782191187\n",
            "batch 8 loss 0.00028986390461533453\n",
            "batch 9 loss 0.00029704538662722634\n",
            "batch 10 loss 0.00032424040993874506\n",
            "batch 11 loss 0.00031564322015144026\n",
            "batch 12 loss 0.0003558125665253497\n",
            "batch 13 loss 0.0003319807867533874\n",
            "batch 14 loss 0.00033159314005180526\n",
            "batch 15 loss 0.00032046745242412367\n",
            "batch 16 loss 0.0002874183146482933\n",
            "batch 17 loss 0.00030061442087451606\n",
            "batch 18 loss 0.0002874053555705757\n",
            "batch 19 loss 0.0003501177678089614\n",
            "batch 20 loss 0.0003209420675481202\n",
            "batch 21 loss 0.0003333971821764048\n",
            "batch 22 loss 0.0003406427583493684\n",
            "batch 23 loss 0.0003364990716992269\n",
            "batch 24 loss 0.00030448525025076115\n",
            "batch 25 loss 0.00031018302213012386\n",
            "batch 26 loss 0.00033797960864745315\n",
            "batch 27 loss 0.0003401722318499859\n",
            "batch 28 loss 0.00031214615251903035\n",
            "batch 29 loss 0.0003273594281177944\n",
            "batch 30 loss 0.00032915871547531573\n",
            "batch 31 loss 0.0002837337164933519\n",
            "batch 32 loss 0.0002918035777314984\n",
            "batch 33 loss 0.00033714313521130085\n",
            "batch 34 loss 0.0002756493882967176\n",
            "batch 35 loss 0.0002967703136492074\n",
            "batch 36 loss 0.0003046568614965018\n",
            "batch 37 loss 0.0003222515170946366\n",
            "batch 38 loss 0.0003340483809594604\n",
            "batch 39 loss 0.0003004435492675954\n",
            "batch 40 loss 0.0003123497139123876\n",
            "batch 41 loss 0.0003228180198342166\n",
            "batch 42 loss 0.0003135934266736067\n",
            "batch 43 loss 0.000264338523016377\n",
            "batch 44 loss 0.00030704726871189254\n",
            "batch 45 loss 0.00032343512166287623\n",
            "batch 46 loss 0.0003059345369920589\n",
            "batch 47 loss 0.00033415134984707756\n",
            "batch 48 loss 0.0002830488932944929\n",
            "batch 49 loss 0.0002985071863324714\n",
            "\n",
            "res: 0.00011475920443727606\n",
            "bc0: 0.0029239750115371698\n",
            "bc1: 0.000810552688584692\n",
            "bc2: 0.00032322193038567443\n",
            "bc3: 0.000808546694258403\n",
            "\n",
            "\n",
            "Epoch 30\n",
            "batch 0 loss 0.0003024441931479156\n",
            "batch 1 loss 0.00031875561491383593\n",
            "batch 2 loss 0.000317835325999774\n",
            "batch 3 loss 0.00029606208627418505\n",
            "batch 4 loss 0.00030796847000841553\n",
            "batch 5 loss 0.00031663144676301227\n",
            "batch 6 loss 0.0002797245672876251\n",
            "batch 7 loss 0.00031268556760370844\n",
            "batch 8 loss 0.00029648847709857183\n",
            "batch 9 loss 0.0003123095017251446\n",
            "batch 10 loss 0.0003329950125671149\n",
            "batch 11 loss 0.00029157048723499535\n",
            "batch 12 loss 0.0002985475527871974\n",
            "batch 13 loss 0.0002658272270439692\n",
            "batch 14 loss 0.00027976970728197113\n",
            "batch 15 loss 0.0002787005582423571\n",
            "batch 16 loss 0.0003353964747761665\n",
            "batch 17 loss 0.0003029712414209678\n",
            "batch 18 loss 0.00027639130426851356\n",
            "batch 19 loss 0.0003093512898348007\n",
            "batch 20 loss 0.00034035973642526857\n",
            "batch 21 loss 0.00023638282662084197\n",
            "batch 22 loss 0.00029889343949679535\n",
            "batch 23 loss 0.0002865284188241433\n",
            "batch 24 loss 0.00032167419527693756\n",
            "batch 25 loss 0.0003116974293076828\n",
            "batch 26 loss 0.0002815919543973763\n",
            "batch 27 loss 0.00031133538792779973\n",
            "batch 28 loss 0.00030924486643333537\n",
            "batch 29 loss 0.0002591027063439448\n",
            "batch 30 loss 0.0003456742104968991\n",
            "batch 31 loss 0.00027856651111471555\n",
            "batch 32 loss 0.0002967233622171887\n",
            "batch 33 loss 0.00035800684454259527\n",
            "batch 34 loss 0.0003237149365827657\n",
            "batch 35 loss 0.0003079996796345547\n",
            "batch 36 loss 0.0002977059942560456\n",
            "batch 37 loss 0.00031355621902739\n",
            "batch 38 loss 0.00029048839263131\n",
            "batch 39 loss 0.0003469718028317222\n",
            "batch 40 loss 0.00028597414020365764\n",
            "batch 41 loss 0.0003275192010162196\n",
            "batch 42 loss 0.00032020733702676043\n",
            "batch 43 loss 0.0003217684556406162\n",
            "batch 44 loss 0.0002774551954844708\n",
            "batch 45 loss 0.0002718403738235064\n",
            "batch 46 loss 0.00028764409166604117\n",
            "batch 47 loss 0.00030310501735329325\n",
            "batch 48 loss 0.00030031085515891127\n",
            "batch 49 loss 0.00028486479446443864\n",
            "\n",
            "res: 0.00011269453815889943\n",
            "bc0: 0.0028946075842824666\n",
            "bc1: 0.0007444546524817577\n",
            "bc2: 0.0003106547125209015\n",
            "bc3: 0.0007439175464904558\n",
            "\n",
            "\n",
            "finished unconstrained optimization\n",
            "res: 0.00011269453815889943\n",
            "bc0: 0.0028946075842824666\n",
            "bc1: 0.0007444546524817577\n",
            "bc2: 0.0003106547125209015\n",
            "bc3: 0.0007439175464904558\n",
            "\n",
            "\n",
            "[0.04777762 0.04803882 0.04153848 0.04791482]\n",
            "8.0\n",
            "Epoch 1\n",
            "batch 0 loss 0.0003977963377587298\n",
            "batch 1 loss 0.00045392217444741\n",
            "batch 2 loss 0.0004825670359037037\n",
            "batch 3 loss 0.0003853123015683214\n",
            "batch 4 loss 0.0003662586179196641\n",
            "batch 5 loss 0.0004490960376277805\n",
            "batch 6 loss 0.0003921963739410425\n",
            "batch 7 loss 0.0004220790744816138\n",
            "batch 8 loss 0.00045695928690681666\n",
            "batch 9 loss 0.00036225806270288196\n",
            "batch 10 loss 0.000453053181147926\n",
            "batch 11 loss 0.00039507979281202995\n",
            "batch 12 loss 0.00042286058126212307\n",
            "batch 13 loss 0.00040502763226228877\n",
            "batch 14 loss 0.00040159941422408465\n",
            "batch 15 loss 0.00041475752903011407\n",
            "batch 16 loss 0.00041063508128358816\n",
            "batch 17 loss 0.00044503607756233584\n",
            "batch 18 loss 0.00043534955442509196\n",
            "batch 19 loss 0.0003748860877584\n",
            "batch 20 loss 0.0004311248239636762\n",
            "batch 21 loss 0.00035084239366033125\n",
            "batch 22 loss 0.0004604884782383084\n",
            "batch 23 loss 0.00035316043109826997\n",
            "batch 24 loss 0.0004124242952982075\n",
            "batch 25 loss 0.00038312320043038813\n",
            "batch 26 loss 0.00042758850756127145\n",
            "batch 27 loss 0.0004082670627072591\n",
            "batch 28 loss 0.00037391734749801027\n",
            "batch 29 loss 0.0004551334899450595\n",
            "batch 30 loss 0.00041800641098806054\n",
            "batch 31 loss 0.0003694294994167733\n",
            "batch 32 loss 0.00038824389514991267\n",
            "batch 33 loss 0.0003711434524206533\n",
            "batch 34 loss 0.00038946332395539213\n",
            "batch 35 loss 0.000386803296450912\n",
            "batch 36 loss 0.0003859030375023988\n",
            "batch 37 loss 0.0004174002927824904\n",
            "batch 38 loss 0.00040674955808211663\n",
            "batch 39 loss 0.0004033641546556888\n",
            "batch 40 loss 0.0004624495015592173\n",
            "batch 41 loss 0.0003974286267659552\n",
            "batch 42 loss 0.00042798940732397754\n",
            "batch 43 loss 0.0003659875932614075\n",
            "batch 44 loss 0.0003869501461298954\n",
            "batch 45 loss 0.00041068801847532135\n",
            "batch 46 loss 0.00033920397997854086\n",
            "batch 47 loss 0.00039925474133295405\n",
            "batch 48 loss 0.0003512363654960326\n",
            "batch 49 loss 0.0004531553011081686\n",
            "\n",
            "res: 0.00011194985212210149\n",
            "bc0: 0.0028572634657493966\n",
            "bc1: 0.0006623213109302294\n",
            "bc2: 0.00030757674963177793\n",
            "bc3: 0.000662256770285091\n",
            "\n",
            "\n",
            "Epoch 2\n",
            "batch 0 loss 0.0003821168776799797\n",
            "batch 1 loss 0.0003795280602333\n",
            "batch 2 loss 0.00045360308966856556\n",
            "batch 3 loss 0.00035310780355022844\n",
            "batch 4 loss 0.0004037335624242267\n",
            "batch 5 loss 0.00039711951158842893\n",
            "batch 6 loss 0.0004612839787920039\n",
            "batch 7 loss 0.00038845299830544017\n",
            "batch 8 loss 0.00041315168147710224\n",
            "batch 9 loss 0.0003874840273034883\n",
            "batch 10 loss 0.00032318262950364716\n",
            "batch 11 loss 0.0004608390200222988\n",
            "batch 12 loss 0.0003810952111954075\n",
            "batch 13 loss 0.00036558905825122924\n",
            "batch 14 loss 0.0003859607376091331\n",
            "batch 15 loss 0.0004135780045980816\n",
            "batch 16 loss 0.000349356682956659\n",
            "batch 17 loss 0.00042240612062998057\n",
            "batch 18 loss 0.0003898868697661999\n",
            "batch 19 loss 0.0003687473957623526\n",
            "batch 20 loss 0.00037919712984139085\n",
            "batch 21 loss 0.0004186710906695464\n",
            "batch 22 loss 0.0004031219794110486\n",
            "batch 23 loss 0.00045219651304477445\n",
            "batch 24 loss 0.00035450743670463946\n",
            "batch 25 loss 0.00037890576092974014\n",
            "batch 26 loss 0.0003561170192013144\n",
            "batch 27 loss 0.00040809944477213187\n",
            "batch 28 loss 0.00035508427709159834\n",
            "batch 29 loss 0.000403045089753101\n",
            "batch 30 loss 0.00033316775726902\n",
            "batch 31 loss 0.0004232214645423004\n",
            "batch 32 loss 0.0004058953169610015\n",
            "batch 33 loss 0.0004381413905279653\n",
            "batch 34 loss 0.00042943723796794864\n",
            "batch 35 loss 0.00033976224158462255\n",
            "batch 36 loss 0.00041212316548140627\n",
            "batch 37 loss 0.00033618263015009896\n",
            "batch 38 loss 0.0004074534439818735\n",
            "batch 39 loss 0.00036795258871981173\n",
            "batch 40 loss 0.00035758527623610647\n",
            "batch 41 loss 0.00041003243681693013\n",
            "batch 42 loss 0.0003640813703678595\n",
            "batch 43 loss 0.0003873568581060421\n",
            "batch 44 loss 0.0004259307996014456\n",
            "batch 45 loss 0.00036063172376893377\n",
            "batch 46 loss 0.0003291778273366026\n",
            "batch 47 loss 0.00042003064833240025\n",
            "batch 48 loss 0.0004247970104682739\n",
            "batch 49 loss 0.0004009133005604184\n",
            "\n",
            "res: 0.00011033793692831419\n",
            "bc0: 0.002819679742504222\n",
            "bc1: 0.0005941859904341669\n",
            "bc2: 0.0002794213311430524\n",
            "bc3: 0.0005928928865592735\n",
            "\n",
            "\n",
            "Epoch 3\n",
            "batch 0 loss 0.00039098037433607486\n",
            "batch 1 loss 0.0003865979094072086\n",
            "batch 2 loss 0.0003834791987211009\n",
            "batch 3 loss 0.00039644069375060787\n",
            "batch 4 loss 0.00035884499043653285\n",
            "batch 5 loss 0.0003743514366389031\n",
            "batch 6 loss 0.00039092056968019766\n",
            "batch 7 loss 0.0003522643508316044\n",
            "batch 8 loss 0.0003930447593770608\n",
            "batch 9 loss 0.0003562503403725812\n",
            "batch 10 loss 0.0003679404655118256\n",
            "batch 11 loss 0.00037009567351824175\n",
            "batch 12 loss 0.0003786517154998542\n",
            "batch 13 loss 0.0003536478352952422\n",
            "batch 14 loss 0.0004148452181950478\n",
            "batch 15 loss 0.00040144152528899166\n",
            "batch 16 loss 0.0003514691509464221\n",
            "batch 17 loss 0.00041725583722853997\n",
            "batch 18 loss 0.0003707370811258946\n",
            "batch 19 loss 0.000364433012313336\n",
            "batch 20 loss 0.0004427677673395782\n",
            "batch 21 loss 0.00033563835155006437\n",
            "batch 22 loss 0.00047014305523244575\n",
            "batch 23 loss 0.0003401192645534642\n",
            "batch 24 loss 0.00046919646326422197\n",
            "batch 25 loss 0.00032793285494482643\n",
            "batch 26 loss 0.0003384412157649044\n",
            "batch 27 loss 0.00038447177809897056\n",
            "batch 28 loss 0.000338592310782292\n",
            "batch 29 loss 0.00040142407388547684\n",
            "batch 30 loss 0.0003773727341057184\n",
            "batch 31 loss 0.0003517159084086862\n",
            "batch 32 loss 0.00040461737553116884\n",
            "batch 33 loss 0.00037055016503329577\n",
            "batch 34 loss 0.0004009113413493868\n",
            "batch 35 loss 0.0003600020166757105\n",
            "batch 36 loss 0.0003865819881141742\n",
            "batch 37 loss 0.0004105068134824426\n",
            "batch 38 loss 0.0003984462833189977\n",
            "batch 39 loss 0.00034073485960314895\n",
            "batch 40 loss 0.0003606644637473827\n",
            "batch 41 loss 0.00036458060803963414\n",
            "batch 42 loss 0.00029893083897559563\n",
            "batch 43 loss 0.0004697564318334546\n",
            "batch 44 loss 0.0003656071193589516\n",
            "batch 45 loss 0.0003668168977405148\n",
            "batch 46 loss 0.0003344207950825741\n",
            "batch 47 loss 0.00038052978159386786\n",
            "batch 48 loss 0.0003560596400747733\n",
            "batch 49 loss 0.00037628465004007743\n",
            "\n",
            "res: 0.00010817030334891377\n",
            "bc0: 0.0027917365863980503\n",
            "bc1: 0.0005360679502149886\n",
            "bc2: 0.000248044375940193\n",
            "bc3: 0.0005369666562798645\n",
            "\n",
            "\n",
            "Epoch 4\n",
            "batch 0 loss 0.0003523331839856116\n",
            "batch 1 loss 0.0004172314312520538\n",
            "batch 2 loss 0.000342918932142933\n",
            "batch 3 loss 0.0003962195888087491\n",
            "batch 4 loss 0.00037271684987295493\n",
            "batch 5 loss 0.00037647557274594033\n",
            "batch 6 loss 0.00034752107029676656\n",
            "batch 7 loss 0.000358205045383152\n",
            "batch 8 loss 0.0003762306958784937\n",
            "batch 9 loss 0.000370699092309548\n",
            "batch 10 loss 0.0003963480539858294\n",
            "batch 11 loss 0.00035201938565067857\n",
            "batch 12 loss 0.00036838563231035825\n",
            "batch 13 loss 0.0003658800688244576\n",
            "batch 14 loss 0.0003284895865316425\n",
            "batch 15 loss 0.0003785325886000506\n",
            "batch 16 loss 0.000337371015574009\n",
            "batch 17 loss 0.00038946282241754447\n",
            "batch 18 loss 0.00039839347330163034\n",
            "batch 19 loss 0.00035853131090417056\n",
            "batch 20 loss 0.000336413587700564\n",
            "batch 21 loss 0.0003584458028057884\n",
            "batch 22 loss 0.00038776575595797556\n",
            "batch 23 loss 0.00035958658330443655\n",
            "batch 24 loss 0.0003654561304979379\n",
            "batch 25 loss 0.0003265195847028501\n",
            "batch 26 loss 0.00035782621473237885\n",
            "batch 27 loss 0.0003543995261523081\n",
            "batch 28 loss 0.0004179535371941844\n",
            "batch 29 loss 0.00031892192420027876\n",
            "batch 30 loss 0.00036128643527759825\n",
            "batch 31 loss 0.0003797161394901598\n",
            "batch 32 loss 0.0003715570467963261\n",
            "batch 33 loss 0.00040202496969862013\n",
            "batch 34 loss 0.0003889230793645879\n",
            "batch 35 loss 0.00034975796258660837\n",
            "batch 36 loss 0.00034057583446186796\n",
            "batch 37 loss 0.0003543073697767214\n",
            "batch 38 loss 0.0003360099129368036\n",
            "batch 39 loss 0.00041291236339258987\n",
            "batch 40 loss 0.00033695493453663936\n",
            "batch 41 loss 0.0004365859989908662\n",
            "batch 42 loss 0.00038051612398673317\n",
            "batch 43 loss 0.0003491962902037786\n",
            "batch 44 loss 0.0003334007447199001\n",
            "batch 45 loss 0.00037390380927857333\n",
            "batch 46 loss 0.0003067932129379839\n",
            "batch 47 loss 0.00035716471765065633\n",
            "batch 48 loss 0.00039776329544816315\n",
            "batch 49 loss 0.0003272197382724579\n",
            "\n",
            "res: 0.00010620869413580649\n",
            "bc0: 0.002762129115744384\n",
            "bc1: 0.0004771011544047514\n",
            "bc2: 0.00022995453915728776\n",
            "bc3: 0.0004760888747479546\n",
            "\n",
            "\n",
            "Epoch 5\n",
            "batch 0 loss 0.0003709326628988137\n",
            "batch 1 loss 0.000313850115626146\n",
            "batch 2 loss 0.00036938684034377154\n",
            "batch 3 loss 0.00033956918429717915\n",
            "batch 4 loss 0.00037162795830521564\n",
            "batch 5 loss 0.0003627820963660753\n",
            "batch 6 loss 0.0003420949201116448\n",
            "batch 7 loss 0.00034146871913164404\n",
            "batch 8 loss 0.00041824055945210515\n",
            "batch 9 loss 0.0003686405761483638\n",
            "batch 10 loss 0.00030718874255356775\n",
            "batch 11 loss 0.00037648651105590743\n",
            "batch 12 loss 0.00035676151724802873\n",
            "batch 13 loss 0.00034712197081736133\n",
            "batch 14 loss 0.00040351977262220913\n",
            "batch 15 loss 0.0003578706001220326\n",
            "batch 16 loss 0.0003084224984595047\n",
            "batch 17 loss 0.0003760153867849081\n",
            "batch 18 loss 0.00036511566101807367\n",
            "batch 19 loss 0.000330102391318812\n",
            "batch 20 loss 0.00042421436725125937\n",
            "batch 21 loss 0.0003293230353183598\n",
            "batch 22 loss 0.00035348045039632\n",
            "batch 23 loss 0.0003638801144979379\n",
            "batch 24 loss 0.0004098265957494813\n",
            "batch 25 loss 0.00032072188327843037\n",
            "batch 26 loss 0.0003675503630529561\n",
            "batch 27 loss 0.0003116718703508068\n",
            "batch 28 loss 0.0003749614395099272\n",
            "batch 29 loss 0.00033440537239667173\n",
            "batch 30 loss 0.00034255627820612013\n",
            "batch 31 loss 0.00034919594216233893\n",
            "batch 32 loss 0.0003218361243241709\n",
            "batch 33 loss 0.0003750886056654632\n",
            "batch 34 loss 0.0003934422636423641\n",
            "batch 35 loss 0.0003351091925967209\n",
            "batch 36 loss 0.0003292470685899445\n",
            "batch 37 loss 0.0003566619089096236\n",
            "batch 38 loss 0.00031365321828490626\n",
            "batch 39 loss 0.0003674587442657218\n",
            "batch 40 loss 0.00033230953689537937\n",
            "batch 41 loss 0.00034319969099715205\n",
            "batch 42 loss 0.00033325014960520864\n",
            "batch 43 loss 0.0003925528510306217\n",
            "batch 44 loss 0.00030786095223459334\n",
            "batch 45 loss 0.000395372857202592\n",
            "batch 46 loss 0.00034059386625310395\n",
            "batch 47 loss 0.0003234540597413962\n",
            "batch 48 loss 0.00033213911913564803\n",
            "batch 49 loss 0.0004076057677008783\n",
            "\n",
            "res: 0.00010431457022111625\n",
            "bc0: 0.0027359342137825817\n",
            "bc1: 0.00042313019573214666\n",
            "bc2: 0.0002100444673133607\n",
            "bc3: 0.00042313662237357846\n",
            "\n",
            "\n",
            "Epoch 6\n",
            "batch 0 loss 0.0003632785564115379\n",
            "batch 1 loss 0.0003328394147797097\n",
            "batch 2 loss 0.00039717037984466263\n",
            "batch 3 loss 0.00034109078205868555\n",
            "batch 4 loss 0.00036243083526765204\n",
            "batch 5 loss 0.0003260932528287673\n",
            "batch 6 loss 0.0003242863927315595\n",
            "batch 7 loss 0.000370736435752268\n",
            "batch 8 loss 0.000333635562673856\n",
            "batch 9 loss 0.00036194740152635535\n",
            "batch 10 loss 0.0003859654188618687\n",
            "batch 11 loss 0.0003147347241531644\n",
            "batch 12 loss 0.0003333232964346266\n",
            "batch 13 loss 0.0003387987382028423\n",
            "batch 14 loss 0.00031974272250106335\n",
            "batch 15 loss 0.00036678409633689147\n",
            "batch 16 loss 0.0003512473549110449\n",
            "batch 17 loss 0.0003246817600213813\n",
            "batch 18 loss 0.00034652410483787673\n",
            "batch 19 loss 0.0004084003076799272\n",
            "batch 20 loss 0.0003560281016787266\n",
            "batch 21 loss 0.00032622243922196385\n",
            "batch 22 loss 0.0003653864568893546\n",
            "batch 23 loss 0.0003151210786116945\n",
            "batch 24 loss 0.00034291184990274513\n",
            "batch 25 loss 0.0003516433373600382\n",
            "batch 26 loss 0.00032659915583415223\n",
            "batch 27 loss 0.0003767308675860727\n",
            "batch 28 loss 0.00038173532438515244\n",
            "batch 29 loss 0.0003393187661841447\n",
            "batch 30 loss 0.0003260060644678456\n",
            "batch 31 loss 0.00032031223138910943\n",
            "batch 32 loss 0.0003518181972503961\n",
            "batch 33 loss 0.00033336964983494017\n",
            "batch 34 loss 0.000329063955086159\n",
            "batch 35 loss 0.00035553703476461096\n",
            "batch 36 loss 0.00036352125380136594\n",
            "batch 37 loss 0.00031645662860325447\n",
            "batch 38 loss 0.0003127718713911157\n",
            "batch 39 loss 0.0003286827156598441\n",
            "batch 40 loss 0.0003350253862703344\n",
            "batch 41 loss 0.00031519810630878755\n",
            "batch 42 loss 0.0003648967591054866\n",
            "batch 43 loss 0.00031430655059838094\n",
            "batch 44 loss 0.00029947926268178784\n",
            "batch 45 loss 0.0003806008551177725\n",
            "batch 46 loss 0.00030629047622397385\n",
            "batch 47 loss 0.0003494212253029882\n",
            "batch 48 loss 0.00033693094530385997\n",
            "batch 49 loss 0.00032503457761641686\n",
            "\n",
            "res: 0.00010284292093136908\n",
            "bc0: 0.0027123443256765567\n",
            "bc1: 0.0003750616012090931\n",
            "bc2: 0.00018011734928805468\n",
            "bc3: 0.00037499628190442407\n",
            "\n",
            "\n",
            "Epoch 7\n",
            "batch 0 loss 0.00037789283661770546\n",
            "batch 1 loss 0.0003308024207221758\n",
            "batch 2 loss 0.00031994381304761\n",
            "batch 3 loss 0.0003239240835674866\n",
            "batch 4 loss 0.00031390938350886076\n",
            "batch 5 loss 0.00036598422549056684\n",
            "batch 6 loss 0.0003280960526561303\n",
            "batch 7 loss 0.00034952177867490113\n",
            "batch 8 loss 0.00031299935593896277\n",
            "batch 9 loss 0.00031775917199370533\n",
            "batch 10 loss 0.0003497663256384142\n",
            "batch 11 loss 0.0003288740114920668\n",
            "batch 12 loss 0.00032063544652622376\n",
            "batch 13 loss 0.00031147555657472524\n",
            "batch 14 loss 0.0003756714876784268\n",
            "batch 15 loss 0.00028989810745328067\n",
            "batch 16 loss 0.00038620145760710523\n",
            "batch 17 loss 0.0003022760801406682\n",
            "batch 18 loss 0.00036362180133054744\n",
            "batch 19 loss 0.0003251535934171216\n",
            "batch 20 loss 0.0003368894241402825\n",
            "batch 21 loss 0.0002824121781232655\n",
            "batch 22 loss 0.0003414174939847595\n",
            "batch 23 loss 0.00029835207575210516\n",
            "batch 24 loss 0.00031771474559119103\n",
            "batch 25 loss 0.00036133978580164025\n",
            "batch 26 loss 0.0003451668250812254\n",
            "batch 27 loss 0.000359342693482055\n",
            "batch 28 loss 0.00035228260635681777\n",
            "batch 29 loss 0.00030680905941386376\n",
            "batch 30 loss 0.00032468752322776156\n",
            "batch 31 loss 0.00036437127703843885\n",
            "batch 32 loss 0.0003194617648265856\n",
            "batch 33 loss 0.00030464156960378636\n",
            "batch 34 loss 0.00029920454619966575\n",
            "batch 35 loss 0.00037685920811776754\n",
            "batch 36 loss 0.0004026448459444984\n",
            "batch 37 loss 0.0003192425522708003\n",
            "batch 38 loss 0.00034386445311753247\n",
            "batch 39 loss 0.0003283743664975991\n",
            "batch 40 loss 0.0003493384877053384\n",
            "batch 41 loss 0.0002725240225726876\n",
            "batch 42 loss 0.0003810787695420331\n",
            "batch 43 loss 0.0003141307685635343\n",
            "batch 44 loss 0.0003231796303382295\n",
            "batch 45 loss 0.0003414092349356086\n",
            "batch 46 loss 0.0003669946793868564\n",
            "batch 47 loss 0.00027733406787851326\n",
            "batch 48 loss 0.0003214445645398968\n",
            "batch 49 loss 0.0003412198402611256\n",
            "\n",
            "res: 0.00010124447975484087\n",
            "bc0: 0.002677094905253009\n",
            "bc1: 0.00032941894395559115\n",
            "bc2: 0.00018802151829942085\n",
            "bc3: 0.00032985925967575367\n",
            "\n",
            "\n",
            "Epoch 8\n",
            "batch 0 loss 0.0002795887906521445\n",
            "batch 1 loss 0.00037273165270187845\n",
            "batch 2 loss 0.0003048079220591811\n",
            "batch 3 loss 0.00037593402745950724\n",
            "batch 4 loss 0.0003035410840279172\n",
            "batch 5 loss 0.0003852355164344888\n",
            "batch 6 loss 0.0003200850473141113\n",
            "batch 7 loss 0.000332881095867657\n",
            "batch 8 loss 0.0002681759144327017\n",
            "batch 9 loss 0.00039135575661972637\n",
            "batch 10 loss 0.00032175340742871575\n",
            "batch 11 loss 0.00032969151791844924\n",
            "batch 12 loss 0.000304253799517869\n",
            "batch 13 loss 0.0003373530491807803\n",
            "batch 14 loss 0.00023022446324716855\n",
            "batch 15 loss 0.0003884599675138477\n",
            "batch 16 loss 0.00038690335407531667\n",
            "batch 17 loss 0.0002620289181569402\n",
            "batch 18 loss 0.00039698057121561594\n",
            "batch 19 loss 0.00026815197559100544\n",
            "batch 20 loss 0.0003537318013364744\n",
            "batch 21 loss 0.00030415031207525626\n",
            "batch 22 loss 0.0003684471348027214\n",
            "batch 23 loss 0.0003270282086810982\n",
            "batch 24 loss 0.00035967171967283835\n",
            "batch 25 loss 0.0002979739520516584\n",
            "batch 26 loss 0.00035809844575506594\n",
            "batch 27 loss 0.0002723970423042797\n",
            "batch 28 loss 0.00033641481088360424\n",
            "batch 29 loss 0.0003207984212434792\n",
            "batch 30 loss 0.000379954944767051\n",
            "batch 31 loss 0.00025128853215036925\n",
            "batch 32 loss 0.00033445324955234495\n",
            "batch 33 loss 0.0003061907339681918\n",
            "batch 34 loss 0.00033102950401886017\n",
            "batch 35 loss 0.0003129979671439821\n",
            "batch 36 loss 0.00032022856336810513\n",
            "batch 37 loss 0.0003085865575142134\n",
            "batch 38 loss 0.0002982395018668162\n",
            "batch 39 loss 0.00029514278897638127\n",
            "batch 40 loss 0.000257787816814875\n",
            "batch 41 loss 0.0003671235030232076\n",
            "batch 42 loss 0.00031865009641712837\n",
            "batch 43 loss 0.00034783279167549666\n",
            "batch 44 loss 0.0003442010248059895\n",
            "batch 45 loss 0.0003036107501550622\n",
            "batch 46 loss 0.0003407996562092384\n",
            "batch 47 loss 0.0003071280963629098\n",
            "batch 48 loss 0.0003339775360637236\n",
            "batch 49 loss 0.0003248211882115336\n",
            "\n",
            "res: 0.00010030123345418083\n",
            "bc0: 0.0026612510687472513\n",
            "bc1: 0.0002923868344535015\n",
            "bc2: 0.00014533081616719992\n",
            "bc3: 0.000292145047234821\n",
            "\n",
            "\n",
            "Epoch 9\n",
            "batch 0 loss 0.00032713346994818287\n",
            "batch 1 loss 0.0003029810025601575\n",
            "batch 2 loss 0.0003215786636939066\n",
            "batch 3 loss 0.000322791958348905\n",
            "batch 4 loss 0.00031392470946556793\n",
            "batch 5 loss 0.00030720012225143975\n",
            "batch 6 loss 0.00033498707746102677\n",
            "batch 7 loss 0.00033755522906823526\n",
            "batch 8 loss 0.000275329491305341\n",
            "batch 9 loss 0.00037601037030487604\n",
            "batch 10 loss 0.0003070409122857153\n",
            "batch 11 loss 0.0003029862839753888\n",
            "batch 12 loss 0.00028525546709125374\n",
            "batch 13 loss 0.00035922791084735126\n",
            "batch 14 loss 0.00029491878966251726\n",
            "batch 15 loss 0.0003493700530024735\n",
            "batch 16 loss 0.00029399428532396276\n",
            "batch 17 loss 0.00033288058978665633\n",
            "batch 18 loss 0.0003062469858433055\n",
            "batch 19 loss 0.00031810981316350903\n",
            "batch 20 loss 0.000328975182229198\n",
            "batch 21 loss 0.00027538177479651113\n",
            "batch 22 loss 0.00032684956989001375\n",
            "batch 23 loss 0.0003116788071277572\n",
            "batch 24 loss 0.00029810483366526696\n",
            "batch 25 loss 0.0003220887446325321\n",
            "batch 26 loss 0.000363477543541123\n",
            "batch 27 loss 0.0003231356759787724\n",
            "batch 28 loss 0.00034560646731772226\n",
            "batch 29 loss 0.0003040652148336923\n",
            "batch 30 loss 0.0003334791372334545\n",
            "batch 31 loss 0.0003588256959908509\n",
            "batch 32 loss 0.0002982348685683273\n",
            "batch 33 loss 0.0003124826952941248\n",
            "batch 34 loss 0.0003335285807991399\n",
            "batch 35 loss 0.00029770171856469883\n",
            "batch 36 loss 0.0003092433182322531\n",
            "batch 37 loss 0.00032189950723051015\n",
            "batch 38 loss 0.0002738032848414187\n",
            "batch 39 loss 0.0003128402647120631\n",
            "batch 40 loss 0.0003203215982057298\n",
            "batch 41 loss 0.00029098890524887705\n",
            "batch 42 loss 0.0002482032983926274\n",
            "batch 43 loss 0.00035258327508140856\n",
            "batch 44 loss 0.0003156867644509195\n",
            "batch 45 loss 0.0003306419315699095\n",
            "batch 46 loss 0.0002864761569749985\n",
            "batch 47 loss 0.00032184251916739335\n",
            "batch 48 loss 0.00033061702884696725\n",
            "batch 49 loss 0.00028163353479818907\n",
            "\n",
            "res: 9.811737996432243e-05\n",
            "bc0: 0.0026308014359176114\n",
            "bc1: 0.00025247423679009527\n",
            "bc2: 0.00014842441032281473\n",
            "bc3: 0.0002527501759685843\n",
            "\n",
            "\n",
            "Epoch 10\n",
            "batch 0 loss 0.00033722312455110575\n",
            "batch 1 loss 0.00028492514662764004\n",
            "batch 2 loss 0.0002722323604451362\n",
            "batch 3 loss 0.0003028265334674504\n",
            "batch 4 loss 0.0003399278322832711\n",
            "batch 5 loss 0.0002994692962948161\n",
            "batch 6 loss 0.00028990320393830645\n",
            "batch 7 loss 0.00031505528454659675\n",
            "batch 8 loss 0.0003164233545863322\n",
            "batch 9 loss 0.00034984711212510995\n",
            "batch 10 loss 0.00028178523272154803\n",
            "batch 11 loss 0.00033478716288095906\n",
            "batch 12 loss 0.00026911734205464094\n",
            "batch 13 loss 0.00029826951354205885\n",
            "batch 14 loss 0.00029444286654285577\n",
            "batch 15 loss 0.00035931060256625845\n",
            "batch 16 loss 0.0003288831798279726\n",
            "batch 17 loss 0.0002962431255782421\n",
            "batch 18 loss 0.000251063853037867\n",
            "batch 19 loss 0.000342773261267092\n",
            "batch 20 loss 0.00031726815224793957\n",
            "batch 21 loss 0.0002952945360479722\n",
            "batch 22 loss 0.0002876040882863872\n",
            "batch 23 loss 0.00031054460745778676\n",
            "batch 24 loss 0.00034070671435191565\n",
            "batch 25 loss 0.00033503528002463246\n",
            "batch 26 loss 0.0003129424706252918\n",
            "batch 27 loss 0.00028465567371332886\n",
            "batch 28 loss 0.0003227543943563766\n",
            "batch 29 loss 0.00031268075468674335\n",
            "batch 30 loss 0.00029224178960603523\n",
            "batch 31 loss 0.00033774469284220457\n",
            "batch 32 loss 0.00033789924356097685\n",
            "batch 33 loss 0.0002886793878709151\n",
            "batch 34 loss 0.000308591958271702\n",
            "batch 35 loss 0.00032372809209465597\n",
            "batch 36 loss 0.00029053387564236736\n",
            "batch 37 loss 0.0003187244961246731\n",
            "batch 38 loss 0.00033697477145523295\n",
            "batch 39 loss 0.0002490702223862249\n",
            "batch 40 loss 0.0002979745298679636\n",
            "batch 41 loss 0.0003171125352737522\n",
            "batch 42 loss 0.00034720168300796\n",
            "batch 43 loss 0.00027125116742683506\n",
            "batch 44 loss 0.00027681160534034617\n",
            "batch 45 loss 0.0003240619542645555\n",
            "batch 46 loss 0.00029085168905347553\n",
            "batch 47 loss 0.00032507904769144794\n",
            "batch 48 loss 0.00026502667599092927\n",
            "batch 49 loss 0.0003118901526856334\n",
            "\n",
            "res: 9.71958268552546e-05\n",
            "bc0: 0.0026043975913535517\n",
            "bc1: 0.00021645473069605635\n",
            "bc2: 0.00014477347842453212\n",
            "bc3: 0.00021654458733980713\n",
            "\n",
            "\n",
            "Epoch 11\n",
            "batch 0 loss 0.00029824332461010736\n",
            "batch 1 loss 0.0003106463485278703\n",
            "batch 2 loss 0.0003235706707159593\n",
            "batch 3 loss 0.0003010577983912411\n",
            "batch 4 loss 0.0002675701411903232\n",
            "batch 5 loss 0.00032388641962162007\n",
            "batch 6 loss 0.00031470792835291\n",
            "batch 7 loss 0.0003100235699737916\n",
            "batch 8 loss 0.0003263578299039346\n",
            "batch 9 loss 0.0002769839219040951\n",
            "batch 10 loss 0.00032349717579003396\n",
            "batch 11 loss 0.0002400468156855\n",
            "batch 12 loss 0.0003169164670803907\n",
            "batch 13 loss 0.0003053202594205503\n",
            "batch 14 loss 0.0002943159453686318\n",
            "batch 15 loss 0.0002847793992201257\n",
            "batch 16 loss 0.0002792232834945282\n",
            "batch 17 loss 0.00033460897618580707\n",
            "batch 18 loss 0.0002904109357043193\n",
            "batch 19 loss 0.0003270792654065521\n",
            "batch 20 loss 0.00032883564152100524\n",
            "batch 21 loss 0.0002760652067852019\n",
            "batch 22 loss 0.00032013338401096404\n",
            "batch 23 loss 0.0002590101439490457\n",
            "batch 24 loss 0.0003215419646106394\n",
            "batch 25 loss 0.00025521797822604256\n",
            "batch 26 loss 0.000292974821892093\n",
            "batch 27 loss 0.00030229520043765097\n",
            "batch 28 loss 0.0003132599738580307\n",
            "batch 29 loss 0.00032704206949606124\n",
            "batch 30 loss 0.0003260322181698073\n",
            "batch 31 loss 0.0003231695767066393\n",
            "batch 32 loss 0.00029835915671354605\n",
            "batch 33 loss 0.00032173611470374995\n",
            "batch 34 loss 0.000272924918977649\n",
            "batch 35 loss 0.0003372612118554115\n",
            "batch 36 loss 0.00027131907069292646\n",
            "batch 37 loss 0.0002976656392727747\n",
            "batch 38 loss 0.0003140830220790384\n",
            "batch 39 loss 0.00030174775283714706\n",
            "batch 40 loss 0.0003124213094023379\n",
            "batch 41 loss 0.0002878709073366345\n",
            "batch 42 loss 0.0003104505099623488\n",
            "batch 43 loss 0.000292047027161583\n",
            "batch 44 loss 0.0002727048611649462\n",
            "batch 45 loss 0.0002792544092787447\n",
            "batch 46 loss 0.00031559058054456097\n",
            "batch 47 loss 0.0002757230785867807\n",
            "batch 48 loss 0.000275949471524615\n",
            "batch 49 loss 0.0002847107226229163\n",
            "\n",
            "res: 9.59283238032313e-05\n",
            "bc0: 0.0025847582787970314\n",
            "bc1: 0.0001928730529480071\n",
            "bc2: 0.00010780313121326254\n",
            "bc3: 0.00019231856263737245\n",
            "\n",
            "\n",
            "Epoch 12\n",
            "batch 0 loss 0.00030039776263579115\n",
            "batch 1 loss 0.00030184845797350787\n",
            "batch 2 loss 0.0002544841508974742\n",
            "batch 3 loss 0.00036061301447330745\n",
            "batch 4 loss 0.00030734544837729075\n",
            "batch 5 loss 0.00029307180348854414\n",
            "batch 6 loss 0.00036401205195373134\n",
            "batch 7 loss 0.00027224347234982535\n",
            "batch 8 loss 0.0003086284114643786\n",
            "batch 9 loss 0.00027043665089170114\n",
            "batch 10 loss 0.0002554857332638569\n",
            "batch 11 loss 0.00031104512994651994\n",
            "batch 12 loss 0.0003155741931282433\n",
            "batch 13 loss 0.00023881767901938166\n",
            "batch 14 loss 0.00030327798657273016\n",
            "batch 15 loss 0.00029402842220154326\n",
            "batch 16 loss 0.0003013310619316557\n",
            "batch 17 loss 0.00034337984418206246\n",
            "batch 18 loss 0.00028796912336249996\n",
            "batch 19 loss 0.0003277913136159144\n",
            "batch 20 loss 0.0002977375845830929\n",
            "batch 21 loss 0.00029227342907169783\n",
            "batch 22 loss 0.00030082637509493544\n",
            "batch 23 loss 0.00030188276123713833\n",
            "batch 24 loss 0.00030341508905922394\n",
            "batch 25 loss 0.0002693272524906785\n",
            "batch 26 loss 0.00033399125510568246\n",
            "batch 27 loss 0.00028671648272656446\n",
            "batch 28 loss 0.00027036023181428953\n",
            "batch 29 loss 0.00026825828986963286\n",
            "batch 30 loss 0.00028770311710569503\n",
            "batch 31 loss 0.00032984538690252916\n",
            "batch 32 loss 0.00025176685485756744\n",
            "batch 33 loss 0.0002939063365660581\n",
            "batch 34 loss 0.00030027674686098553\n",
            "batch 35 loss 0.00025100336874389867\n",
            "batch 36 loss 0.0002604161360030063\n",
            "batch 37 loss 0.0003051863581401269\n",
            "batch 38 loss 0.00026476279755428773\n",
            "batch 39 loss 0.00032454710310630936\n",
            "batch 40 loss 0.00027877197649013585\n",
            "batch 41 loss 0.00031632700868428553\n",
            "batch 42 loss 0.000260627232531381\n",
            "batch 43 loss 0.0002720354457748657\n",
            "batch 44 loss 0.00028585206791134746\n",
            "batch 45 loss 0.00029658134480411844\n",
            "batch 46 loss 0.00024292414853592888\n",
            "batch 47 loss 0.0003260826152349689\n",
            "batch 48 loss 0.0002863826335488155\n",
            "batch 49 loss 0.00030886025693043726\n",
            "\n",
            "res: 9.3705560684391e-05\n",
            "bc0: 0.0025703701331177965\n",
            "bc1: 0.00016535016533695496\n",
            "bc2: 9.57144581793123e-05\n",
            "bc3: 0.0001657942153014877\n",
            "\n",
            "\n",
            "Epoch 13\n",
            "batch 0 loss 0.0003345662569248633\n",
            "batch 1 loss 0.000262052898824001\n",
            "batch 2 loss 0.0002713373502977152\n",
            "batch 3 loss 0.0002657889600148971\n",
            "batch 4 loss 0.0002375375591157496\n",
            "batch 5 loss 0.00030052495539467086\n",
            "batch 6 loss 0.00027527646523789733\n",
            "batch 7 loss 0.00030433917146575543\n",
            "batch 8 loss 0.0002967671471995521\n",
            "batch 9 loss 0.0003008369964962152\n",
            "batch 10 loss 0.0002938210132810074\n",
            "batch 11 loss 0.00030055167063016124\n",
            "batch 12 loss 0.0002493082030799733\n",
            "batch 13 loss 0.00035228458598062775\n",
            "batch 14 loss 0.00033862702430877015\n",
            "batch 15 loss 0.0002443709234319853\n",
            "batch 16 loss 0.00027584991487427773\n",
            "batch 17 loss 0.00031523918333967903\n",
            "batch 18 loss 0.00028711532796683794\n",
            "batch 19 loss 0.0002844553235029574\n",
            "batch 20 loss 0.00027678877018516904\n",
            "batch 21 loss 0.0002832114210187099\n",
            "batch 22 loss 0.00031088910023684336\n",
            "batch 23 loss 0.00027176710316522844\n",
            "batch 24 loss 0.00037043279375253697\n",
            "batch 25 loss 0.0002373022353638326\n",
            "batch 26 loss 0.0002819565003497746\n",
            "batch 27 loss 0.00027923145652806536\n",
            "batch 28 loss 0.00029273957813625293\n",
            "batch 29 loss 0.0002998941774099094\n",
            "batch 30 loss 0.00026247311538152925\n",
            "batch 31 loss 0.0003125804048706309\n",
            "batch 32 loss 0.00032114953687975746\n",
            "batch 33 loss 0.0002655405756863222\n",
            "batch 34 loss 0.0002896862013014914\n",
            "batch 35 loss 0.0002400491703885661\n",
            "batch 36 loss 0.0003596248217394354\n",
            "batch 37 loss 0.00025590110220890047\n",
            "batch 38 loss 0.0003089241032536549\n",
            "batch 39 loss 0.0002543242598715628\n",
            "batch 40 loss 0.0002520397254666294\n",
            "batch 41 loss 0.00029212719963701725\n",
            "batch 42 loss 0.00031436578890105374\n",
            "batch 43 loss 0.00026535323397496583\n",
            "batch 44 loss 0.0002952929666970475\n",
            "batch 45 loss 0.00027216635241044394\n",
            "batch 46 loss 0.00022993559481657093\n",
            "batch 47 loss 0.00029111527951279756\n",
            "batch 48 loss 0.00028356417441826774\n",
            "batch 49 loss 0.0002897394304008441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeDLBQ_ZU5wO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}